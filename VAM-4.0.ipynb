{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Depdencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchviz matplotlib numpy tqdm scikit-image tensorboard_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "import skimage.measure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import unittest\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    TRAIN = 1\n",
    "    VALID = 2\n",
    "    TEST  = 3\n",
    "\n",
    "class DatasetName(Enum):\n",
    "    MNIST = 1\n",
    "    AUGMENTED = 2\n",
    "    TRANSFORMED = 3\n",
    "    AUGMENTED_MEDICAL = 4\n",
    "    CLOSED_SQUARES = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # glimpse network params\n",
    "        self.patch_size      = 12         # size of extracted patch at highest res\n",
    "        self.glimpse_scale   = 2         # scale of successive patches\n",
    "        self.num_patches     = 5         # Num of downscaled patches per glimpse\n",
    "        self.loc_hidden      = 128       # hidden size of loc fc layer\n",
    "        self.glimpse_hidden  = 128       # hidden size of glimpse fc\n",
    "\n",
    "        # core network params\n",
    "        self.num_glimpses    = 6         # Num of glimpses, i.e. BPTT iterations\n",
    "        self.hidden_size     = 256       # hidden size of rnn\n",
    "\n",
    "        # reinforce params\n",
    "        self.std             = 0.11      # gaussian policy standard deviation\n",
    "        self.M               = 1         # Monte Carlo sampling for valid and test sets\n",
    "        self.reward_multi    = 1         # reward multiplier(0-1] setting it to values <1\n",
    "                                         # should make the policy less flakey ( I think. let's see the effects)\n",
    "\n",
    "        # action network\n",
    "        self.num_classes     = 10         # the number of classes\n",
    "\n",
    "        # ETC params\n",
    "        self.valid_size      = 0.1       # Proportion of training set used for validation\n",
    "        self.batch_size      = 100       # Num of images in each batch of data\n",
    "        self.num_workers     = 4         # Num of subprocesses to use for data loading\n",
    "        self.shuffle         = True      # Whether to shuffle the train and valid indices\n",
    "        self.show_sample     = False     # Whether to visualize a sample grid of the data\n",
    "\n",
    "        # training params\n",
    "        self.is_train        = True      # Whether to train(true) or test the model\n",
    "        self.resume          = False     # Whether to resume training from checkpoint\n",
    "        self.weight_decay    = 1e-5      # Weight decay for regularization\n",
    "        self.momentum        = 0.5       # Nesterov momentum value TODO not used\n",
    "        self.epochs          = 500       # Num of epochs to train for\n",
    "        self.init_lr         = 0.001      # Initial learning rate value\n",
    "        self.lr_patience     = 50        # Number of epochs to wait before reducing lr\n",
    "        self.train_patience  = 100       # Number of epochs to wait before stopping train\n",
    "\n",
    "        # other params\n",
    "        self.use_gpu         = True      # Whether to run on the GPU\n",
    "        self.best            = True      # Load best model or most recent for testing\n",
    "        self.random_seed     = 1         # Seed to ensure reproducibility\n",
    "        self.data_dir        = \"./data\"  # Directory in which data is stored\n",
    "        self.ckpt_dir        = \"./ckpt\"  # Directory in which to save model checkpoints\n",
    "        self.logs_dir        = \"./logs/\" # Directory in which Tensorboard logs wil be stored\n",
    "        self.use_tensorboard = False     # Whether to use tensorboard for visualization\n",
    "        self.print_freq      = 100       # How frequently to print training details\n",
    "        self.plot_freq       = 1         # How frequently to plot glimpses\n",
    "        self.dataset         = DatasetName.AUGMENTED_MEDICAL\n",
    "        self.model_name      = \"ram_{}_{}x{}_{}\".format(\n",
    "            self.num_glimpses,\n",
    "            self.patch_size,\n",
    "            self.patch_size,\n",
    "            self.glimpse_scale,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "class AugmentedMedicalMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Augmented mnist meant to mimic whole-slide-images of tumor cells.\n",
    "    9's represent cancer cells. There are 4 different labels, based on the number of 9's:\n",
    "\n",
    "    zero 9's          - no cancer\n",
    "    one 9             - isolated tumor cell\n",
    "    two 9's           - micro-metastasis \n",
    "    three or more 9's - macro-metastasis\n",
    "\n",
    "    Each image contains between 3 and 10 cells at random, which may be overlapping.\n",
    "    It consists of 5000 items of each category(total 20.000) for training and 500(2.000) of each for testing\n",
    "    of size 256 x 256. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 train,\n",
    "                 data_dir=\"MEDNIST\",\n",
    "                 mnist_transform=None,\n",
    "                 transform=None,\n",
    "                 total_train=20000,\n",
    "                 total_test=2000,\n",
    "                 n_partitions_test=1,\n",
    "                 n_partitions_train=5):\n",
    "\n",
    "        self.mnist_transform = mnist_transform\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.total = total_train if self.train else total_test\n",
    "        self.n_partitions_test = n_partitions_test\n",
    "        self.n_partitions_train = n_partitions_train\n",
    "        self.dir = os.path.join(root_dir, data_dir, \"train\" if train else \"test\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.__create_dataset_if_needed()\n",
    "\n",
    "        self.__load_data()\n",
    "\n",
    "    def __dataset_exists(self):\n",
    "        # mkdir if not exists\n",
    "        os.makedirs(self.dir, exist_ok=True)\n",
    "        len_files = len(os.listdir(self.dir))\n",
    "        if len_files > 0:\n",
    "            print(\"Data existing, skipping creation.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Dataset missing. Creating...\")\n",
    "        return False\n",
    "\n",
    "    def __combine_images(self, images, output_dim):\n",
    "        \"\"\"\n",
    "        Combines the given images into a single image of output_dim size. Combinations are done randomly and \n",
    "        overlapping is possible. Images will always be within bounds completely.\n",
    "        \"\"\"\n",
    "        np_images = np.array(images)\n",
    "        input_dim = np_images.shape[-1]\n",
    "        new_image = np.zeros(shape=(output_dim, output_dim), dtype=np.float32)\n",
    "        for image in np_images:\n",
    "            i, j = np.random.randint(0, output_dim - input_dim, size=2)\n",
    "            new_image[i:i + input_dim, j:j + input_dim] = image\n",
    "        return new_image\n",
    "\n",
    "    def __get_cell_counts(self, items_per_class_count, class_index):\n",
    "        # exclusive\n",
    "        max_items = 11\n",
    "        min_number_of_cells = 3\n",
    "        # 0,1,2,3+ for no tumor cells, isolated tumor cells, \n",
    "        # micro-metastasis and macro-metastasis respectively\n",
    "        num_tumor_cells = class_index if class_index != 3 else np.random.randint(3, max_items)\n",
    "\n",
    "        num_healthy_cells = max_items - num_tumor_cells\n",
    "        if num_healthy_cells + num_tumor_cells < min_number_of_cells:\n",
    "            num_healthy_cells = min_number_of_cells - num_tumor_cells\n",
    "\n",
    "        return (num_tumor_cells, num_healthy_cells)\n",
    "\n",
    "    def __generate_for_class(self,\n",
    "                             items,\n",
    "                             items_per_class_count,\n",
    "                             class_index,\n",
    "                             uid,\n",
    "                             all_tumor_cell_images,\n",
    "                             all_healthy_cell_images):\n",
    "        for _ in range(items_per_class_count):\n",
    "            num_tumors, num_healthy = self.__get_cell_counts(items_per_class_count, class_index)\n",
    "\n",
    "            healthy_idxs = np.random.randint(0, len(all_healthy_cell_images), num_healthy)\n",
    "            tumor_idxs = np.random.randint(0, len(all_tumor_cell_images), num_tumors)\n",
    "\n",
    "            healthy_cells = all_healthy_cell_images[healthy_idxs]\n",
    "            tumor_cells = all_tumor_cell_images[tumor_idxs]\n",
    "            cells = np.vstack((healthy_cells, tumor_cells))\n",
    "            image = self.__combine_images(cells, 256)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            self.data.append(image)\n",
    "            self.source_images.append(tumor_cells.numpy())\n",
    "            self.labels.append(class_index)\n",
    "            uid += 1\n",
    "        return uid\n",
    "\n",
    "    def __create_dataset_if_needed(self):\n",
    "        if self.__dataset_exists():\n",
    "            return\n",
    "\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.source_images = []\n",
    "\n",
    "        # in how many partitions to split dataset creation\n",
    "        partitions_count = 10\n",
    "\n",
    "        # number of classes in output (fixed)\n",
    "        num_classes = 4\n",
    "\n",
    "        mnist = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           download=True,\n",
    "                                           transform=self.mnist_transform)\n",
    "\n",
    "        mnist_loader = iter(torch.utils.data.DataLoader(mnist,\n",
    "                                                        batch_size=int(self.total / partitions_count),\n",
    "                                                        shuffle=False,\n",
    "                                                        num_workers=0))\n",
    "        uid = 0\n",
    "        batch, mnist_labels = mnist_loader.next()\n",
    "        # 9's represent tumors\n",
    "        all_tumor_cell_images = batch[mnist_labels == 9]\n",
    "        # everything else except 6's healthy cells\n",
    "        all_healthy_cell_images = batch[(mnist_labels != 9) & (mnist_labels != 6)]\n",
    "\n",
    "        for _ in range(partitions_count):\n",
    "            items_per_class_count = int(self.total / (num_classes * partitions_count))\n",
    "\n",
    "            for class_index in range(num_classes):\n",
    "                uid = self.__generate_for_class(class_index,\n",
    "                                                items_per_class_count,\n",
    "                                                class_index,\n",
    "                                                uid,\n",
    "                                                all_tumor_cell_images,\n",
    "                                                all_healthy_cell_images)\n",
    "        self.__store()\n",
    "        print(\"Done.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, uid):\n",
    "        if torch.is_tensor(uid):\n",
    "            uid = uid.tolist()\n",
    "        label = self.labels[uid]\n",
    "        sample = self.data[uid]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return (sample, label)\n",
    "\n",
    "    def __store(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "\n",
    "        assert (len(self.data) == len(self.labels))\n",
    "        max_index = len(self.data)\n",
    "        partition_size = max_index / n_partitions\n",
    "        for i in range(n_partitions):\n",
    "            start, end = (int(partition_size * i), int(partition_size * (i + 1)))\n",
    "            partition = np.array(self.data[start:end])\n",
    "            np.save(os.path.join(self.dir, \"part_\" + str(i)), partition)\n",
    "\n",
    "        np.save(os.path.join(self.dir, \"labels\"), np.array(self.labels))\n",
    "\n",
    "        if not self.train:\n",
    "            np.save(os.path.join(self.dir, \"sources\"), np.array(self.source_images))\n",
    "\n",
    "    def __load_data(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        data = []\n",
    "        for i in range(n_partitions):\n",
    "            data.append(np.load(os.path.join(self.dir, \"part_\" + str(i) + \".npy\")))\n",
    "        self.data = np.vstack(data)\n",
    "        self.labels = np.load(os.path.join(self.dir, \"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "class ClosedSquaresDataset(Dataset):\n",
    "    \"\"\"Binary: number of not closed squares\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train,\n",
    "                 size=64,\n",
    "                 object_width=3,\n",
    "                 n_missing=2,\n",
    "                 n_classes=4,\n",
    "                 n_circles=6,\n",
    "                 total_train=16000,\n",
    "                 total_test=1600):\n",
    "        if train:\n",
    "            np.random.seed(1)\n",
    "        else:\n",
    "            np.random.seed(2)\n",
    "        self.n = total_train if train else total_test\n",
    "        self.__create_data(n_classes, n_circles, size, object_width, n_missing)\n",
    "\n",
    "    def __create_data(self, n_classes, n_circles, size, object_width, n_missing):\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "\n",
    "        for class_i in range(n_classes):\n",
    "            for _ in range(int(self.n / n_classes)):\n",
    "                image = self.__generate_image(class_i, n_circles, size, object_width, n_missing)\n",
    "                self.data.append(torch.tensor(image))\n",
    "                self.labels.append(class_i)\n",
    "\n",
    "    def __generate_image(self, n_open, n_all, size, object_width, n_missing):\n",
    "        image = np.zeros((size, size))\n",
    "        # top left x,y positions within bounds\n",
    "        top_lefts = (np.random.rand(n_all, 2) * (size - (object_width + 2))).astype(int)\n",
    "        # ensure no overlapping\n",
    "        for top_left in top_lefts:\n",
    "            x_0, y_0 = top_left[0], top_left[1]\n",
    "            # 1 bigger so no overlaps\n",
    "            image[x_0: x_0 + object_width + 2, y_0:y_0 + object_width + 2] += 1\n",
    "        # make sure no overlapping\n",
    "        is_valid = np.all(image <= 1)\n",
    "        if is_valid:\n",
    "            image = np.zeros((size, size)).astype(np.float32)\n",
    "            for i, top_left in enumerate(top_lefts):\n",
    "                x_0, y_0 = top_left[0] + 1, top_left[1] + 1\n",
    "                image[x_0: x_0 + object_width, y_0:y_0 + object_width] = 1\n",
    "                # open it\n",
    "                if i < n_open:\n",
    "                    pos = (np.random.rand(n_missing, 2) * object_width).astype(int)\n",
    "                    for p in pos:\n",
    "                        image[x_0 + p[0], y_0 + p[1]] = 0\n",
    "            return image.reshape(1, size, size)\n",
    "        else:\n",
    "            return self.__generate_image(n_open, n_all, size, object_width, n_missing)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, uid):\n",
    "        if torch.is_tensor(uid):\n",
    "            uid = uid.tolist()\n",
    "        label = self.labels[uid]\n",
    "        sample = self.data[uid]\n",
    "\n",
    "        return (sample, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DatasetLocator:\n",
    "    def __init__(self, conf: Config):\n",
    "\n",
    "        self.dataset = conf.dataset\n",
    "        self.gpu_run = conf.use_gpu\n",
    "        self.batch_size = conf.batch_size\n",
    "        train, valid, test = self.__load_data()\n",
    "\n",
    "        self.dataset_dict = {\n",
    "            DatasetType.TRAIN: train,\n",
    "            DatasetType.VALID: valid,\n",
    "            DatasetType.TEST: test\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def __f(image):\n",
    "        np_image = np.array(image)\n",
    "        input_dim = np_image.shape[-1]\n",
    "        new_image = np.zeros(shape=(60, 60), dtype=np.float32)\n",
    "        i, j = np.random.randint(0, 60 - input_dim, size=2)\n",
    "        new_image[i:i + input_dim, j:j + input_dim] = np_image\n",
    "        return new_image\n",
    "\n",
    "    def __transformed_mnist_transformation(self):\n",
    "        return transforms.Compose(\n",
    "            [torchvision.transforms.Lambda(self.__f),\n",
    "             torchvision.transforms.ToTensor(),\n",
    "             transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    @staticmethod\n",
    "    def __augmented_mnist_transformation():\n",
    "        return transforms.Compose([\n",
    "            torchvision.transforms.RandomAffine(degrees=(-180, 180), scale=(0.5, 1.0), ),\n",
    "            torchvision.transforms.ToTensor()])\n",
    "\n",
    "    @staticmethod\n",
    "    def __augmented_mnist_simple_transformation():\n",
    "        return transforms.Compose([\n",
    "            torchvision.transforms.RandomAffine(degrees=(0, 90), scale=(0.9, 1.0), ),\n",
    "            torchvision.transforms.ToTensor()])\n",
    "\n",
    "    def __load_data(self):\n",
    "        train_total = self.__load_dataset(True)\n",
    "        test = self.__load_dataset(False)\n",
    "\n",
    "        train_length = int(len(train_total) * 0.9)\n",
    "        valid_length = len(train_total) - train_length\n",
    "        (train, valid) = torch.utils.data.random_split(train_total, (train_length, valid_length))\n",
    "        return train, valid, test\n",
    "\n",
    "    def __load_dataset(self, is_train):\n",
    "        transform = None\n",
    "        if self.dataset == DatasetName.MNIST:\n",
    "            transform = torchvision.transforms.ToTensor()\n",
    "        elif self.dataset == DatasetName.AUGMENTED:\n",
    "            transform = self.__augmented_mnist_transformation()\n",
    "        elif self.dataset == DatasetName.TRANSFORMED:\n",
    "            transform = self.__transformed_mnist_transformation()\n",
    "        elif self.dataset == DatasetName.AUGMENTED_MEDICAL:\n",
    "            return AugmentedMedicalMNISTDataset(root_dir='.', data_dir = \"MEDNIST\",train = is_train, mnist_transform = self.__augmented_mnist_transformation())\n",
    "\n",
    "        elif self.dataset == DatasetName.CLOSED_SQUARES:\n",
    "            return ClosedSquaresDataset(train=is_train)\n",
    "        return torchvision.datasets.MNIST(root='./data', train=is_train, download=True, transform=transform)\n",
    "\n",
    "    def data_loader(self, dataset: DatasetType):\n",
    "        should_shuffle = dataset == DatasetType.TRAIN\n",
    "        data = self.dataset_dict[dataset]\n",
    "        return torch.utils.data.DataLoader(data,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           pin_memory=self.gpu_run,\n",
    "                                           shuffle=should_shuffle,\n",
    "                                           num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ActionNetwork(nn.Module):\n",
    "    \"\"\"The action network.\n",
    "\n",
    "    Uses the internal state `h_t` of the core network to\n",
    "    produce the final output classification.\n",
    "\n",
    "    Concretely, feeds the hidden state `h_t` through a fc\n",
    "    layer followed by a softmax to create a vector of\n",
    "    output probabilities over the possible classes.\n",
    "\n",
    "    Hence, the environment action `a_t` is drawn from a\n",
    "    distribution conditioned on an affine transformation\n",
    "    of the hidden state vector `h_t`, or in other words,\n",
    "    the action network is simply a linear softmax classifier.\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the fc layer.\n",
    "        output_size: output size of the fc layer.\n",
    "        h_t: the hidden state vector of the core network\n",
    "            for the current time step `t`.\n",
    "\n",
    "    Returns:\n",
    "        a_t: output probability vector over the classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        logging.info(self)\n",
    "\n",
    "    def forward(self, h_t):\n",
    "        logging.debug(\"\\n\\nActionNetwork\")\n",
    "        logging.debug(f\"Input:   {h_t.shape}\")\n",
    "        a_t = F.log_softmax(self.fc(h_t), dim=1)\n",
    "        logging.debug(f\"Softmax: {a_t.shape}\\n\\n\")\n",
    "        return a_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"The baseline network.\n",
    "\n",
    "    This network regresses the baseline in the\n",
    "    reward function to reduce the variance of\n",
    "    the gradient update.\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the fc layer.\n",
    "        output_size: output size of the fc layer.\n",
    "        h_t: the hidden state vector of the core network\n",
    "            for the current time step `t`.\n",
    "\n",
    "    Returns:\n",
    "        b_t: a 2D vector of shape (B, 1). The baseline\n",
    "            for the current time step `t`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        logging.info(self)\n",
    "\n",
    "    def forward(self, h_t):\n",
    "        logging.debug(\"\\n\\nBaselineNetwork\")\n",
    "        logging.debug(f\"Input: {h_t.shape}\")\n",
    "        b_t = F.relu(self.fc(h_t.detach()))\n",
    "        logging.debug(f\"Fc1:   {b_t.shape}\\n\\n\")\n",
    "        return b_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoreNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        logging.info(self)\n",
    "\n",
    "    def forward(self, g_t):\n",
    "        h1 = self.i2h(g_t)\n",
    "        h2 = self.h2h(self.hidden_state)\n",
    "        h_t = F.relu(h1 + h2)\n",
    "        self.hidden_state = h_t\n",
    "        return h_t\n",
    "\n",
    "    def reset(self, batch_size, device):\n",
    "        self.hidden_state = torch.zeros(\n",
    "            (batch_size, self.hidden_size),\n",
    "            dtype=torch.float,\n",
    "            device=device,\n",
    "            requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Retina:\n",
    "    \"\"\"\n",
    "    Extracts a glimpse `phi` around location `l`\n",
    "    from an image `x`.\n",
    "\n",
    "    Encodes the region around `l` at a\n",
    "    high-resolution but uses a progressively lower\n",
    "    resolution for pixels further from `l`, resulting\n",
    "    in a compressed representation of the original\n",
    "    image `x`.\n",
    "\n",
    "    Args:\n",
    "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "            of images.\n",
    "        l: a 2D Tensor of shape (B, 2). Contains normalized\n",
    "            coordinates in the range [-1, 1].\n",
    "        From config:\n",
    "\n",
    "        patch_size: size of the first square patch.\n",
    "        num_patches: number of patches to extract in the glimpse.\n",
    "        scale: scaling factor that controls the size of\n",
    "            successive patches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf:Config):\n",
    "        self.patch_size = conf.patch_size\n",
    "        self.num_patches = conf.num_patches\n",
    "        self.scale = conf.glimpse_scale\n",
    "\n",
    "    def foveate(self, x, l):\n",
    "        \"\"\"Extract `num_patches` square patches of size `patch_size`, centered\n",
    "        at location `l`. The initial patch is a square of\n",
    "        size `patch_size`, and each subsequent patch is a square\n",
    "        whose side is `scale` times the size of the previous\n",
    "        patch.\n",
    "\n",
    "        The `num_patches` patches are finally resized to (patch_size, patch_size) and\n",
    "        concatenated into a tensor of shape (B, k, g, g, C).\n",
    "        \"\"\"\n",
    "        phi = []\n",
    "        size = self.patch_size\n",
    "\n",
    "        # extract k patches of increasing size\n",
    "        for i in range(self.num_patches):\n",
    "            phi.append(self.extract_patch(x, l, size))\n",
    "            size = int(self.scale * size)\n",
    "\n",
    "        # resize the patches to squares of size g\n",
    "        for i in range(1, len(phi)):\n",
    "            k = phi[i].shape[-1] // self.patch_size\n",
    "            phi[i] = F.avg_pool2d(phi[i], k)\n",
    "\n",
    "        # concatenate into a single tensor and flatten\n",
    "        phi = torch.cat(phi, 1)\n",
    "        # phi = phi.view(phi.shape[0], -1)\n",
    "\n",
    "        return phi\n",
    "\n",
    "    def extract_patch(self, x, l, size):\n",
    "        \"\"\"Extract a single patch for each image in `x`.\n",
    "\n",
    "        Args:\n",
    "        x: a 4D Tensor of shape (B, C, H, W). The minibatch\n",
    "            of images.\n",
    "        l: a 2D Tensor of shape (B, 2).\n",
    "        size: a scalar defining the size of the extracted patch.\n",
    "\n",
    "        Returns:\n",
    "            patch: a 4D Tensor of shape (B, num_patches, size, size)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        start = self.denormalize(H, l)\n",
    "        end = start + size\n",
    "\n",
    "        # pad with zeros\n",
    "        x = F.pad(x, (size // 2, size // 2, size // 2, size // 2))\n",
    "\n",
    "        # loop through mini-batch and extract patches\n",
    "        patch = []\n",
    "        for i in range(B):\n",
    "            patch.append(x[i, :, start[i, 1]: end[i, 1], start[i, 0]: end[i, 0]])\n",
    "        return torch.stack(patch)\n",
    "\n",
    "    def denormalize(self, T, coords):\n",
    "        \"\"\"Convert coordinates in the range [-1, 1] to\n",
    "        coordinates in the range [0, T] where `T` is\n",
    "        the size of the image.\n",
    "        \"\"\"\n",
    "        return (0.5 * ((coords + 1.0) * T)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GlimpseNetwork(nn.Module):\n",
    "    \"\"\"The glimpse network.\n",
    "\n",
    "    TODO\n",
    "\n",
    "    Args:\n",
    "        conf.glimpse_hidden: hidden layer size of the fc layer for `phi`.\n",
    "        conf.loc_hidden: hidden layer size of the fc layer for `l`.\n",
    "        g: size of the square patches in the glimpses extracted\n",
    "        by the retina.\n",
    "        k: number of patches to extract per glimpse.\n",
    "        s: scaling factor that controls the size of successive patches.\n",
    "        c: number of channels in each image.\n",
    "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "            of images.\n",
    "        l_t_prev: a 2D tensor of shape (B, 2). Contains the glimpse\n",
    "            coordinates [x, y] for the previous timestep `t-1`.\n",
    "\n",
    "    Returns:\n",
    "        g_t: a 2D tensor of shape (B, hidden_size).\n",
    "            The glimpse representation returned by\n",
    "            the glimpse network for the current\n",
    "            timestep `t`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retina = Retina(conf)\n",
    "\n",
    "        D_out = conf.glimpse_hidden + conf.loc_hidden\n",
    "\n",
    "        # what\n",
    "\n",
    "        # padding of 1, to ensure same dimensions\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.retina.num_patches, out_channels=16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv1.out_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=self.conv2.out_channels, track_running_stats=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=self.conv2.out_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "\n",
    "        D_in = self.conv3.out_channels * conf.patch_size * conf.patch_size\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=D_in, out_features=D_out)\n",
    "\n",
    "        # where\n",
    "        # in_features = 2, loc is a tuple of (x,y)\n",
    "        self.loc_fc1 = nn.Linear(in_features=2, out_features=D_out)\n",
    "\n",
    "        logging.info(self)\n",
    "\n",
    "    def forward(self, x, l_t_prev):\n",
    "        logging.debug(\"\\n\\nGlimpseNetwork shapes\")\n",
    "        logging.debug(\"#### What ####\")\n",
    "        # generate glimpse phi from image x\n",
    "        phi = self.retina.foveate(x, l_t_prev)\n",
    "        logging.debug(phi.shape)\n",
    "\n",
    "        # what\n",
    "        # 3 conv layers\n",
    "        h = self.conv1(phi)\n",
    "        logging.debug(f\"Conv1:      {h.shape}\")\n",
    "        h = F.relu(h)\n",
    "        logging.debug(f\"Conv1 ReLu: {h.shape}\")\n",
    "        h = F.relu(self.bn1(self.conv2(h)))\n",
    "        logging.debug(f\"Conv2:        {h.shape}\")\n",
    "        logging.debug(f\"Bn1 ReLu:   {h.shape}\")\n",
    "        h = F.relu(self.conv3(h))\n",
    "        logging.debug(f\"Conv3:      {h.shape}\")\n",
    "        # flatten\n",
    "        # keep batch dimension and determine other one automatically\n",
    "        h = h.view(x.shape[0], -1)\n",
    "        logging.debug(f\"Flatten:    {h.shape}\")\n",
    "\n",
    "        # fully connected layers\n",
    "        h = F.relu(self.fc1(h))\n",
    "        logging.debug(f\"Fc1:        {h.shape}\")\n",
    "\n",
    "        # where\n",
    "        logging.debug(\"#### Where ####\")\n",
    "        # flatten location vector\n",
    "        l_t_prev = l_t_prev.view(l_t_prev.size(0), -1)\n",
    "        logging.debug(f\"Input:         {l_t_prev.shape}\")\n",
    "\n",
    "        l = F.relu(self.loc_fc1(l_t_prev))\n",
    "        logging.debug(f\"Fc1(loc):      {l.shape}\")\n",
    "        logging.debug(\"#### Combined ####\")\n",
    "        # combine what and where\n",
    "        g = F.relu(h * l)\n",
    "\n",
    "        logging.debug(f\"relu(h * l):   {g.shape}\\n\\n\")\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LocationNetwork(nn.Module):\n",
    "    \"\"\"The location network.\n",
    "\n",
    "    Uses the internal state `h_t` of the core network to\n",
    "    produce the location coordinates `l_t` for the next\n",
    "    time step.\n",
    "\n",
    "    Concretely, feeds the hidden state `h_t` through a fc\n",
    "    layer followed by a tanh to clamp the output beween\n",
    "    [-1, 1]. This produces a 2D vector of means used to\n",
    "    parametrize a two-component Gaussian with a fixed\n",
    "    variance from which the location coordinates `l_t`\n",
    "    for the next time step are sampled.\n",
    "\n",
    "    Hence, the location `l_t` is chosen stochastically\n",
    "    from a distribution conditioned on an affine\n",
    "    transformation of the hidden state vector `h_t`.\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the fc layer.\n",
    "        output_size: output size of the fc layer.\n",
    "        std: standard deviation of the normal distribution.\n",
    "        h_t: the hidden state vector of the core network for\n",
    "            the current time step `t`.\n",
    "\n",
    "    Returns:\n",
    "        mu: a 2D vector of shape (B, 2).\n",
    "        l_t: a 2D vector of shape (B, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.std = std\n",
    "\n",
    "        hid_size = input_size // 2\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        #self.fc_lt = nn.Linear(hid_size, output_size)\n",
    "        logging.info(self)\n",
    "\n",
    "    def forward(self, h_t):\n",
    "        logging.debug(\"\\n\\nLocationNetwork\")\n",
    "        logging.debug(f\"Input:     {h_t.shape}\")\n",
    "        # compute mean\n",
    "        mean = torch.tanh(self.fc(h_t.detach()))\n",
    "        logging.debug(f\"fc2+tanh:  {mean.shape}\")\n",
    "\n",
    "        if self.training:\n",
    "            l_t = torch.distributions.Normal(mean, self.std).rsample().detach()\n",
    "        #eval, not stochastic\n",
    "        else:\n",
    "            l_t = mean\n",
    "\n",
    "        #if torch.any(l_t < -1):\n",
    "            #print(\"MEAN\")\n",
    "            #print(mean)\n",
    "            #print(\"L_T\")\n",
    "            #print(l_t)\n",
    "        ##if torch.any(l_t > 1):\n",
    "        #    print(\"MEAN\")\n",
    "        #    print(mean)\n",
    "        # bound between [-1, 1]\n",
    "        l_t = torch.clamp(l_t, -1, 1)\n",
    "\n",
    "        return mean, l_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RecurrentAttention(nn.Module):\n",
    "    \"\"\"A Recurrent Model of Visual Attention (RAM) [1].\n",
    "\n",
    "    RAM is a recurrent neural network that processes\n",
    "    inputs sequentially, attending to different locations\n",
    "    within the image one at a time, and incrementally\n",
    "    combining information from these fixations to build\n",
    "    up a dynamic internal representation of the image.\n",
    "\n",
    "    References:\n",
    "      [1]: Minh et. al., https://arxiv.org/abs/1406.6247\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,config):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "          g: size of the square patches in the glimpses extracted by the retina.\n",
    "          k: number of patches to extract per glimpse.\n",
    "          s: scaling factor that controls the size of successive patches.\n",
    "          c: number of channels in each image.\n",
    "          h_g: hidden layer size of the fc layer for `phi`.\n",
    "          h_l: hidden layer size of the fc layer for `l`.\n",
    "          std: standard deviation of the Gaussian policy.\n",
    "          hidden_size: hidden size of the rnn.\n",
    "          num_classes: number of classes in the dataset.\n",
    "          num_glimpses: number of glimpses to take per image,\n",
    "            i.e. number of BPTT steps.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.sensor = GlimpseNetwork(config)\n",
    "        self.rnn = CoreNetwork(config.hidden_size, config.hidden_size)\n",
    "        self.locator = LocationNetwork(config.hidden_size, 2, config.std)\n",
    "        self.classifier = ActionNetwork(config.hidden_size, config.num_classes)\n",
    "        self.baseliner = BaselineNetwork(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, l_t_prev, last=False):\n",
    "        \"\"\"Run RAM for one timestep on a minibatch of images.\n",
    "\n",
    "        Args:\n",
    "            x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "                of images.\n",
    "            l_t_prev: a 2D tensor of shape (B, 2). The location vector\n",
    "                containing the glimpse coordinates [x, y] for the previous\n",
    "                timestep `t-1`.\n",
    "            h_t_prev: a 2D tensor of shape (B, hidden_size). The hidden\n",
    "                state vector for the previous timestep `t-1`.\n",
    "            last: a bool indicating whether this is the last timestep.\n",
    "                If True, the action network returns an output probability\n",
    "                vector over the classes and the baseline `b_t` for the\n",
    "                current timestep `t`. Else, the core network returns the\n",
    "                hidden state vector for the next timestep `t+1` and the\n",
    "                location vector for the next timestep `t+1`.\n",
    "\n",
    "        Returns:\n",
    "            h_t: a 2D tensor of shape (B, hidden_size). The hidden\n",
    "                state vector for the current timestep `t`.\n",
    "            mu: a 2D tensor of shape (B, 2). The mean that parametrizes\n",
    "                the Gaussian policy.\n",
    "            l_t: a 2D tensor of shape (B, 2). The location vector\n",
    "                containing the glimpse coordinates [x, y] for the\n",
    "                current timestep `t`.\n",
    "            b_t: a vector of length (B,). The baseline for the\n",
    "                current time step `t`.\n",
    "            probabilities: a 2D tensor of shape (B, num_classes). The\n",
    "                output log probability vector over the classes.\n",
    "            mean_t: a vector of length (B,).\n",
    "        \"\"\"\n",
    "        g_t = self.sensor(x, l_t_prev)\n",
    "        h_t = self.rnn(g_t)\n",
    "        mean_t, l_t = self.locator(h_t.detach())\n",
    "        b_t = self.baseliner(h_t.detach()).squeeze()\n",
    "\n",
    "        if last:\n",
    "            probabilities = self.classifier(h_t)\n",
    "            return h_t, l_t, b_t, probabilities, mean_t\n",
    "\n",
    "        return h_t, l_t, b_t, mean_t\n",
    "\n",
    "    def reset(self, batch_size, device):\n",
    "        # h_t maintained by rnn itself\n",
    "        self.rnn.reset(batch_size=batch_size, device=device)\n",
    "\n",
    "        #l_t = torch.zeros(batch_size, 2).to(device)\n",
    "        l_t = torch.FloatTensor(batch_size, 2).uniform_(-1, 1).to(device)\n",
    "        logging.debug(f\"DRAM reset, l_0: {l_t}\")\n",
    "        # TODO it doesn't right?\n",
    "        l_t.requires_grad = True\n",
    "\n",
    "        return l_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def prepare_dirs(config):\n",
    "    for path in [config.data_dir, config.ckpt_dir, config.logs_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "def save_config(config):\n",
    "    model_name = \"ram_{}_{}x{}_{}\".format(\n",
    "        config.num_glimpses, config.patch_size, config.patch_size, config.glimpse_scale\n",
    "    )\n",
    "    filename = model_name + \"_params.json\"\n",
    "    param_path = os.path.join(config.ckpt_dir, filename)\n",
    "\n",
    "    print(\"[*] Model Checkpoint Dir: {}\".format(config.ckpt_dir))\n",
    "    print(\"[*] Param Path: {}\".format(param_path))\n",
    "\n",
    "    with open(param_path, \"w\") as fp:\n",
    "        json.dump(config.__dict__, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "import numpy as np\n",
    "class Trainer:\n",
    "    \"\"\"A Recurrent Attention Model trainer.\n",
    "\n",
    "    All hyperparameters are provided by the user in the\n",
    "    config file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data_loader):\n",
    "        \"\"\"\n",
    "        Construct a new Trainer instance.\n",
    "\n",
    "        Args:\n",
    "            config: object containing command line arguments.\n",
    "            data_loader: A data iterator.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        if config.use_gpu and torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.num_glimpses = config.num_glimpses\n",
    "\n",
    "        # data params\n",
    "        if config.is_train:\n",
    "            self.train_loader = data_loader[0]\n",
    "            self.valid_loader = data_loader[1]\n",
    "            self.num_train = len(self.train_loader.dataset)\n",
    "            self.num_valid = len(self.valid_loader.dataset)\n",
    "        else:\n",
    "            self.test_loader = data_loader\n",
    "            self.num_test = len(self.test_loader.dataset)\n",
    "\n",
    "        # training params\n",
    "        self.epochs = config.epochs\n",
    "        self.start_epoch = 0\n",
    "        self.lr = config.init_lr\n",
    "\n",
    "        # misc params\n",
    "        self.best = config.best\n",
    "        self.best_valid_acc = 0.0\n",
    "        self.counter = 0\n",
    "\n",
    "        self.plot_dir = \"./plots/\" + self.config.model_name + \"/\"\n",
    "        if not os.path.exists(self.plot_dir):\n",
    "            os.makedirs(self.plot_dir)\n",
    "\n",
    "        # configure tensorboard logging\n",
    "        if self.config.use_tensorboard:\n",
    "            tensorboard_dir = self.config.logs_dir + self.config.model_name\n",
    "            logging.info(\"[*] Saving tensorboard logs to {}\".format(tensorboard_dir))\n",
    "            if not os.path.exists(tensorboard_dir):\n",
    "                os.makedirs(tensorboard_dir)\n",
    "            configure(tensorboard_dir)\n",
    "\n",
    "        # build RAM model\n",
    "        self.model = RecurrentAttention(config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # initialize optimizer and scheduler\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.init_lr,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, \"min\", patience=config.lr_patience\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model on the training set.\n",
    "\n",
    "        A checkpoint of the model is saved after each epoch\n",
    "        and if the validation accuracy is improved upon,\n",
    "        a separate ckpt is created for use on the test set.\n",
    "        \"\"\"\n",
    "        # load the most recent checkpoint\n",
    "        if self.config.resume:\n",
    "            self.load_checkpoint(best=False)\n",
    "\n",
    "        logging.info(\"\\n[*] Train on {} samples, validate on {} samples\"\n",
    "              .format(self.num_train, self.num_valid))\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "            logging.info(\"\\nEpoch: {}/{} - LR: {:.6f}\".format(epoch + 1, self.epochs, self.optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "            # train for 1 epoch\n",
    "            train_loss, train_acc, loss_act,loss_base,loss_reinf = self.train_one_epoch(epoch)\n",
    "\n",
    "            # evaluate on validation set\n",
    "            valid_loss, valid_acc = self.validate(epoch)\n",
    "\n",
    "            # # reduce lr if validation loss plateaus\n",
    "            self.scheduler.step(-valid_acc)\n",
    "\n",
    "            is_best = valid_acc > self.best_valid_acc\n",
    "            msg1 = \"train loss: {:.3f} - train acc: {:.3f} - action: {:.3f}, baseline: {:.3f} reinforce: {:.3f} \"\n",
    "            msg2 = \"- val loss: {:.3f} - val acc: {:.3f}\"\n",
    "            if is_best:\n",
    "                self.counter = 0\n",
    "                msg2 += \" [*]\"\n",
    "            msg = msg1 + msg2\n",
    "            logging.info(msg.format(train_loss, train_acc, loss_act,loss_base,loss_reinf , valid_loss, valid_acc))\n",
    "\n",
    "            # check for improvement\n",
    "            if not is_best:\n",
    "                self.counter += 1\n",
    "            if self.counter > self.config.train_patience:\n",
    "                logging.info(\"[!] No improvement in a while, stopping training.\")\n",
    "                return\n",
    "            self.best_valid_acc = max(valid_acc, self.best_valid_acc)\n",
    "            self.save_checkpoint({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state\": self.model.state_dict(),\n",
    "                \"optim_state\": self.optimizer.state_dict(),\n",
    "                \"best_valid_acc\": self.best_valid_acc,\n",
    "            },\n",
    "                is_best)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Train the model for 1 epoch of the training set.\n",
    "\n",
    "        An epoch corresponds to one full pass through the entire\n",
    "        training set in successive mini-batches.\n",
    "\n",
    "        This is used by train() and should not be called manually.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        losses_action = AverageMeter()\n",
    "        losses_reinforce = AverageMeter()\n",
    "        losses_baseline = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "\n",
    "        tic = time.time()\n",
    "        with tqdm(total=self.num_train) as pbar:\n",
    "            for i, (x, y) in enumerate(self.train_loader):\n",
    "\n",
    "                loss, acc, preds, locs, imgs, loss_action ,loss_baseline, loss_reinforce  = self.one_batch(x, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # compute gradients and update SGD\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # store\n",
    "                losses.update(loss.item(), x.size()[0])\n",
    "                losses_reinforce.update(loss_reinforce.item(), x.size()[0])\n",
    "                losses_baseline.update(loss_baseline.item(), x.size()[0])\n",
    "                losses_action.update(loss_action.item(), x.size()[0])\n",
    "                accs.update(acc.item(), x.size()[0])\n",
    "\n",
    "                # measure elapsed time\n",
    "                toc = time.time()\n",
    "                batch_time.update(toc - tic)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    (\"{:.1f}s - loss: {:.3f} - acc: {:.3f}\".format((toc - tic), loss.item(), acc.item())))\n",
    "\n",
    "                batch_size = x.shape[0]\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "                # log to tensorboard\n",
    "                if self.config.use_tensorboard:\n",
    "                    iteration = epoch * len(self.train_loader) + i\n",
    "                    log_value(\"train_loss\", losses.avg, iteration)\n",
    "                    log_value(\"train_acc\", accs.avg, iteration)\n",
    "\n",
    "            return losses.avg, accs.avg, losses_action.avg,losses_baseline.avg,losses_reinforce.avg\n",
    "\n",
    "    def one_batch(self, x, y):\n",
    "        # initialize location vector and hidden state\n",
    "        batch_size = x.shape[0]\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "        imgs = []\n",
    "        locs = []\n",
    "        means = []\n",
    "        baselines = []\n",
    "        locations = []\n",
    "        l_t = self.model.reset(batch_size, self.device)\n",
    "        locs.append(l_t[0:9])\n",
    "        for t in range(self.num_glimpses - 1):\n",
    "            # forward pass through model\n",
    "            h_t, l_t, b_t, mean_t = self.model(x, l_t)\n",
    "\n",
    "            # save locs for plotting\n",
    "            locs.append(l_t[0:9])\n",
    "            locations.append(l_t)\n",
    "            baselines.append(b_t)\n",
    "            means.append(mean_t)\n",
    "\n",
    "        # last iteration\n",
    "        _, _, _, probabilities, _ = self.model(x, l_t, last=True)\n",
    "\n",
    "        # save locs and images for plotting\n",
    "        imgs.append(x[0:9])\n",
    "\n",
    "        # convert list to tensors and reshape\n",
    "        #TODO verify the transpoe\n",
    "        baselines = torch.stack(baselines).transpose(1, 0)\n",
    "        means = torch.stack(means).transpose(1, 0)\n",
    "        locations = torch.stack(locations).transpose(1, 0)\n",
    "\n",
    "        # calculate reward\n",
    "        predicted = torch.argmax(probabilities, 1)\n",
    "        R = (predicted.detach() == y).float()\n",
    "        #print(f\"Act:  {np.bincount(y.numpy())}\")\n",
    "        #print(f\"Pred: {np.bincount(predicted.numpy())}\")\n",
    "        #print(f\"Base: {baselines.sum(dim=0)}\")\n",
    "        #print(f\"R:     {R.sum()}\")\n",
    "        #print(\"---------------\")\n",
    "        # either 1 (if correct) or 0\n",
    "        R = R.unsqueeze(1).repeat(1, self.num_glimpses-1)\n",
    "\n",
    "        # compute losses for differentiable modules\n",
    "        # smaller, better, no need invert for nll\n",
    "        loss_action = F.nll_loss(probabilities, y)\n",
    "\n",
    "        loss_baseline = F.mse_loss(baselines, R)\n",
    "\n",
    "        # compute reinforce loss\n",
    "\n",
    "        # todo NEGATE reinforce loss?\n",
    "        adjusted_reward = R - baselines.detach()\n",
    "\n",
    "        adjusted_reward=adjusted_reward.repeat(1, 2).reshape(self.config.batch_size,-1,2).detach()\n",
    "        probs = Normal(means, self.model.locator.std).log_prob(locations)\n",
    "        # summed over timesteps and averaged across batch\n",
    "        loss_reinforce = torch.sum(-probs * adjusted_reward, dim=1).sum(dim = 1)\n",
    "        loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "\n",
    "        #TODO LOGITS directly?\n",
    "        # sum up into a hybrid loss\n",
    "        #TODO super high loss with other sensor\n",
    "        loss = loss_action + loss_baseline + loss_reinforce * self.config.reward_multi\n",
    "\n",
    "        # compute accuracy\n",
    "        correct = (predicted == y).float()\n",
    "        acc = 100 * (correct.sum() / len(y))\n",
    "\n",
    "        return loss, acc, predicted, locs, imgs, loss_action ,loss_baseline, loss_reinforce\n",
    "\n",
    "    def __save_images_if_plotting(self, epoch, i, locs, imgs,y):\n",
    "        # dump the glimpses and locs\n",
    "        if (epoch % self.config.plot_freq == 0) and (i == 0):\n",
    "            #print(y)\n",
    "            imgs = [g.cpu().data.numpy().squeeze() for g in imgs]\n",
    "            locs = [l.cpu().data.numpy() for l in locs]\n",
    "            pickle.dump(imgs, open(self.plot_dir + \"g_{}.p\".format(epoch + 1), \"wb\"))\n",
    "            pickle.dump(locs, open(self.plot_dir + \"l_{}.p\".format(epoch + 1), \"wb\"))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch):\n",
    "        \"\"\"Evaluate the RAM model on the validation set.\n",
    "        \"\"\"\n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "        # TODO check\n",
    "        self.model.eval()\n",
    "\n",
    "        for i, (x, y) in enumerate(self.valid_loader):\n",
    "            # 3, 3, 0, 2, 3, 0, 1, 1, 1\n",
    "            loss, acc, preds, locs, imgs, _,_,_ = self.one_batch(x, y)\n",
    "            self.__save_images_if_plotting(epoch, i, locs, imgs,y)\n",
    "            # store\n",
    "            losses.update(loss.item(), x.size()[0])\n",
    "            accs.update(acc.item(), x.size()[0])\n",
    "\n",
    "            # log to tensorboard\n",
    "            if self.config.use_tensorboard:\n",
    "                iteration = epoch * len(self.valid_loader) + i\n",
    "                log_value(\"valid_loss\", losses.avg, iteration)\n",
    "                log_value(\"valid_acc\", accs.avg, iteration)\n",
    "\n",
    "        return losses.avg, accs.avg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Test the RAM model.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        preds = []\n",
    "\n",
    "        # load the best checkpoint\n",
    "        self.load_checkpoint(best=self.best)\n",
    "        # TODO check\n",
    "        self.model.eval()\n",
    "\n",
    "        for i, (x, y) in enumerate(self.test_loader):\n",
    "            loss, acc, predictions, locs, imgs,_,_,_ = self.one_batch(x, y)\n",
    "\n",
    "            correct += sum(predictions == y)\n",
    "            preds.append(predictions)\n",
    "        perc = (100.0 * correct) / (self.num_test)\n",
    "        error = 100 - perc\n",
    "\n",
    "        logging.info(\"[*] Test Acc: {}/{} ({:.2f}% - {:.2f}%)\".format(\n",
    "            correct, self.num_test, perc, error))\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def save_checkpoint(self, state, is_best):\n",
    "        \"\"\"Saves a checkpoint of the model.\n",
    "\n",
    "        If this model has reached the best validation accuracy thus\n",
    "        far, a separate file with the suffix `best` is created.\n",
    "        \"\"\"\n",
    "        filename = self.config.model_name + \"_ckpt.pth.tar\"\n",
    "        ckpt_path = os.path.join(self.config.ckpt_dir, filename)\n",
    "        torch.save(state, ckpt_path)\n",
    "        if is_best:\n",
    "            filename = self.config.model_name + \"_model_best.pth.tar\"\n",
    "            shutil.copyfile(ckpt_path, os.path.join(self.config.ckpt_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, best=False):\n",
    "        \"\"\"Load the best copy of a model.\n",
    "        Args:\n",
    "            best: if set to True, loads the best model.\n",
    "        \"\"\"\n",
    "        logging.info(\"[*] Loading model from {}\".format(self.config.ckpt_dir))\n",
    "\n",
    "        filename = self.config.model_name + \"_ckpt.pth.tar\"\n",
    "        if best:\n",
    "            filename = self.config.model_name + \"_model_best.pth.tar\"\n",
    "        ckpt_path = os.path.join(self.config.ckpt_dir, filename)\n",
    "        logging.info(os.path.abspath(ckpt_path))\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # load variables from checkpoint\n",
    "        self.start_epoch = ckpt[\"epoch\"]\n",
    "        self.best_valid_acc = ckpt[\"best_valid_acc\"]\n",
    "        self.model.load_state_dict(ckpt[\"model_state\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
    "\n",
    "        if best:\n",
    "            logging.info(\n",
    "                \"[*] Loaded {} checkpoint @ epoch {} \"\n",
    "                \"with best valid acc of {:.3f}\".format(\n",
    "                    filename, ckpt[\"epoch\"], ckpt[\"best_valid_acc\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            logging.info(\"[*] Loaded {} checkpoint @ epoch {}\".format(filename, ckpt[\"epoch\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(config):\n",
    "    prepare_dirs(config)\n",
    "    torch.random.manual_seed(config.random_seed)\n",
    "    if config.use_gpu:\n",
    "        torch.cuda.manual_seed(config.random_seed)\n",
    "\n",
    "    locator = DatasetLocator(config)\n",
    "    # instantiate data loaders\n",
    "    if config.is_train:\n",
    "        train_loader = locator.data_loader(DatasetType.TRAIN)\n",
    "        valid_loader = locator.data_loader(DatasetType.VALID)\n",
    "        dloader = (train_loader,valid_loader)\n",
    "    else:\n",
    "        dloader = locator.data_loader(DatasetType.TEST)\n",
    "\n",
    "    trainer = Trainer(config, dloader)\n",
    "\n",
    "    # either train\n",
    "    if config.is_train:\n",
    "        trainer.train()\n",
    "    # or load a pretrained model and test\n",
    "    else:\n",
    "        trainer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = logging.INFO\n",
    "logging.basicConfig(level=log_level, format='%(name)-12s %(levelname)-8s %(message)s',\n",
    "                datefmt='%m-%d %H:%M')\n",
    "fh = logging.FileHandler('run.log')\n",
    "fh.setLevel(log_level)\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "config = Config()\n",
    "main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
