{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmdlWwrndNyR"
   },
   "source": [
    "# Visual Attention Models for Medical Image Analysis\n",
    "\n",
    "#### Anton Karakochev (s0553324@htw-berlin.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependencies using pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchviz matplotlib numpy tqdm scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M39GsUddNyT"
   },
   "source": [
    "## 1. Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfM_PNNQdNyV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "import skimage.measure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import unittest\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lt3_1w6PdNyh"
   },
   "outputs": [],
   "source": [
    "#parameters, just as global variables for convenience\n",
    "\n",
    "# glimpse sensor\n",
    "global_glimpse_sensor_n = 3\n",
    "global_glimpse_senor_patch_size = 12\n",
    "global_glimpse_sensor_scale = 2\n",
    "\n",
    "# glimpse network\n",
    "global_glimpse_network_output_size = 28\n",
    "global_glimpse_network_hidden_layer_size = 32\n",
    "\n",
    "# core network\n",
    "global_lstm_hidden_size = 28\n",
    "global_lstm_num_layers = 2\n",
    "\n",
    "# location network\n",
    "global_std_dev = 0.18\n",
    "\n",
    "# action network\n",
    "global_num_classes = 10\n",
    "\n",
    "# training\n",
    "global_num_epochs = 500\n",
    "global_batch_size = 50\n",
    "global_learning_rate = 0.001\n",
    "global_model_patience = 100\n",
    "\n",
    "# dataset\n",
    "#note that when using 'augmented-medical the number of output classes is 4'\n",
    "#one of 'mnist', 'augmented', 'transformed', 'augmented-medical'\n",
    "global_dataset_name = 'transformed'\n",
    "\n",
    "global_num_glimpses = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputs"
    ]
   },
   "outputs": [],
   "source": [
    "# model name, used for storage and checkpointing\n",
    "global_model_name = f\"{global_dataset_name}-{global_num_epochs}-{global_glimpse_sensor_n}-{global_glimpse_senor_patch_size}x{global_glimpse_senor_patch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HR1U5-tUdNyq",
    "outputId": "2451fe8a-8a8e-42a7-9baa-dbcc154b4027"
   },
   "outputs": [],
   "source": [
    "#if gpu is available, use it \n",
    "global_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#set seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert(global_device.type == \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(images, output_dim):\n",
    "    \"\"\"\n",
    "    Combines the given images into a single image of output_dim size. Combinations are done randomly and \n",
    "    overlapping is possible. Images will always be within bounds completely.\n",
    "    \"\"\"\n",
    "    np_images = np.array(images)\n",
    "    input_dim = np_images.shape[-1]\n",
    "    new_image = np.zeros(shape=(output_dim,output_dim), dtype = np.float32)\n",
    "    for image in np_images:\n",
    "        i, j = np.random.randint(0, output_dim  - input_dim, size = 2)\n",
    "        new_image[i:i+input_dim, j:j+input_dim] = image\n",
    "    return new_image\n",
    "\n",
    "class AugmentedMedicalMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Augmented mnist meant to mimic whole-slide-images of tumor cells.\n",
    "    9's represent cancer cells. There are 4 different labels, based on the number of 9's:\n",
    "    \n",
    "    zero 9's          - no cancer\n",
    "    one 9             - isolated tumor cell\n",
    "    two 9's           - micro-metastasis \n",
    "    three or more 9's - macro-metastasis\n",
    "    \n",
    "    Each image contains between 3 and 10 cells at random, which may be overlapping.\n",
    "    It consists of 5000 items of each category(total 20.000) for training and 500(2.000) of each for testing\n",
    "    of size 256 x 256. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 root_dir,\n",
    "                 train,\n",
    "                 mnist_transform = None,\n",
    "                 transform = None, \n",
    "                 total_train = 20000,\n",
    "                 total_test = 2000,\n",
    "                 n_partitions_test = 1,\n",
    "                 n_partitions_train = 5):\n",
    "        \n",
    "        self.mnist_transform = mnist_transform\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.total = total_train if self.train else total_test\n",
    "        self.n_partitions_test  = n_partitions_test\n",
    "        self.n_partitions_train = n_partitions_train\n",
    "        self.dir = os.path.join(root_dir,\"MEDNIST\", \"train\" if train else \"test\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.__create_dataset_if_needed()\n",
    "                \n",
    "        self.__load_data()\n",
    "        \n",
    "    def __dataset_exists(self):\n",
    "        # mkdir if not exists\n",
    "        os.makedirs(self.dir, exist_ok = True)\n",
    "        len_files = len(os.listdir(self.dir)) \n",
    "        if len_files > 0:\n",
    "            print(\"Data existing, skipping creation.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Dataset missing. Creating...\")\n",
    "        return False\n",
    "            \n",
    "    \n",
    "    def __get_cell_counts(self, items_per_class_count, class_index):\n",
    "        # exclusive\n",
    "        max_items = 11\n",
    "        min_number_of_cells = 3\n",
    "        # 0,1,2,3+ for no tumor cells, isolated tumor cells, \n",
    "        # micro-metastasis and macro-metastasis respectively\n",
    "        num_tumor_cells = class_index if class_index != 3 else np.random.randint(3, max_items) \n",
    "\n",
    "        num_healthy_cells = max_items - num_tumor_cells\n",
    "        if num_healthy_cells + num_tumor_cells < min_number_of_cells:\n",
    "            num_healthy_cells = min_number_of_cells - num_tumor_cells\n",
    "\n",
    "        return (num_tumor_cells, num_healthy_cells)\n",
    "            \n",
    "    def __generate_for_class(self,\n",
    "                                   items,\n",
    "                                   items_per_class_count,\n",
    "                                   class_index,\n",
    "                                   uid,\n",
    "                                   all_tumor_cell_images,\n",
    "                                   all_healthy_cell_images):\n",
    "        for _ in range(items_per_class_count):\n",
    "            num_tumors, num_healthy = self.__get_cell_counts(items_per_class_count, class_index)\n",
    "\n",
    "            healthy_idxs = np.random.randint(0,len(all_healthy_cell_images), num_healthy)\n",
    "            tumor_idxs   = np.random.randint(0,len(all_tumor_cell_images), num_tumors)\n",
    "\n",
    "            cells = np.vstack((all_healthy_cell_images[healthy_idxs], all_tumor_cell_images[tumor_idxs]))\n",
    "            image = combine_images(cells, 256)\n",
    "            image = np.expand_dims(image, axis = 0)\n",
    "            self.data.append(image)\n",
    "            self.labels.append(class_index)\n",
    "            uid += 1\n",
    "        return uid\n",
    "            \n",
    "    def __create_dataset_if_needed(self):\n",
    "        if self.__dataset_exists():\n",
    "            return \n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # in how many partitions to split dataset creation\n",
    "        partitions_count = 10\n",
    "        \n",
    "        # number of classes in output (fixed)\n",
    "        num_classes = 4\n",
    "        \n",
    "        mnist = torchvision.datasets.MNIST(root ='./data',\n",
    "                                           train = self.train,\n",
    "                                           download = True,\n",
    "                                           transform = self.mnist_transform)\n",
    "        \n",
    "        mnist_loader = iter(torch.utils.data.DataLoader(mnist, \n",
    "                                                        batch_size = int(self.total/partitions_count), \n",
    "                                                        shuffle = True, \n",
    "                                                        num_workers = 0))\n",
    "        uid = 0\n",
    "        \n",
    "        for _ in range(partitions_count):\n",
    "            batch, mnist_labels = mnist_loader.next()\n",
    "            # 9's represent tumors\n",
    "            all_tumor_cell_images = batch[mnist_labels == 9]\n",
    "            # everything else except 6's healthy cells\n",
    "            all_healthy_cell_images = batch[(mnist_labels != 9) & (mnist_labels != 6)]\n",
    "            \n",
    "            items_per_class_count = int(self.total/(num_classes * partitions_count))\n",
    "            \n",
    "            for class_index in range(num_classes):\n",
    "                    uid = self.__generate_for_class(class_index, \n",
    "                                                  items_per_class_count,\n",
    "                                                  class_index,\n",
    "                                                  uid,\n",
    "                                                  all_tumor_cell_images,\n",
    "                                                  all_healthy_cell_images)\n",
    "        self.__store()\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, uid):\n",
    "        if torch.is_tensor(uid):\n",
    "            uid = uid.tolist()\n",
    "        label = self.labels[uid]\n",
    "        sample = self.data[uid]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return (sample,label)\n",
    "\n",
    "    def __store(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        \n",
    "        assert(len(self.data) == len(self.labels))\n",
    "        max_index = len(self.data)\n",
    "        partition_size = max_index/n_partitions\n",
    "        for i in range(n_partitions):\n",
    "            start,end =(int(partition_size * i), int(partition_size * (i+1)))\n",
    "            partition = np.array(self.data[start:end])\n",
    "            np.save(os.path.join(self.dir, \"part_\" + str(i)), partition)\n",
    "        \n",
    "        np.save(os.path.join(self.dir, \"labels\"), np.array(self.labels))\n",
    "    \n",
    "    def __load_data(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        data = []\n",
    "        for i in range(n_partitions):\n",
    "            data.append(np.load(os.path.join(self.dir, \"part_\" + str(i)+\".npy\")))\n",
    "        self.data = np.vstack(data)\n",
    "        self.labels = np.load(os.path.join(self.dir, \"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LJPCDjmdNyz"
   },
   "outputs": [],
   "source": [
    "def transformed_mnist_transformation():\n",
    "    def f(image):\n",
    "        np_image = np.array(image)\n",
    "        input_dim = np_image.shape[-1]\n",
    "        new_image = np.zeros(shape=(60,60), dtype = np.float32)\n",
    "        i, j = np.random.randint(0, 60  - input_dim, size = 2)\n",
    "        new_image[i:i+input_dim, j:j+input_dim] = np_image\n",
    "        return new_image\n",
    "    return transforms.Compose(\n",
    "        [torchvision.transforms.Lambda(f),\n",
    "        torchvision.transforms.ToTensor()])\n",
    "\n",
    "def augmented_mnist_transformation():\n",
    "    return transforms.Compose([\n",
    "        torchvision.transforms.RandomAffine(degrees = (-180,180),scale = (0.5,1.0),),\n",
    "        torchvision.transforms.ToTensor()])\n",
    "\n",
    "def dataset(train = True,\n",
    "            name = global_dataset_name):\n",
    "    if name == 'mnist':\n",
    "        transform = torchvision.transforms.ToTensor()\n",
    "    elif name == 'augmented':\n",
    "        transform = augmented_mnist_transformation()\n",
    "    elif name == 'transformed':\n",
    "        transform = transformed_mnist_transformation()\n",
    "    elif name == 'augmented-medical' :\n",
    "        return AugmentedMedicalMNISTDataset(root_dir='.', train = train,mnist_transform = augmented_mnist_transformation())\n",
    "    else:\n",
    "        raise Exception(\"unrecognized dataset name.\")\n",
    "    return torchvision.datasets.MNIST(root='./data', train = train , download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lV2LwhAudNy7"
   },
   "outputs": [],
   "source": [
    "mnist_train   = dataset()\n",
    "train_loader  = torch.utils.data.DataLoader(mnist_train, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "images,labels = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "-YgAccwsdNzB",
    "outputId": "6ed60c57-6830-489a-9d7a-232c66cafa83",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show(images,labels = None):\n",
    "    if labels == None:\n",
    "        labels = [\" \"] * len(images)\n",
    "    elif isinstance(labels,torch.Tensor): \n",
    "        labels = [label.item() for label in labels]\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(25,25))\n",
    "    for (a,image,label) in zip(axes.ravel(),images,labels):\n",
    "        a.imshow(image[0].numpy(), cmap = plt.cm.gray)\n",
    "        a.set_title(label, fontsize=20)\n",
    "\n",
    "    fig.tight_layout()\n",
    "show(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTP4KcWpdNzG"
   },
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-isBN9KdNzI"
   },
   "source": [
    "### 2.1 Glimpse Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMYLC4PodNzJ"
   },
   "outputs": [],
   "source": [
    "def normalized_position(positions, dim):\n",
    "    \"\"\" \n",
    "    Given positions `positions` [0, dim] and image dimension `dim`,\n",
    "    converts them to normalized positions [-1, -1]\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    positions: 2D Tensor (B x 2)\n",
    "    dim: Int\n",
    "    \"\"\"\n",
    "    res = (2 * positions / float(dim)) - 1\n",
    "    return torch.clamp(res,-1,1)\n",
    "\n",
    "def inverse_normalized_position(norm_positions, dim):\n",
    "    \"\"\" \n",
    "    Given normalized positions `norm_positions` [-1,-1] and image dimension `dim`,\n",
    "    converts them to positions within the image [0, dim]\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    norm_positions: 2D Tensor (B x 2)\n",
    "    dim: Int\n",
    "    \"\"\"\n",
    "    res = (((norm_positions + 1) * float(dim)) / 2).int()\n",
    "    return torch.clamp(res,0,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseSensor:\n",
    "    \"\"\"\n",
    "    Extracts retina-like representations of given images.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, scale, patch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        n: Int          - number of patches to extract\n",
    "        patch_size: Int - size of a patch\n",
    "        scale: Int      - scaling factor for each lower resolution \n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def extract_glimpses(self, images, location):\n",
    "        \"\"\"\n",
    "        Extracts a retina like representation of the given images `x`, \n",
    "        centered at `location` and containing `n` resolution patches of size (`patch_size`, `patch_size`), \n",
    "        where each has a `scale` times lower resolution than the previous.\n",
    "        \n",
    "        Currently images of only 1 Channel (greyscale) can be processed, \n",
    "        but extension to coloured images is not hard.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        x: 4D Tensor (B x C x H x W) - The image mini batch\n",
    "        location: 2D Tensor (B x 2)  - normalized positions in images\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        4D Tensor (B x n x scale_size x scale_size ) - extracted glimpses\n",
    "        \"\"\"\n",
    "        \n",
    "        B, C, H ,W = images.shape\n",
    "        l = inverse_normalized_position(location, W)\n",
    "        patches = []\n",
    "        for patch_index in range(self.n):\n",
    "            current_scale = self.scale ** patch_index\n",
    "            size = self.patch_size * current_scale\n",
    "            patch = self.extract_patch(images, l , size)\n",
    "            patches.append(patch)\n",
    "        \n",
    "        # for each patch in patches make sure size is equal to patch_size\n",
    "        patches_square = []\n",
    "        for patch in patches:\n",
    "            size = patch.shape[-1]\n",
    "            stride = int(size / self.patch_size)\n",
    "            squared = F.avg_pool2d(patch, stride)\n",
    "            patches_square.append(squared)\n",
    "        # stack and bring in desired dimensions\n",
    "        return torch.stack(patches_square).squeeze().transpose(0,1)\n",
    "        \n",
    "    def extract_patch(self, images, l , size):\n",
    "        # shift image by half-size, effectively making centered l coordinates into top left coordinates\n",
    "        # and at the same time handling out of bounds problems\n",
    "        half_size = math.ceil (size / 2)\n",
    "        images = F.pad(images,(half_size, half_size, half_size, half_size))\n",
    "        \n",
    "        # x,y tuples\n",
    "        start = l\n",
    "        # shifted x,y tuples\n",
    "        end = start + size\n",
    "        \n",
    "        n_images = images.shape[0]\n",
    "        patch = []\n",
    "        for i in range(n_images):\n",
    "            patch_i = images[i, :, start[i, 1] : end[i, 1], start[i, 0] : end[i, 0]]\n",
    "            patch.append(patch_i)\n",
    "        return torch.stack(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "colab_type": "code",
    "id": "MsDXhBaYdNzT",
    "outputId": "69513aea-2345-49f5-c369-3601e8759a09",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot some images by looking at the center\n",
    "gs = GlimpseSensor(global_glimpse_sensor_n, global_glimpse_sensor_scale, global_glimpse_senor_patch_size)\n",
    "glimpses = gs.extract_glimpses(images,torch.zeros((images.shape[0],2)))\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    scales = [(gs.scale ** n_i) * gs.patch_size for n_i in range(gs.n)]\n",
    "    l = [f\"class: {labels[i].item()}, {images[i].shape[1]} x {images[i].shape[2]}\"] + [f\"{scale} x {scale}\" for scale in scales]\n",
    "    to_show = [images[i]] + [g.unsqueeze(0) for g in glimpses[i,:,:,:]]\n",
    "    show(to_show,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[ 1.0633, -0.0473],\n",
    "        [-0.0921,  0.0515],\n",
    "        [ 0.1845,  0.1011]])\n",
    "gs.extract_glimpses(images[0:2],p[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zV38KcPXdNzX"
   },
   "source": [
    "### 2.2 Glimpse Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5hCXvRpdNzZ"
   },
   "outputs": [],
   "source": [
    "class GlimpseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a location and encodes a `where` and `what` feature representation, which are combined together. \n",
    "    Consists of a combination of convolutional, max pooling and fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n, \n",
    "                 patch_size, \n",
    "                 scale,\n",
    "                 glimpse_output_size,\n",
    "                 hidden_layer_size\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        n: Int                   - number of patches to extract\n",
    "        patch_size: Int          - size of a patch\n",
    "        scale: Int               - scaling factor for each lower resolution \n",
    "        glimpse_output_size: Int - size of output of last fully connected layer\n",
    "        hidden_layer_size: Int   - size of hidden layer (number of units)\n",
    "        \"\"\"\n",
    "        super(GlimpseNetwork, self).__init__()\n",
    "        self.glimpse_sensor = GlimpseSensor(n = n, scale = scale, patch_size = patch_size)\n",
    "        self.glimpse_output_size = glimpse_output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "                \n",
    "        # what \n",
    "        \n",
    "        # padding of 1, to ensure same dimensions\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.glimpse_sensor.n, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 16, track_running_stats = True)\n",
    "         \n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 3, stride = 3, padding = 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 3, stride = 3, padding = 1)\n",
    "        \n",
    "        # TODO size in and out\n",
    "        # W * H of previous layer * depth\n",
    "        self.fc1 = nn.Linear(in_features = 128, out_features = self.glimpse_output_size)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = self.glimpse_output_size, track_running_stats = True)\n",
    "        \n",
    "        # where\n",
    "        # in_features = 2, loc is a tuple of (x,y)\n",
    "        self.loc_fc1 = nn.Linear(in_features = 2, out_features = self.hidden_layer_size)\n",
    "        self.loc_fc2 = nn.Linear(in_features = self.hidden_layer_size, out_features = self.glimpse_output_size)\n",
    "        \n",
    "    def forward(self, x, location):\n",
    "        glimpses = self.glimpse_sensor.extract_glimpses(x, location)\n",
    "        \n",
    "        # what\n",
    "        # 3 conv layers\n",
    "        h = self.conv1(glimpses)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self.max_pool1(h)\n",
    "        \n",
    "        h = F.relu(self.conv3(h))\n",
    "        \n",
    "        h = self.max_pool2(h) \n",
    "        \n",
    "        # flatten\n",
    "        # keep batch dimension and determine other one automatically\n",
    "        h = h.view(x.shape[0],-1)\n",
    "        \n",
    "        # fully connected layers\n",
    "        h = self.fc1(h)\n",
    "        h = self.bn2(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # where\n",
    "        l = self.loc_fc1(location)\n",
    "        l = F.relu(l)\n",
    "        l = self.loc_fc2(l)\n",
    "        \n",
    "        # combine what and where\n",
    "        g = F.relu(h * l)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gW-augEdNzg"
   },
   "source": [
    "### 2.3 Core Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpfu5EoXdNzg"
   },
   "outputs": [],
   "source": [
    "class CoreNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the `what` and `where` feature representation by the Glimpse Network\n",
    "    and a hidden state to produce a new hidden state and prediction.\n",
    "    Consists of 2 LSTM layers stacked on top of each other.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 lstm_hidden_size,\n",
    "                 glimpse_input_size,\n",
    "                 num_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        batch_size:Int          - size of a batch. Used compute the inital hidden state \n",
    "        glimpse_input_size: Int - size of the input. Equal to the output size of the Glimpse Network \n",
    "        lstm_hidden_size: Int   - size of the output and hidden layers\n",
    "        num_layers: Int         - number of stacked LSTM layers\n",
    "        ----\n",
    "        \"\"\"\n",
    "        super(CoreNetwork, self).__init__()\n",
    "        self.glimpse_input_size = glimpse_input_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size \n",
    "        # batch_first = false -> (SEQ x B x Features)\n",
    "        self.stacked_lstm = nn.LSTM(input_size = self.glimpse_input_size,\n",
    "                                    hidden_size = self.lstm_hidden_size,\n",
    "                                    num_layers = self.num_layers,\n",
    "                                    batch_first = False)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # does one step \n",
    "        (prediction, new_hidden) = self.stacked_lstm(x, hidden)\n",
    "        # new_hidden is a tuple of layer state and cell state \n",
    "        # remove channel dimension\n",
    "        return (prediction[0], new_hidden)\n",
    "        \n",
    "    def initial_hidden_state(self):\n",
    "        \"\"\"Initialize hidden state and cell state with zeroes.\"\"\"\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.lstm_hidden_size).to(global_device),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.lstm_hidden_size).to(global_device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNI9kmzDdNzm"
   },
   "source": [
    "### 2.4 Location Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukMXiXXldNzq"
   },
   "outputs": [],
   "source": [
    "class LocationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the hidden state of the Core Network and produces a next location to attend to. \n",
    "    The next location is determined stochastically from a normal distribution \n",
    "    with a fixed deviation and a learned mean.\n",
    "    \n",
    "    Outputs the next location alongside the mean.\n",
    "    \n",
    "    Non-differentiable and needs to be trained with Reinforcment Learning.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                input_size,\n",
    "                std_dev):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        input_size: Int - the input size. Equal to the output size of the Core Network\n",
    "        std_dev: Float  - the fixed standard deviation/variance \n",
    "        \"\"\"\n",
    "        super(LocationNetwork, self).__init__()\n",
    "        # location -> (x, y)\n",
    "        self.fc = nn.Linear(input_size, 2)\n",
    "        self.std_dev = std_dev\n",
    "        \n",
    "    def forward(self, h_t):\n",
    "        #predicts the mean [-1,1] based on the network\n",
    "        mean = torch.tanh(self.fc(h_t.detach()))\n",
    "                                  \n",
    "        normal_dist = Normal(mean, torch.tensor((self.std_dev,self.std_dev)).to(global_device))\n",
    "        \n",
    "        next_location = normal_dist.sample()\n",
    "        log_probs = normal_dist.log_prob(next_location)       \n",
    "        log_probs = torch.sum(log_probs, dim = 1)\n",
    "        return (next_location, log_probs)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDYKb2y8dNzx"
   },
   "outputs": [],
   "source": [
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Incorporates the baseline in the reward function\n",
    "    to reduce the variance of the gradient update.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 output_size):\n",
    "        super(BaselineNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, hidden_t):\n",
    "        baseline_t = F.relu(self.fc(hidden_t.detach()))\n",
    "        return baseline_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_l1kI_jIdNz2"
   },
   "source": [
    "### 2.5 Action Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1lejKRqdNz4"
   },
   "outputs": [],
   "source": [
    "class ActionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the internal state of the Core Network to produce the final output classification.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 num_classes):\n",
    "        super(ActionNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, hidden_t_bottom_layer):\n",
    "        predictions = F.log_softmax(self.fc(hidden_t_bottom_layer), dim = 1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MEwgIMEdNz_"
   },
   "source": [
    "### 2.6 Deep Recurrent Attention Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qEQfi6BdN0B"
   },
   "outputs": [],
   "source": [
    "class DeepRecurrentAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines all networks described above.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 glimpse_sensor_n = global_glimpse_sensor_n, \n",
    "                 glimpse_senor_patch_size = global_glimpse_senor_patch_size, \n",
    "                 glimpse_sensor_scale = global_glimpse_sensor_scale,\n",
    "                 glimpse_network_output_size = global_glimpse_network_output_size,\n",
    "                 glimpse_network_hidden_layer_size = global_glimpse_network_hidden_layer_size,\n",
    "                 lstm_hidden_size = global_lstm_hidden_size,\n",
    "                 lstm_num_layers = global_lstm_num_layers,\n",
    "                 number_of_glimpses = global_num_glimpses,\n",
    "                 batch_size = global_batch_size,\n",
    "                 num_classes = global_num_classes,\n",
    "                 std_dev = global_std_dev\n",
    "                ):\n",
    "        super(DeepRecurrentAttentionModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.std_dev = std_dev\n",
    "        self.number_of_glimpses = number_of_glimpses\n",
    "        \n",
    "        self.glimpse_network = GlimpseNetwork(n = glimpse_sensor_n,\n",
    "                                              patch_size = glimpse_senor_patch_size,\n",
    "                                              scale = glimpse_sensor_scale,\n",
    "                                              glimpse_output_size = glimpse_network_output_size,\n",
    "                                              hidden_layer_size = glimpse_network_hidden_layer_size)\n",
    "        \n",
    "        self.core_network = CoreNetwork(lstm_hidden_size = lstm_hidden_size,\n",
    "                                       glimpse_input_size = glimpse_network_output_size,\n",
    "                                       num_layers = lstm_num_layers,\n",
    "                                       batch_size = batch_size)\n",
    "        \n",
    "        self.location_network = LocationNetwork(input_size = lstm_hidden_size, std_dev = std_dev)\n",
    "        \n",
    "        self.action_network = ActionNetwork(input_size = lstm_hidden_size, num_classes = num_classes)\n",
    "        \n",
    "        self.baseline_network = BaselineNetwork(input_size = lstm_hidden_size, output_size = 1)\n",
    "    \n",
    "    def initial_hidden_state(self):\n",
    "        return self.core_network.initial_hidden_state()\n",
    "    \n",
    "    def initial_glimpse_location(self):\n",
    "        return torch.Tensor(self.batch_size, 2).fill_(0.0).to(global_device)\n",
    "    \n",
    "    def forward(self,\n",
    "                x,\n",
    "                prev_hidden,\n",
    "                prev_location,\n",
    "                also_compute_probabilities = False):\n",
    "        \n",
    "        g_t = self.glimpse_network(x, prev_location.detach())\n",
    "        # add seq dimension\n",
    "        g_t = g_t.unsqueeze(0)\n",
    "        \n",
    "        # hidden_t - top layer output\n",
    "        # total_hidden_t - all layers including cell parameters\n",
    "        (hidden_t, total_hidden_t) = self.core_network(g_t, prev_hidden)\n",
    "        \n",
    "        (location_t, location_mean_t) = self.location_network(hidden_t.detach())\n",
    "        baseline_t = self.baseline_network(hidden_t.detach())\n",
    "        \n",
    "        if also_compute_probabilities:\n",
    "            # takes not the top, but the bottom layer in the paper\n",
    "            probs_t = self.action_network(total_hidden_t[0][0])\n",
    "            return (total_hidden_t, location_t, location_mean_t, baseline_t, probs_t)\n",
    "        else:\n",
    "            return (total_hidden_t, location_t, location_mean_t, baseline_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhznbPaLdN0F"
   },
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P38bdFMydN0G"
   },
   "outputs": [],
   "source": [
    "class RunningAverage(object):\n",
    "    \"\"\"Computes a running average.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def add(self, val, n = 1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGSBofZWdN0K"
   },
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \"\"\"\n",
    "    Entry class, takes care of training, checkpoints and output.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train_loader,\n",
    "                 valid_loader,\n",
    "                 test_loader,\n",
    "                 model_name = global_model_name, \n",
    "                 learning_rate = global_learning_rate,\n",
    "                 num_epochs = global_num_epochs,\n",
    "                 batch_size = global_batch_size,\n",
    "                 num_glimpses = global_num_glimpses,\n",
    "                 checkpoint_dir = \"./models\",\n",
    "                 model_patience = global_model_patience ,\n",
    "                 resume = True\n",
    "                 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_glimpses = num_glimpses\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.best_valid_acc = 0\n",
    "        self.start_epoch = 0\n",
    "        self.resume = resume\n",
    "        self.model_patience = model_patience\n",
    "        self.current_without_improvement = 0\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = global_device\n",
    "        \n",
    "        # loaders\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.num_train = len(train_loader.dataset)\n",
    "        self.num_valid = len(valid_loader.dataset)\n",
    "        self.num_test = len(test_loader.dataset)\n",
    "        \n",
    "        self.model = DeepRecurrentAttentionModel().to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        self.MSELoss = nn.MSELoss()\n",
    "        self.NLLLoss = nn.NLLLoss()\n",
    "        \n",
    "    def train(self):\n",
    "        if self.resume:\n",
    "            self.load_checkpoint(load_best = False)\n",
    "        for epoch in range(self.start_epoch, self.num_epochs):\n",
    "            # switch layers to train mode\n",
    "            self.model.train()\n",
    "            # train for 1 epoch\n",
    "            train_loss, train_acc = self.train_single_epoch()            \n",
    "            print(\"Epoch: {}, avg. loss: {:3.3f}, avg. acc: {:3.3f}\".format((epoch), train_loss, train_acc))\n",
    "            \n",
    "            # evaluate on validation set\n",
    "            valid_loss, valid_acc = self.evaluate(is_test = False, epoch = epoch)\n",
    "\n",
    "            # update best\n",
    "            is_best = valid_acc >  self.best_valid_acc\n",
    "            if is_best:\n",
    "                self.current_without_improvement = 0\n",
    "            else:\n",
    "                self.current_without_improvement += 1\n",
    "            self.best_valid_acc = max(valid_acc, self.best_valid_acc)\n",
    "            \n",
    "            # checkpoint\n",
    "            self.save_checkpoint(\n",
    "                {'epoch': epoch + 1,\n",
    "                 'model_state': self.model.state_dict(),\n",
    "                 'optim_state': self.optimizer.state_dict(),\n",
    "                 'best_valid_acc': self.best_valid_acc,\n",
    "                 },is_best\n",
    "            )\n",
    "            if self.current_without_improvement > self.model_patience:\n",
    "                print(f\"{self.model_patience} epochs without improvement. Stopping.\")\n",
    "                return\n",
    "    \n",
    "    def forward_pass(self, images, labels):\n",
    "        hidden_t = self.model.initial_hidden_state()\n",
    "        location_t = self.model.initial_glimpse_location()\n",
    "        baselines = []\n",
    "        log_probs = []\n",
    "        for glimpse_i in range(self.num_glimpses):\n",
    "            is_last = glimpse_i == self.num_glimpses - 1\n",
    "            if is_last :\n",
    "                (hidden_t, location_t, log_probs_t, baseline_t, probs_t) = self.model(images,\n",
    "                                                                                  hidden_t,\n",
    "                                                                                  location_t, \n",
    "                                                                                  also_compute_probabilities = True)\n",
    "            else:\n",
    "                (hidden_t, location_t, log_probs_t, baseline_t) = self.model(images, hidden_t, location_t)\n",
    "            baselines.append(baseline_t[:,0])\n",
    "            \n",
    "            log_probs.append(log_probs_t)\n",
    " \n",
    "        baselines = torch.stack(baselines).transpose(1, 0)\n",
    "        log_probs = torch.stack(log_probs).transpose(1, 0)\n",
    "         \n",
    "        # compute loss and rewards\n",
    "\n",
    "        # reward\n",
    "        predicted = probs_t.argmax(dim = 1).detach()\n",
    "        rewards = (predicted == labels).float()\n",
    "\n",
    "        rewards = rewards.unsqueeze(1).repeat(1, self.num_glimpses)\n",
    "\n",
    "        # losses \n",
    "        # action\n",
    "        loss_action = self.NLLLoss(probs_t, labels)\n",
    "\n",
    "        # baseline\n",
    "        loss_baseline = self.MSELoss(baselines, rewards)\n",
    "\n",
    "        # reinforcement\n",
    "        adjusted_reward = rewards - baselines.detach()\n",
    "        \n",
    "        #minus because the higher the reward: the better\n",
    "        log_likelihood_rewards = torch.sum( - log_probs * adjusted_reward, dim = 1)\n",
    "        loss_reinforce = torch.mean(log_likelihood_rewards, dim = 0)\n",
    "\n",
    "        loss = loss_action + loss_baseline + loss_reinforce\n",
    "        \n",
    "        #if np.random.randint(1,51,1).item() == 50:\n",
    "        #    print(f\"Loss = [A:{loss_action:+7.3},B:{loss_baseline:+7.3},R:{loss_reinforce:+7.3}] - Weight sum = [A:{self.model.action_network.fc.weight.detach().sum():+7.3},B:{self.model.baseline_network.fc.weight.detach().sum():+7.3},R:{self.model.location_network.fc.weight.detach().sum():+7.3}]\")\n",
    "        \n",
    "        return (predicted, loss)\n",
    "    \n",
    "    def train_single_epoch(self):\n",
    "        epoch_losses = RunningAverage()\n",
    "        epoch_accuracies = RunningAverage()\n",
    "        \n",
    "        with tqdm(total = self.num_train) as pbar:\n",
    "            start = time.time()\n",
    "            # images - mini batch\n",
    "            for i, (images, labels) in enumerate(self.train_loader): \n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                # forward pass\n",
    "                (predictions,loss) = self.forward_pass(images, labels)\n",
    "                # backward and optimize\n",
    "                #print(self.model.action_network.fc.weight.grad)\n",
    "                # zero out current gradiate accumulators\n",
    "                self.optimizer.zero_grad()\n",
    "                # propagate new ones\n",
    "                loss.backward()\n",
    "                \n",
    "                #print(self.model.baseline_network.fc.weight.grad)    \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute accuracy\n",
    "                correct = (predictions == labels).float()\n",
    "                acc = 100 * (correct.sum() / len(labels))\n",
    "                \n",
    "                elapsed = time.time() - start\n",
    "                # store\n",
    "                epoch_losses.add(loss.item(), len(images))\n",
    "                epoch_accuracies.add(acc.item(), len(images))\n",
    "                \n",
    "                pbar.set_description((\"{:.1f}s loss: {:.3f} , acc: {:3.3f}\"\n",
    "                                      .format(elapsed, loss.item(), acc.item())))\n",
    "                pbar.update(self.batch_size)\n",
    "        return (epoch_losses.avg, epoch_accuracies.avg)\n",
    "        \n",
    "    def evaluate(self, is_test, epoch = None):\n",
    "        loader = self.test_loader if is_test else self.valid_loader\n",
    "        name = \"test\" if is_test else f\"Epoch: {epoch}, validation\"\n",
    "        \n",
    "        n = self.num_test if is_test else self.num_valid\n",
    "        if is_test:\n",
    "            self.load_checkpoint(load_best = True)\n",
    "        \n",
    "        # switch layers to evaluation mode    \n",
    "        self.model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        losses = RunningAverage()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #with tqdm(total = self.num_train) as pbar:\n",
    "            start = time.time()\n",
    "            # images - mini batch\n",
    "            for i, (images, labels) in enumerate(loader):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                # forward pass\n",
    "                (predictions, loss) = self.forward_pass(images, labels)\n",
    "                \n",
    "                # compute accuracy\n",
    "                correct += (predictions == labels).int().sum()\n",
    "                losses.add(loss.item(), len(images))\n",
    "                elapsed = time.time() - start\n",
    "                \n",
    "                # store\n",
    "                losses.add(loss.item(), len(images))\n",
    "\n",
    "                #pbar.set_description((\"{:.1f}s Evaluating... \".format(elapsed)))\n",
    "                #pbar.update(self.batch_size)\n",
    "        accuracy = (100. * correct) / n\n",
    "        print(f'[*] {name} - {correct}/{n} Correct, Acc: ({accuracy:3.2f}%), Err({(100-accuracy):3.2f}%)')\n",
    "        return (losses.avg, accuracy)\n",
    "    \n",
    "    def save_checkpoint(self, state, is_best):\n",
    "        filename = self.model_name + '_ckpt'\n",
    "        ckpt_path = os.path.join(self.checkpoint_dir, filename)\n",
    "        os.makedirs(self.checkpoint_dir,exist_ok = True)\n",
    "        torch.save(state, ckpt_path)\n",
    "                    \n",
    "        if is_best:\n",
    "            filename = self.model_name + '_model_best'\n",
    "            shutil.copyfile(ckpt_path, os.path.join(self.checkpoint_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, load_best):\n",
    "        print(\"[*] Loading model from {}\".format(self.checkpoint_dir))\n",
    "\n",
    "        filename = self.model_name + '_ckpt'\n",
    "        if load_best:\n",
    "            filename = self.model_name + '_model_best'\n",
    "        ckpt_path = os.path.join(self.checkpoint_dir, filename)\n",
    "        if  not os.path.exists(ckpt_path):\n",
    "            print(\"No checkpoint found, starting from scratch\")\n",
    "            return\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        # load variables from checkpoint\n",
    "        self.best_valid_acc = ckpt['best_valid_acc']\n",
    "        #self.start_epoch = ckpt['epoch']\n",
    "        self.model.load_state_dict(ckpt['model_state'])\n",
    "        self.optimizer.load_state_dict(ckpt['optim_state'])\n",
    "\n",
    "        if load_best:\n",
    "            print(\n",
    "                \"[*] Loaded {} checkpoint @ epoch {} \"\n",
    "                \"with best valid acc of {:.3f}\".format(\n",
    "                    filename, ckpt['epoch'], ckpt['best_valid_acc'])\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"[*] Loaded {} checkpoint @ epoch {}\".format(\n",
    "                    filename, ckpt['epoch'])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8da835af50a743e79a3ca6d72b13c2fa",
      "310e691438e94a72ba0597ca0348dac2",
      "05986c728e6e4e28ab042c60e9e321b8",
      "b730e82da031476695af610084cb3531",
      "4083770cefc6482b95e990403e9f393b",
      "91ff2de9b3d446b59114c23d3985bdfc",
      "51a7999fab27419ebea3ca78b0587cc0",
      "11cb8e4f9ced499b83bc92cb11cd22dd"
     ]
    },
    "colab_type": "code",
    "id": "hOdbH7UkdN0P",
    "outputId": "aaefec4b-ee70-4075-991a-4142c5babf67",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_train_data = dataset()\n",
    "mnist_train_length = int(len(mnist_train_data) * 0.9)\n",
    "mnist_valid_length = len(mnist_train_data) - mnist_train_length\n",
    "\n",
    "(mnist_train, mnist_valid) = torch.utils.data.random_split(mnist_train_data,(mnist_train_length, mnist_valid_length))\n",
    "mnist_test = dataset(train = False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size = global_batch_size,\n",
    "                                          shuffle = True, num_workers = 2)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size = global_batch_size,\n",
    "                                          shuffle = False, num_workers = 2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_valid, batch_size = global_batch_size,\n",
    "                                          shuffle = False, num_workers = 2)\n",
    "\n",
    "trainer = Train(train_loader = train_loader,\n",
    "      valid_loader = valid_loader,\n",
    "      test_loader = test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plot backward graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "imgs,lbls = iter(train_loader).next()\n",
    "(_, loss) = trainer.forward_pass(imgs,lbls)\n",
    "dot = make_dot(loss,dict(trainer.model.named_parameters()))\n",
    "dot.render('./tmp/dot-graph.gv', view = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lj9oTT3bdN0X"
   },
   "source": [
    "## 4. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZJYgsAadN0Y",
    "scrolled": true
   },
   "source": [
    "trainer.evaluate(is_test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot glimpses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images, plot_labels = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def glimpse_locations_and_prediction(model,images, labels):\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            images = images\n",
    "            labels = labels\n",
    "\n",
    "            hidden_t = model.initial_hidden_state()\n",
    "            location_t = model.initial_glimpse_location()\n",
    "\n",
    "            locations = []\n",
    "            for glimpse_i in range(model.number_of_glimpses):\n",
    "                is_last = glimpse_i == model.number_of_glimpses - 1\n",
    "                if is_last :\n",
    "                    (hidden_t, location_t, mean_t, baseline_t, probs_t) = model(images,\n",
    "                                                                                      hidden_t,\n",
    "                                                                                      location_t, \n",
    "                                                                                      also_compute_probabilities = True)\n",
    "                else:\n",
    "                    (hidden_t, location_t, mean_t, baseline_t) = model(images, hidden_t, location_t)\n",
    "                \n",
    "                locations.append(location_t.unsqueeze(0))\n",
    "            prediction = probs_t.argmax(dim =1)\n",
    "    return (prediction, torch.cat(locations).transpose(0,1))  \n",
    "\n",
    "def plot_glimpses(imgs, preds, locs):\n",
    "    for (image,norm_locs) in zip(imgs,locs):\n",
    "        fig,ax = plt.subplots(1,len(norm_locs),figsize=(20,20))\n",
    "\n",
    "        for norm_loc, axis in zip(norm_locs,ax.ravel()):\n",
    "            # center position\n",
    "            loc = inverse_normalized_position(norm_loc,images.shape[-1])\n",
    "            # top left\n",
    "            loc = loc - global_glimpse_senor_patch_size/2\n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle(loc,global_glimpse_senor_patch_size,global_glimpse_senor_patch_size,linewidth=1,edgecolor='r',facecolor='none')\n",
    "            axis.add_patch(rect)\n",
    "            axis.imshow(image.numpy(), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "pedictions, locations = glimpse_locations_and_prediction(trainer.model, plot_images, plot_labels)\n",
    "\n",
    "imgs  = plot_images[0:5].squeeze(1)\n",
    "preds = pedictions[0:5]\n",
    "locs  = locations[0:5]\n",
    "plot_glimpses(imgs,preds,locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQH6-BF1dN0g"
   },
   "source": [
    "## 6. Unit Tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQA48fVIdN0h",
    "scrolled": true
   },
   "source": [
    "class TestGlimpseSensor(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Various unit tests for the glimpse sensor, since it is mainly written in 'native' python and pytorch,\n",
    "    which makes it more error prone.\n",
    "    \"\"\"\n",
    "    def assertTorchEqual(self,a , b):\n",
    "        return self.assertTrue(torch.all(a.eq(b)))\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.gs_1 = GlimpseSensor(n = 1, scale = 2, patch_size = 3)\n",
    "        self.gs_4 = GlimpseSensor(n = 4, scale = 2, patch_size = 3)\n",
    "        self.images = torch.arange(300).reshape((3,1,10,10))\n",
    "    \n",
    "    def test_extract_glimpses_patch_from_center(self):\n",
    "        position = normalized_position(torch.tensor(((5,5),(5,5),(5,5))), 10)\n",
    "        actual = self.gs_1.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_extract_glimpses_patch_top_left(self):\n",
    "        position = normalized_position(torch.tensor(((0,0),(0,0),(0,0))), 10)\n",
    "        actual = self.gs_1.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_extract_glimpses_patch_left(self):\n",
    "        position = normalized_position(torch.tensor(((5,0),(5,0),(5,0))), 10)\n",
    "        actual = self.gs_1.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)\n",
    "        \n",
    "    def test_extract_glimpses_patch_bottom_left(self):\n",
    "        position = normalized_position(torch.tensor(((10,10),(10,10),(10,10))), 10)\n",
    "        actual = self.gs_1.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_extract_glimpses_patch_bottom(self):\n",
    "        position = normalized_position(torch.tensor(((10,5),(10,5),(10,5))), 10)\n",
    "        actual = self.gs_1.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)\n",
    "        \n",
    "    #tests for multiple glimpses\n",
    "    def test_extract_glimpses_patch_multiple_from_center(self):\n",
    "        position = normalized_position(torch.tensor(((5,5),(5,5),(5,5))), 10)\n",
    "        actual = self.gs_4.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)\n",
    "        \n",
    "    def test_extract_glimpses_patch_multiple_top_left(self):\n",
    "        position = normalized_position(torch.tensor(((0,0),(0,0),(0,0))), 10)\n",
    "        actual = self.gs_4.extract_glimpses(self.images,position)\n",
    "        self.assertTrue(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNormalizedPositions(unittest.TestCase):\n",
    "    \n",
    "    def assertTorchEqual(self,a , b):\n",
    "        return self.assertTrue(torch.all(a.eq(b)))\n",
    "    \n",
    "    def test_normalized_position_center(self):\n",
    "        actual = normalized_position(torch.tensor((50,50)),100)\n",
    "        expected = torch.tensor((0.0,0.0))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_normalized_position_top_left(self):\n",
    "        actual = normalized_position(torch.tensor((0,0)),100)\n",
    "        expected = torch.tensor((-1,-1))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_normalized_position_bottom_right(self):\n",
    "        actual = normalized_position(torch.tensor((100,100)), 100)\n",
    "        expected = torch.tensor((1,1))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_normalized_position_other_location(self):\n",
    "        actual = normalized_position(torch.tensor((25,75)),100)\n",
    "        expected = torch.tensor((-0.5,0.5))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "    \n",
    "    def test_normalized_position_out_of_bound(self):\n",
    "        actual = normalized_position(torch.tensor((-10,120)),100)\n",
    "        expected = torch.tensor((-1,1))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_normalized_position_multiple(self):\n",
    "        actual = normalized_position(torch.tensor(((25,75),(75,75),(50,50))),100)\n",
    "        expected = torch.tensor(((-0.5,0.5),(0.5,0.5),(0.0,0.0)))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    #inverse\n",
    "    \n",
    "    def test_inverse_normalized_position_center(self):\n",
    "        actual = inverse_normalized_position(torch.tensor((0.0,0.0)),100)\n",
    "        expected = torch.tensor((50,50))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_inverse_normalized_position_top_left(self):\n",
    "        actual = inverse_normalized_position(torch.tensor((-1,-1)),100)\n",
    "        expected = torch.tensor((0,0))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_inverse_normalized_position_bottom_right(self):\n",
    "        actual = inverse_normalized_position(torch.tensor((1,1)), 100)\n",
    "        expected = torch.tensor((100,100))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "        \n",
    "    def test_inverse_normalized_position_other_location(self):\n",
    "        actual = inverse_normalized_position(torch.tensor((-0.5,0.5)),100)\n",
    "        expected = torch.tensor((25,75))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "    \n",
    "    def test_inverse_normalized_position_out_of_bounds(self):\n",
    "        actual = inverse_normalized_position(torch.tensor((-1.2,1.2)),100)\n",
    "        expected = torch.tensor((0,100))\n",
    "        self.assertTorchEqual(expected,actual)\n",
    "\n",
    "    def test_inverse_normalized_position_multiple(self):\n",
    "        actual = inverse_normalized_position(torch.tensor(((-0.5,0.5),(0.5,0.5),(0.0,0.0))),100)\n",
    "        expected = torch.tensor(((25,75),(75,75),(50,50)))\n",
    "        self.assertTorchEqual(expected,actual)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "gs = GlimpseSensor(4, 2, 50)\n",
    "im = scipy.misc.face(True).astype(int)[100:600,300:800]\n",
    "im_racoon = torch.tensor([im,im]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im,cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpses = gs.extract_glimpses(im_racoon, torch.tensor(((0.0,0.0),(0.0,0.0))))\n",
    "show([g.unsqueeze(0) for g in glimpses[0]],[\"CENTER\",\"\",\"\",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpses = gs.extract_glimpses(im_racoon, torch.tensor(((-1,-1),(-1,-1))))\n",
    "show([g.unsqueeze(0) for g in glimpses[0]],[\"TOP-LEFT\",\"\",\"\",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glimpses = gs.extract_glimpses(im_racoon, torch.tensor(((-1,0),(-1,0))))\n",
    "show([g.unsqueeze(0) for g in glimpses[0]],[\"LEFT\",\"\",\"\",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpses = gs.extract_glimpses(im_racoon, torch.tensor(((-0.5,0.5),(-0.5,0.5))))\n",
    "show([g.unsqueeze(0) for g in glimpses[0]],[\"offset from center\",\"\",\"\",\"\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "name": "Visual-Attention-augmented-mnist-n6-scale2-28-14-28.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05986c728e6e4e28ab042c60e9e321b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "248.6s loss: 1.942 , acc: 48.000:  85%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91ff2de9b3d446b59114c23d3985bdfc",
      "max": 54000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4083770cefc6482b95e990403e9f393b",
      "value": 45800
     }
    },
    "11cb8e4f9ced499b83bc92cb11cd22dd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310e691438e94a72ba0597ca0348dac2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4083770cefc6482b95e990403e9f393b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "51a7999fab27419ebea3ca78b0587cc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8da835af50a743e79a3ca6d72b13c2fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_05986c728e6e4e28ab042c60e9e321b8",
       "IPY_MODEL_b730e82da031476695af610084cb3531"
      ],
      "layout": "IPY_MODEL_310e691438e94a72ba0597ca0348dac2"
     }
    },
    "91ff2de9b3d446b59114c23d3985bdfc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b730e82da031476695af610084cb3531": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11cb8e4f9ced499b83bc92cb11cd22dd",
      "placeholder": "​",
      "style": "IPY_MODEL_51a7999fab27419ebea3ca78b0587cc0",
      "value": " 45750/54000 [04:08&lt;00:45, 182.76it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
