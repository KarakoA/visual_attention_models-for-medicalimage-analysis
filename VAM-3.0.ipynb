{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Depdencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/anaconda3/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: torchviz in /usr/local/anaconda3/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/anaconda3/lib/python3.7/site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.7/site-packages (1.18.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/lib/python3.7/site-packages (4.42.1)\n",
      "Requirement already satisfied: scikit-image in /usr/local/anaconda3/lib/python3.7/site-packages (0.16.2)\n",
      "Requirement already satisfied: tensorboard_logger in /usr/local/anaconda3/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: future in /usr/local/anaconda3/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: graphviz in /usr/local/anaconda3/lib/python3.7/site-packages (from torchviz) (0.14)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from scikit-image) (2.6.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from scikit-image) (2.4)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard_logger) (1.14.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard_logger) (3.12.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from networkx>=2.0->scikit-image) (4.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchviz matplotlib numpy tqdm scikit-image tensorboard_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "import skimage.measure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import unittest\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetName(Enum):\n",
    "    MNIST = 1\n",
    "    AUGMENTED = 2\n",
    "    TRANSFORMED = 3\n",
    "    AUGMENTED_MEDICAL = 4\n",
    "    AUGMENTED_MEDICAL_3 = 5\n",
    "    AUGMENTED_MEDICAL_SIMPLE = 6\n",
    "    AUGMENTED_MEDICAL_SIMPLE_2 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # glimpse network params\n",
    "        self.patch_size      = 8         # size of extracted patch at highest res\n",
    "        self.glimpse_scale   = 1         # scale of successive patches\n",
    "        self.num_patches     = 1         # Num of downscaled patches per glimpse\n",
    "        self.loc_hidden      = 128       # hidden size of loc fc layer\n",
    "        self.glimpse_hidden  = 128       # hidden size of glimpse fc\n",
    "\n",
    "        # core network params\n",
    "        self.num_glimpses    = 6         # Num of glimpses, i.e. BPTT iterations\n",
    "        self.hidden_size     = 256       # hidden size of rnn\n",
    "\n",
    "        # reinforce params\n",
    "        self.std             = 0.05      # gaussian policy standard deviation\n",
    "        self.M               = 1         # Monte Carlo sampling for valid and test sets\n",
    "\n",
    "        # ETC params\n",
    "        self.valid_size      = 0.1       # Proportion of training set used for validation\n",
    "        self.batch_size      = 128       # Num of images in each batch of data\n",
    "        self.num_workers     = 4         # Num of subprocesses to use for data loading\n",
    "        self.shuffle         = True      # Whether to shuffle the train and valid indices\n",
    "        self.show_sample     = False     # Whether to visualize a sample grid of the data\n",
    "\n",
    "        # training params\n",
    "        self.is_train        = False      # Whether to train(true) or test the model\n",
    "        self.momentum        = 0.5       # Nesterov momentum value\n",
    "        self.epochs          = 200        # Num of epochs to train for\n",
    "        self.init_lr         = 3e-4      # Initial learning rate value\n",
    "        self.lr_patience     = 20        # Number of epochs to wait before reducing lr\n",
    "        self.train_patience  = 150        # Number of epochs to wait before stopping train\n",
    "\n",
    "        # other params\n",
    "        self.use_gpu         = True      # Whether to run on the GPU\n",
    "        self.best            = True      # Load best model or most recent for testing\n",
    "        self.random_seed     = 1         # Seed to ensure reproducibility\n",
    "        self.data_dir        = \"./data\"  # Directory in which data is stored\n",
    "        self.ckpt_dir        = \"./ckpt\"  # Directory in which to save model checkpoints\n",
    "        self.logs_dir        = \"./logs/\" # Directory in which Tensorboard logs wil be stored\n",
    "        self.use_tensorboard = False     # Whether to use tensorboard for visualization\n",
    "        self.resume          = True     # Whether to resume training from checkpoint\n",
    "        self.print_freq      = 100        # How frequently to print training details\n",
    "        self.plot_freq       = 1         # How frequently to plot glimpses\n",
    "        self.dataset         = DatasetName.MNIST\n",
    "        self.model_name      = \"ram_{}_{}x{}_{}\".format(\n",
    "            self.num_glimpses,\n",
    "            self.patch_size,\n",
    "            self.patch_size,\n",
    "            self.glimpse_scale,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "torch.manual_seed(global_config.random_seed)\n",
    "np.random.seed(global_config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMedicalMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Augmented mnist meant to mimic whole-slide-images of tumor cells.\n",
    "    9's represent cancer cells. There are 4 different labels, based on the number of 9's:\n",
    "    \n",
    "    zero 9's          - no cancer\n",
    "    one 9             - isolated tumor cell\n",
    "    two 9's           - micro-metastasis \n",
    "    three or more 9's - macro-metastasis\n",
    "    \n",
    "    Each image contains between 3 and 10 cells at random, which may be overlapping.\n",
    "    It consists of 5000 items of each category(total 20.000) for training and 500(2.000) of each for testing\n",
    "    of size 256 x 256. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 root_dir,\n",
    "                 train,\n",
    "                 data_dir = \"MEDNIST\",\n",
    "                 mnist_transform = None,\n",
    "                 transform = None, \n",
    "                 total_train = 20000,\n",
    "                 total_test = 2000,\n",
    "                 n_partitions_test = 1,\n",
    "                 n_partitions_train = 5):\n",
    "        \n",
    "        self.mnist_transform = mnist_transform\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.total = total_train if self.train else total_test\n",
    "        self.n_partitions_test  = n_partitions_test\n",
    "        self.n_partitions_train = n_partitions_train\n",
    "        self.dir = os.path.join(root_dir,data_dir, \"train\" if train else \"test\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.__create_dataset_if_needed()\n",
    "                \n",
    "        self.__load_data()\n",
    "        \n",
    "    def __dataset_exists(self):\n",
    "        # mkdir if not exists\n",
    "        os.makedirs(self.dir, exist_ok = True)\n",
    "        len_files = len(os.listdir(self.dir)) \n",
    "        if len_files > 0:\n",
    "            print(\"Data existing, skipping creation.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Dataset missing. Creating...\")\n",
    "        return False\n",
    "            \n",
    "    \n",
    "    def __combine_images(self,images, output_dim):\n",
    "        \"\"\"\n",
    "        Combines the given images into a single image of output_dim size. Combinations are done randomly and \n",
    "        overlapping is possible. Images will always be within bounds completely.\n",
    "        \"\"\"\n",
    "        np_images = np.array(images)\n",
    "        input_dim = np_images.shape[-1]\n",
    "        new_image = np.zeros(shape=(output_dim,output_dim), dtype = np.float32)\n",
    "        for image in np_images:\n",
    "            i, j = np.random.randint(0, output_dim  - input_dim, size = 2)\n",
    "            new_image[i:i+input_dim, j:j+input_dim] = image\n",
    "        return new_image\n",
    "    \n",
    "    def __get_cell_counts(self, items_per_class_count, class_index):\n",
    "        # exclusive\n",
    "        max_items = 11\n",
    "        min_number_of_cells = 3\n",
    "        # 0,1,2,3+ for no tumor cells, isolated tumor cells, \n",
    "        # micro-metastasis and macro-metastasis respectively\n",
    "        num_tumor_cells = class_index if class_index != 3 else np.random.randint(3, max_items) \n",
    "\n",
    "        num_healthy_cells = max_items - num_tumor_cells\n",
    "        if num_healthy_cells + num_tumor_cells < min_number_of_cells:\n",
    "            num_healthy_cells = min_number_of_cells - num_tumor_cells\n",
    "\n",
    "        return (num_tumor_cells, num_healthy_cells)\n",
    "            \n",
    "    def __generate_for_class(self,\n",
    "                                   items,\n",
    "                                   items_per_class_count,\n",
    "                                   class_index,\n",
    "                                   uid,\n",
    "                                   all_tumor_cell_images,\n",
    "                                   all_healthy_cell_images):\n",
    "        for _ in range(items_per_class_count):\n",
    "            num_tumors, num_healthy = self.__get_cell_counts(items_per_class_count, class_index)\n",
    "\n",
    "            healthy_idxs = np.random.randint(0,len(all_healthy_cell_images), num_healthy)\n",
    "            tumor_idxs   = np.random.randint(0,len(all_tumor_cell_images), num_tumors)\n",
    "\n",
    "            cells = np.vstack((all_healthy_cell_images[healthy_idxs], all_tumor_cell_images[tumor_idxs]))\n",
    "            image = self.__combine_images(cells, 256)\n",
    "            image = np.expand_dims(image, axis = 0)\n",
    "            self.data.append(image)\n",
    "            self.labels.append(class_index)\n",
    "            uid += 1\n",
    "        return uid\n",
    "            \n",
    "    def __create_dataset_if_needed(self):\n",
    "        if self.__dataset_exists():\n",
    "            return \n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # in how many partitions to split dataset creation\n",
    "        partitions_count = 10\n",
    "        \n",
    "        # number of classes in output (fixed)\n",
    "        num_classes = 4\n",
    "        \n",
    "        mnist = torchvision.datasets.MNIST(root ='./data',\n",
    "                                           train = self.train,\n",
    "                                           download = True,\n",
    "                                           transform = self.mnist_transform)\n",
    "        \n",
    "        mnist_loader = iter(torch.utils.data.DataLoader(mnist, \n",
    "                                                        batch_size = int(self.total/partitions_count), \n",
    "                                                        shuffle = False, \n",
    "                                                        num_workers = 0))\n",
    "        uid = 0\n",
    "        batch, mnist_labels = mnist_loader.next()\n",
    "        for _ in range(partitions_count):\n",
    "            # 9's represent tumors\n",
    "            all_tumor_cell_images = batch[mnist_labels == 9]\n",
    "            # everything else except 6's healthy cells\n",
    "            all_healthy_cell_images = batch[(mnist_labels != 9) & (mnist_labels != 6)]\n",
    "            \n",
    "            items_per_class_count = int(self.total/(num_classes * partitions_count))\n",
    "            \n",
    "            for class_index in range(num_classes):\n",
    "                    uid = self.__generate_for_class(class_index, \n",
    "                                                  items_per_class_count,\n",
    "                                                  class_index,\n",
    "                                                  uid,\n",
    "                                                  all_tumor_cell_images,\n",
    "                                                  all_healthy_cell_images)\n",
    "        self.__store()\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, uid):\n",
    "        if torch.is_tensor(uid):\n",
    "            uid = uid.tolist()\n",
    "        label = self.labels[uid]\n",
    "        sample = self.data[uid]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return (sample,label)\n",
    "\n",
    "    def __store(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        \n",
    "        assert(len(self.data) == len(self.labels))\n",
    "        max_index = len(self.data)\n",
    "        partition_size = max_index/n_partitions\n",
    "        for i in range(n_partitions):\n",
    "            start,end =(int(partition_size * i), int(partition_size * (i+1)))\n",
    "            partition = np.array(self.data[start:end])\n",
    "            np.save(os.path.join(self.dir, \"part_\" + str(i)), partition)\n",
    "        \n",
    "        np.save(os.path.join(self.dir, \"labels\"), np.array(self.labels))\n",
    "    \n",
    "    def __load_data(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        data = []\n",
    "        for i in range(n_partitions):\n",
    "            data.append(np.load(os.path.join(self.dir, \"part_\" + str(i)+\".npy\")))\n",
    "        self.data = np.vstack(data)\n",
    "        self.labels = np.load(os.path.join(self.dir, \"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(Enum):\n",
    "    TRAIN = 1\n",
    "    VALID = 2\n",
    "    TEST  = 3\n",
    "    \n",
    "class DatasetLocator():\n",
    "    def __init__(self, conf: Config):\n",
    "        \n",
    "        self.dataset = conf.dataset\n",
    "        self.gpu_run = conf.use_gpu\n",
    "        self.batch_size = conf.batch_size\n",
    "        train, valid, test = self.__load_data()\n",
    "        \n",
    "        self.dataset_dict = {\n",
    "            DatasetType.TRAIN: train,\n",
    "            DatasetType.VALID: valid,\n",
    "            DatasetType.TEST: test\n",
    "        }\n",
    "        \n",
    "    def __f(self,image):\n",
    "        np_image = np.array(image)\n",
    "        input_dim = np_image.shape[-1]\n",
    "        new_image = np.zeros(shape=(60,60), dtype = np.float32)\n",
    "        i, j = np.random.randint(0, 60  - input_dim, size = 2)\n",
    "        new_image[i:i+input_dim, j:j+input_dim] = np_image\n",
    "        return new_image\n",
    "    \n",
    "    def __transformed_mnist_transformation(self):\n",
    "        return transforms.Compose(\n",
    "            [torchvision.transforms.Lambda(self.__f),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def __augmented_mnist_transformation(self):\n",
    "        return transforms.Compose([\n",
    "            torchvision.transforms.RandomAffine(degrees = (-180,180),scale = (0.5,1.0),),\n",
    "            torchvision.transforms.ToTensor()])\n",
    "    \n",
    "    def __augmented_mnist_simple_transformation(self):\n",
    "        return transforms.Compose([\n",
    "            torchvision.transforms.RandomAffine(degrees = (0,90),scale = (0.9,1.0),),\n",
    "            torchvision.transforms.ToTensor()])\n",
    "\n",
    "    def __load_data(self):\n",
    "        train_total = self.__load_dataset(True)\n",
    "        test = self.__load_dataset(False)\n",
    "        \n",
    "        train_length = int(len(train_total) * 0.9)\n",
    "        valid_length = len(train_total) - train_length\n",
    "        (train, valid) = torch.utils.data.random_split(train_total,(train_length, valid_length))\n",
    "        return (train, valid, test)\n",
    "    \n",
    "    def __load_dataset(self, is_train):\n",
    "        if self.dataset == DatasetName.MNIST:\n",
    "            transform = torchvision.transforms.ToTensor()\n",
    "        elif self.dataset == DatasetName.AUGMENTED:\n",
    "            transform = self.__augmented_mnist_transformation()\n",
    "        elif self.dataset == DatasetName.TRANSFORMED:\n",
    "            transform = self.__transformed_mnist_transformation()\n",
    "        elif self.dataset == DatasetName.AUGMENTED_MEDICAL:\n",
    "            return AugmentedMedicalMNISTDataset(root_dir='.', data_dir = \"MEDNIST\",train = is_train, mnist_transform = self.__augmented_mnist_transformation())\n",
    "        elif self.dataset == DatasetName.AUGMENTED_MEDICAL_3:\n",
    "            return AugmentedMedicalMNISTDataset(root_dir='.',total_train = 60000, n_partitions_train = 15, data_dir = \"MEDNIST_3\", train = is_train, mnist_transform = self.__augmented_mnist_transformation())\n",
    "        elif self.dataset == DatasetName.AUGMENTED_MEDICAL_SIMPLE:\n",
    "            return AugmentedMedicalMNISTDataset(root_dir='.',data_dir = \"MEDNIST_SIMPLE\", train = is_train, mnist_transform = self.__augmented_mnist_simple_transformation())\n",
    "        elif self.dataset == DatasetName.AUGMENTED_MEDICAL_SIMPLE_2:\n",
    "            return AugmentedMedicalMNISTDataset(root_dir='.',data_dir = \"MEDNIST_SIMPLE_2\", train = is_train, mnist_transform = self.__augmented_mnist_simple_transformation())\n",
    "        return torchvision.datasets.MNIST(root='./data', train = is_train , download = True, transform = transform)\n",
    "        \n",
    "    def data_loader(self, dataset:DatasetType):\n",
    "        should_shuffle = dataset == DatasetType.TRAIN\n",
    "        data = self.dataset_dict[dataset] \n",
    "        return torch.utils.data.DataLoader(data,\n",
    "                                           batch_size = self.batch_size,\n",
    "                                           pin_memory = self.gpu_run,\n",
    "                                           shuffle = should_shuffle,\n",
    "                                           num_workers = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = DatasetLocator(global_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = iter(locator.data_loader(DatasetType.TRAIN)).next()\n",
    "images = images[0:4]\n",
    "labels = labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show(images,labels = None):\n",
    "    if labels == None:\n",
    "        labels = [\" \"] * len(images)\n",
    "    elif isinstance(labels,torch.Tensor): \n",
    "        labels = [label.item() for label in labels]\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(25,25))\n",
    "    for (a,image,label) in zip(axes.ravel(),images,labels):\n",
    "        a.imshow(image[0].numpy(), cmap = plt.cm.gray)\n",
    "        a.set_title(label, fontsize=20)\n",
    "\n",
    "    fig.tight_layout()\n",
    "show(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def denormalize(T, coords):\n",
    "    return 0.5 * ((coords + 1.0) * T)\n",
    "\n",
    "\n",
    "def bounding_box(x, y, size, color=\"w\"):\n",
    "    x = int(x - (size / 2))\n",
    "    y = int(y - (size / 2))\n",
    "    rect = patches.Rectangle(\n",
    "        (x, y), size, size, linewidth=1, edgecolor=color, fill=False\n",
    "    )\n",
    "    return rect\n",
    "\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def resize_array(x, size):\n",
    "    # 3D and 4D tensors allowed only\n",
    "    assert x.ndim in [3, 4], \"Only 3D and 4D Tensors allowed!\"\n",
    "\n",
    "    # 4D Tensor\n",
    "    if x.ndim == 4:\n",
    "        res = []\n",
    "        for i in range(x.shape[0]):\n",
    "            img = array2img(x[i])\n",
    "            img = img.resize((size, size))\n",
    "            img = np.asarray(img, dtype=\"float32\")\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            img /= 255.0\n",
    "            res.append(img)\n",
    "        res = np.concatenate(res)\n",
    "        res = np.expand_dims(res, axis=1)\n",
    "        return res\n",
    "\n",
    "    # 3D Tensor\n",
    "    img = array2img(x)\n",
    "    img = img.resize((size, size))\n",
    "    res = np.asarray(img, dtype=\"float32\")\n",
    "    res = np.expand_dims(res, axis=0)\n",
    "    res /= 255.0\n",
    "    return res\n",
    "\n",
    "\n",
    "def img2array(data_path, desired_size=None, expand=False, view=False):\n",
    "    \"\"\"\n",
    "    Util function for loading RGB image into a numpy array.\n",
    "\n",
    "    Returns array of shape (1, H, W, C).\n",
    "    \"\"\"\n",
    "    img = Image.open(data_path)\n",
    "    img = img.convert(\"RGB\")\n",
    "    if desired_size:\n",
    "        img = img.resize((desired_size[1], desired_size[0]))\n",
    "    if view:\n",
    "        img.show()\n",
    "    x = np.asarray(img, dtype=\"float32\")\n",
    "    if expand:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def array2img(x):\n",
    "    \"\"\"\n",
    "    Util function for converting anumpy array to a PIL img.\n",
    "\n",
    "    Returns PIL RGB img.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    x = x + max(-np.min(x), 0)\n",
    "    x_max = np.max(x)\n",
    "    if x_max != 0:\n",
    "        x /= x_max\n",
    "    x *= 255\n",
    "    return Image.fromarray(x.astype(\"uint8\"), \"RGB\")\n",
    "\n",
    "\n",
    "def plot_images(images, gd_truth):\n",
    "\n",
    "    images = images.squeeze()\n",
    "    assert len(images) == len(gd_truth) == 9\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # plot the image\n",
    "        ax.imshow(images[i], cmap=\"Greys_r\")\n",
    "\n",
    "        xlabel = \"{}\".format(gd_truth[i])\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_dirs(config):\n",
    "    for path in [config.data_dir, config.ckpt_dir, config.logs_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "def save_config(config):\n",
    "    model_name = \"ram_{}_{}x{}_{}\".format(\n",
    "        config.num_glimpses, config.patch_size, config.patch_size, config.glimpse_scale\n",
    "    )\n",
    "    filename = model_name + \"_params.json\"\n",
    "    param_path = os.path.join(config.ckpt_dir, filename)\n",
    "\n",
    "    print(\"[*] Model Checkpoint Dir: {}\".format(config.ckpt_dir))\n",
    "    print(\"[*] Param Path: {}\".format(param_path))\n",
    "\n",
    "    with open(param_path, \"w\") as fp:\n",
    "        json.dump(config.__dict__, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "class Retina:\n",
    "    \"\"\"A visual retina.\n",
    "\n",
    "    Extracts a foveated glimpse `phi` around location `l`\n",
    "    from an image `x`.\n",
    "\n",
    "    Concretely, encodes the region around `l` at a\n",
    "    high-resolution but uses a progressively lower\n",
    "    resolution for pixels further from `l`, resulting\n",
    "    in a compressed representation of the original\n",
    "    image `x`.\n",
    "\n",
    "    Args:\n",
    "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "            of images.\n",
    "        l: a 2D Tensor of shape (B, 2). Contains normalized\n",
    "            coordinates in the range [-1, 1].\n",
    "        g: size of the first square patch.\n",
    "        k: number of patches to extract in the glimpse.\n",
    "        s: scaling factor that controls the size of\n",
    "            successive patches.\n",
    "\n",
    "    Returns:\n",
    "        phi: a 5D tensor of shape (B, k, g, g, C). The\n",
    "            foveated glimpse of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g, k, s):\n",
    "        self.g = g\n",
    "        self.k = k\n",
    "        self.s = s\n",
    "\n",
    "    def foveate(self, x, l):\n",
    "        \"\"\"Extract `k` square patches of size `g`, centered\n",
    "        at location `l`. The initial patch is a square of\n",
    "        size `g`, and each subsequent patch is a square\n",
    "        whose side is `s` times the size of the previous\n",
    "        patch.\n",
    "\n",
    "        The `k` patches are finally resized to (g, g) and\n",
    "        concatenated into a tensor of shape (B, k, g, g, C).\n",
    "        \"\"\"\n",
    "        phi = []\n",
    "        size = self.g\n",
    "\n",
    "        # extract k patches of increasing size\n",
    "        for i in range(self.k):\n",
    "            phi.append(self.extract_patch(x, l, size))\n",
    "            size = int(self.s * size)\n",
    "\n",
    "        # resize the patches to squares of size g\n",
    "        for i in range(1, len(phi)):\n",
    "            k = phi[i].shape[-1] // self.g\n",
    "            phi[i] = F.avg_pool2d(phi[i], k)\n",
    "\n",
    "        # concatenate into a single tensor and flatten\n",
    "        phi = torch.cat(phi, 1)\n",
    "        phi = phi.view(phi.shape[0], -1)\n",
    "\n",
    "        return phi\n",
    "\n",
    "    def extract_patch(self, x, l, size):\n",
    "        \"\"\"Extract a single patch for each image in `x`.\n",
    "\n",
    "        Args:\n",
    "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "            of images.\n",
    "        l: a 2D Tensor of shape (B, 2).\n",
    "        size: a scalar defining the size of the extracted patch.\n",
    "\n",
    "        Returns:\n",
    "            patch: a 4D Tensor of shape (B, size, size, C)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        start = self.denormalize(H, l)\n",
    "        end = start + size\n",
    "\n",
    "        # pad with zeros\n",
    "        x = F.pad(x, (size // 2, size // 2, size // 2, size // 2))\n",
    "\n",
    "        # loop through mini-batch and extract patches\n",
    "        patch = []\n",
    "        for i in range(B):\n",
    "            patch.append(x[i, :, start[i, 1] : end[i, 1], start[i, 0] : end[i, 0]])\n",
    "        return torch.stack(patch)\n",
    "\n",
    "    def denormalize(self, T, coords):\n",
    "        \"\"\"Convert coordinates in the range [-1, 1] to\n",
    "        coordinates in the range [0, T] where `T` is\n",
    "        the size of the image.\n",
    "        \"\"\"\n",
    "        return (0.5 * ((coords + 1.0) * T)).long()\n",
    "\n",
    "    def exceeds(self, from_x, to_x, from_y, to_y, T):\n",
    "        \"\"\"Check whether the extracted patch will exceed\n",
    "        the boundaries of the image of size `T`.\n",
    "        \"\"\"\n",
    "        if (from_x < 0) or (from_y < 0) or (to_x > T) or (to_y > T):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class GlimpseNetwork(nn.Module):\n",
    "    \"\"\"The glimpse network.\n",
    "\n",
    "    Combines the \"what\" and the \"where\" into a glimpse\n",
    "    feature vector `g_t`.\n",
    "\n",
    "    - \"what\": glimpse extracted from the retina.\n",
    "    - \"where\": location tuple where glimpse was extracted.\n",
    "\n",
    "    Concretely, feeds the output of the retina `phi` to\n",
    "    a fc layer and the glimpse location vector `l_t_prev`\n",
    "    to a fc layer. Finally, these outputs are fed each\n",
    "    through a fc layer and their sum is rectified.\n",
    "\n",
    "    In other words:\n",
    "\n",
    "        `g_t = relu( fc( fc(l) ) + fc( fc(phi) ) )`\n",
    "\n",
    "    Args:\n",
    "        h_g: hidden layer size of the fc layer for `phi`.\n",
    "        h_l: hidden layer size of the fc layer for `l`.\n",
    "        g: size of the square patches in the glimpses extracted\n",
    "        by the retina.\n",
    "        k: number of patches to extract per glimpse.\n",
    "        s: scaling factor that controls the size of successive patches.\n",
    "        c: number of channels in each image.\n",
    "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "            of images.\n",
    "        l_t_prev: a 2D tensor of shape (B, 2). Contains the glimpse\n",
    "            coordinates [x, y] for the previous timestep `t-1`.\n",
    "\n",
    "    Returns:\n",
    "        g_t: a 2D tensor of shape (B, hidden_size).\n",
    "            The glimpse representation returned by\n",
    "            the glimpse network for the current\n",
    "            timestep `t`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h_g, h_l, g, k, s, c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retina = Retina(g, k, s)\n",
    "\n",
    "        # glimpse layer\n",
    "        D_in = k * g * g * c\n",
    "        self.fc1 = nn.Linear(D_in, h_g)\n",
    "\n",
    "        # location layer\n",
    "        D_in = 2\n",
    "        self.fc2 = nn.Linear(D_in, h_l)\n",
    "\n",
    "        self.fc3 = nn.Linear(h_g, h_g + h_l)\n",
    "        self.fc4 = nn.Linear(h_l, h_g + h_l)\n",
    "\n",
    "    def forward(self, x, l_t_prev):\n",
    "        # generate glimpse phi from image x\n",
    "        phi = self.retina.foveate(x, l_t_prev)\n",
    "\n",
    "        # flatten location vector\n",
    "        l_t_prev = l_t_prev.view(l_t_prev.size(0), -1)\n",
    "\n",
    "        # feed phi and l to respective fc layers\n",
    "        phi_out = F.relu(self.fc1(phi))\n",
    "        l_out = F.relu(self.fc2(l_t_prev))\n",
    "\n",
    "        what = self.fc3(phi_out)\n",
    "        where = self.fc4(l_out)\n",
    "\n",
    "        # feed to fc layer\n",
    "        g_t = F.relu(what + where)\n",
    "\n",
    "        return g_t\n",
    "\n",
    "\n",
    "class CoreNetwork(nn.Module):\n",
    "    \"\"\"The core network.\n",
    "\n",
    "    An RNN that maintains an internal state by integrating\n",
    "    information extracted from the history of past observations.\n",
    "    It encodes the agent's knowledge of the environment through\n",
    "    a state vector `h_t` that gets updated at every time step `t`.\n",
    "\n",
    "    Concretely, it takes the glimpse representation `g_t` as input,\n",
    "    and combines it with its internal state `h_t_prev` at the previous\n",
    "    time step, to produce the new internal state `h_t` at the current\n",
    "    time step.\n",
    "\n",
    "    In other words:\n",
    "\n",
    "        `h_t = relu( fc(h_t_prev) + fc(g_t) )`\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the rnn.\n",
    "        hidden_size: hidden size of the rnn.\n",
    "        g_t: a 2D tensor of shape (B, hidden_size). The glimpse\n",
    "            representation returned by the glimpse network for the\n",
    "            current timestep `t`.\n",
    "        h_t_prev: a 2D tensor of shape (B, hidden_size). The\n",
    "            hidden state vector for the previous timestep `t-1`.\n",
    "\n",
    "    Returns:\n",
    "        h_t: a 2D tensor of shape (B, hidden_size). The hidden\n",
    "            state vector for the current timestep `t`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, g_t, h_t_prev):\n",
    "        h1 = self.i2h(g_t)\n",
    "        h2 = self.h2h(h_t_prev)\n",
    "        h_t = F.relu(h1 + h2)\n",
    "        return h_t\n",
    "\n",
    "\n",
    "class ActionNetwork(nn.Module):\n",
    "    \"\"\"The action network.\n",
    "\n",
    "    Uses the internal state `h_t` of the core network to\n",
    "    produce the final output classification.\n",
    "\n",
    "    Concretely, feeds the hidden state `h_t` through a fc\n",
    "    layer followed by a softmax to create a vector of\n",
    "    output probabilities over the possible classes.\n",
    "\n",
    "    Hence, the environment action `a_t` is drawn from a\n",
    "    distribution conditioned on an affine transformation\n",
    "    of the hidden state vector `h_t`, or in other words,\n",
    "    the action network is simply a linear softmax classifier.\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the fc layer.\n",
    "        output_size: output size of the fc layer.\n",
    "        h_t: the hidden state vector of the core network\n",
    "            for the current time step `t`.\n",
    "\n",
    "    Returns:\n",
    "        a_t: output probability vector over the classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, h_t):\n",
    "        a_t = F.log_softmax(self.fc(h_t), dim=1)\n",
    "        return a_t\n",
    "\n",
    "\n",
    "class LocationNetwork(nn.Module):\n",
    "    \"\"\"The location network.\n",
    "\n",
    "    Uses the internal state `h_t` of the core network to\n",
    "    produce the location coordinates `l_t` for the next\n",
    "    time step.\n",
    "\n",
    "    Concretely, feeds the hidden state `h_t` through a fc\n",
    "    layer followed by a tanh to clamp the output beween\n",
    "    [-1, 1]. This produces a 2D vector of means used to\n",
    "    parametrize a two-component Gaussian with a fixed\n",
    "    variance from which the location coordinates `l_t`\n",
    "    for the next time step are sampled.\n",
    "\n",
    "    Hence, the location `l_t` is chosen stochastically\n",
    "    from a distribution conditioned on an affine\n",
    "    transformation of the hidden state vector `h_t`.\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the fc layer.\n",
    "        output_size: output size of the fc layer.\n",
    "        std: standard deviation of the normal distribution.\n",
    "        h_t: the hidden state vector of the core network for\n",
    "            the current time step `t`.\n",
    "\n",
    "    Returns:\n",
    "        mu: a 2D vector of shape (B, 2).\n",
    "        l_t: a 2D vector of shape (B, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.std = std\n",
    "\n",
    "        hid_size = input_size // 2\n",
    "        self.fc = nn.Linear(input_size, hid_size)\n",
    "        self.fc_lt = nn.Linear(hid_size, output_size)\n",
    "\n",
    "    def forward(self, h_t):\n",
    "        # compute mean\n",
    "        feat = F.relu(self.fc(h_t.detach()))\n",
    "        mu = torch.tanh(self.fc_lt(feat))\n",
    "\n",
    "        # reparametrization trick\n",
    "        l_t = torch.distributions.Normal(mu, self.std).rsample()\n",
    "        l_t = l_t.detach()\n",
    "        log_pi = Normal(mu, self.std).log_prob(l_t)\n",
    "\n",
    "        # we assume both dimensions are independent\n",
    "        # 1. pdf of the joint is the product of the pdfs\n",
    "        # 2. log of the product is the sum of the logs\n",
    "        log_pi = torch.sum(log_pi, dim=1)\n",
    "\n",
    "        # bound between [-1, 1]\n",
    "        l_t = torch.clamp(l_t, -1, 1)\n",
    "\n",
    "        return log_pi, l_t\n",
    "\n",
    "\n",
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"The baseline network.\n",
    "\n",
    "    This network regresses the baseline in the\n",
    "    reward function to reduce the variance of\n",
    "    the gradient update.\n",
    "\n",
    "    Args:\n",
    "        input_size: input size of the fc layer.\n",
    "        output_size: output size of the fc layer.\n",
    "        h_t: the hidden state vector of the core network\n",
    "            for the current time step `t`.\n",
    "\n",
    "    Returns:\n",
    "        b_t: a 2D vector of shape (B, 1). The baseline\n",
    "            for the current time step `t`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, h_t):\n",
    "        b_t = self.fc(h_t.detach())\n",
    "        return b_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RecurrentAttention(nn.Module):\n",
    "    \"\"\"A Recurrent Model of Visual Attention (RAM) [1].\n",
    "\n",
    "    RAM is a recurrent neural network that processes\n",
    "    inputs sequentially, attending to different locations\n",
    "    within the image one at a time, and incrementally\n",
    "    combining information from these fixations to build\n",
    "    up a dynamic internal representation of the image.\n",
    "\n",
    "    References:\n",
    "      [1]: Minh et. al., https://arxiv.org/abs/1406.6247\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, g, k, s, c, h_g, h_l, std, hidden_size, num_classes,\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "          g: size of the square patches in the glimpses extracted by the retina.\n",
    "          k: number of patches to extract per glimpse.\n",
    "          s: scaling factor that controls the size of successive patches.\n",
    "          c: number of channels in each image.\n",
    "          h_g: hidden layer size of the fc layer for `phi`.\n",
    "          h_l: hidden layer size of the fc layer for `l`.\n",
    "          std: standard deviation of the Gaussian policy.\n",
    "          hidden_size: hidden size of the rnn.\n",
    "          num_classes: number of classes in the dataset.\n",
    "          num_glimpses: number of glimpses to take per image,\n",
    "            i.e. number of BPTT steps.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.std = std\n",
    "\n",
    "        self.sensor = GlimpseNetwork(h_g, h_l, g, k, s, c)\n",
    "        self.rnn = CoreNetwork(hidden_size, hidden_size)\n",
    "        self.locator = LocationNetwork(hidden_size, 2, std)\n",
    "        self.classifier = ActionNetwork(hidden_size, num_classes)\n",
    "        self.baseliner = BaselineNetwork(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, l_t_prev, h_t_prev, last=False):\n",
    "        \"\"\"Run RAM for one timestep on a minibatch of images.\n",
    "\n",
    "        Args:\n",
    "            x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
    "                of images.\n",
    "            l_t_prev: a 2D tensor of shape (B, 2). The location vector\n",
    "                containing the glimpse coordinates [x, y] for the previous\n",
    "                timestep `t-1`.\n",
    "            h_t_prev: a 2D tensor of shape (B, hidden_size). The hidden\n",
    "                state vector for the previous timestep `t-1`.\n",
    "            last: a bool indicating whether this is the last timestep.\n",
    "                If True, the action network returns an output probability\n",
    "                vector over the classes and the baseline `b_t` for the\n",
    "                current timestep `t`. Else, the core network returns the\n",
    "                hidden state vector for the next timestep `t+1` and the\n",
    "                location vector for the next timestep `t+1`.\n",
    "\n",
    "        Returns:\n",
    "            h_t: a 2D tensor of shape (B, hidden_size). The hidden\n",
    "                state vector for the current timestep `t`.\n",
    "            mu: a 2D tensor of shape (B, 2). The mean that parametrizes\n",
    "                the Gaussian policy.\n",
    "            l_t: a 2D tensor of shape (B, 2). The location vector\n",
    "                containing the glimpse coordinates [x, y] for the\n",
    "                current timestep `t`.\n",
    "            b_t: a vector of length (B,). The baseline for the\n",
    "                current time step `t`.\n",
    "            log_probas: a 2D tensor of shape (B, num_classes). The\n",
    "                output log probability vector over the classes.\n",
    "            log_pi: a vector of length (B,).\n",
    "        \"\"\"\n",
    "        g_t = self.sensor(x, l_t_prev)\n",
    "        h_t = self.rnn(g_t, h_t_prev)\n",
    "\n",
    "        log_pi, l_t = self.locator(h_t)\n",
    "        b_t = self.baseliner(h_t).squeeze()\n",
    "\n",
    "        if last:\n",
    "            log_probas = self.classifier(h_t)\n",
    "            return h_t, l_t, b_t, log_probas, log_pi\n",
    "\n",
    "        return h_t, l_t, b_t, log_pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"A Recurrent Attention Model trainer.\n",
    "\n",
    "    All hyperparameters are provided by the user in the\n",
    "    config file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data_loader):\n",
    "        \"\"\"\n",
    "        Construct a new Trainer instance.\n",
    "\n",
    "        Args:\n",
    "            config: object containing command line arguments.\n",
    "            data_loader: A data iterator.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        if config.use_gpu and torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # glimpse network params\n",
    "        self.patch_size = config.patch_size\n",
    "        self.glimpse_scale = config.glimpse_scale\n",
    "        self.num_patches = config.num_patches\n",
    "        self.loc_hidden = config.loc_hidden\n",
    "        self.glimpse_hidden = config.glimpse_hidden\n",
    "\n",
    "        # core network params\n",
    "        self.num_glimpses = config.num_glimpses\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # reinforce params\n",
    "        self.std = config.std\n",
    "        self.M = config.M\n",
    "\n",
    "        # data params\n",
    "        if config.is_train:\n",
    "            self.train_loader = data_loader[0]\n",
    "            self.valid_loader = data_loader[1]\n",
    "            self.num_train = len(self.train_loader.dataset)\n",
    "            self.num_valid = len(self.valid_loader.dataset)\n",
    "        else:\n",
    "            self.test_loader = data_loader\n",
    "            self.num_test = len(self.test_loader.dataset)\n",
    "        self.num_classes = 10\n",
    "        self.num_channels = 1\n",
    "\n",
    "        # training params\n",
    "        self.epochs = config.epochs\n",
    "        self.start_epoch = 0\n",
    "        self.momentum = config.momentum\n",
    "        self.lr = config.init_lr\n",
    "\n",
    "        # misc params\n",
    "        self.best = config.best\n",
    "        self.ckpt_dir = config.ckpt_dir\n",
    "        self.logs_dir = config.logs_dir\n",
    "        self.best_valid_acc = 0.0\n",
    "        self.counter = 0\n",
    "        self.lr_patience = config.lr_patience\n",
    "        self.train_patience = config.train_patience\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.resume = config.resume\n",
    "        self.print_freq = config.print_freq\n",
    "        self.plot_freq = config.plot_freq\n",
    "        self.model_name = config.model_name\n",
    "\n",
    "\n",
    "        self.plot_dir = \"./plots/\" + self.model_name + \"/\"\n",
    "        if not os.path.exists(self.plot_dir):\n",
    "            os.makedirs(self.plot_dir)\n",
    "\n",
    "        # configure tensorboard logging\n",
    "        if self.use_tensorboard:\n",
    "            tensorboard_dir = self.logs_dir + self.model_name\n",
    "            print(\"[*] Saving tensorboard logs to {}\".format(tensorboard_dir))\n",
    "            if not os.path.exists(tensorboard_dir):\n",
    "                os.makedirs(tensorboard_dir)\n",
    "            configure(tensorboard_dir)\n",
    "\n",
    "        # build RAM model\n",
    "        self.model = RecurrentAttention(\n",
    "            self.patch_size,\n",
    "            self.num_patches,\n",
    "            self.glimpse_scale,\n",
    "            self.num_channels,\n",
    "            self.loc_hidden,\n",
    "            self.glimpse_hidden,\n",
    "            self.std,\n",
    "            self.hidden_size,\n",
    "            self.num_classes,\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # initialize optimizer and scheduler\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.config.init_lr\n",
    "        )\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, \"min\", patience=self.lr_patience\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        h_t = torch.zeros(\n",
    "            self.batch_size,\n",
    "            self.hidden_size,\n",
    "            dtype=torch.float,\n",
    "            device=self.device,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        l_t = torch.FloatTensor(self.batch_size, 2).uniform_(-1, 1).to(self.device)\n",
    "        l_t.requires_grad = True\n",
    "\n",
    "        return h_t, l_t\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model on the training set.\n",
    "\n",
    "        A checkpoint of the model is saved after each epoch\n",
    "        and if the validation accuracy is improved upon,\n",
    "        a separate ckpt is created for use on the test set.\n",
    "        \"\"\"\n",
    "        # load the most recent checkpoint\n",
    "        if self.resume:\n",
    "            self.load_checkpoint(best=False)\n",
    "\n",
    "        print(\n",
    "            \"\\n[*] Train on {} samples, validate on {} samples\".format(\n",
    "                self.num_train, self.num_valid\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "\n",
    "            print(\n",
    "                \"\\nEpoch: {}/{} - LR: {:.6f}\".format(\n",
    "                    epoch + 1, self.epochs, self.optimizer.param_groups[0][\"lr\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # train for 1 epoch\n",
    "            train_loss, train_acc = self.train_one_epoch(epoch)\n",
    "\n",
    "            # evaluate on validation set\n",
    "            valid_loss, valid_acc = self.validate(epoch)\n",
    "\n",
    "            # # reduce lr if validation loss plateaus\n",
    "            self.scheduler.step(-valid_acc)\n",
    "\n",
    "            is_best = valid_acc > self.best_valid_acc\n",
    "            msg1 = \"train loss: {:.3f} - train acc: {:.3f} \"\n",
    "            msg2 = \"- val loss: {:.3f} - val acc: {:.3f} - val err: {:.3f}\"\n",
    "            if is_best:\n",
    "                self.counter = 0\n",
    "                msg2 += \" [*]\"\n",
    "            msg = msg1 + msg2\n",
    "            print(\n",
    "                msg.format(\n",
    "                    train_loss, train_acc, valid_loss, valid_acc, 100 - valid_acc\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # check for improvement\n",
    "            if not is_best:\n",
    "                self.counter += 1\n",
    "            if self.counter > self.train_patience:\n",
    "                print(\"[!] No improvement in a while, stopping training.\")\n",
    "                return\n",
    "            self.best_valid_acc = max(valid_acc, self.best_valid_acc)\n",
    "            self.save_checkpoint(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state\": self.model.state_dict(),\n",
    "                    \"optim_state\": self.optimizer.state_dict(),\n",
    "                    \"best_valid_acc\": self.best_valid_acc,\n",
    "                },\n",
    "                is_best,\n",
    "            )\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Train the model for 1 epoch of the training set.\n",
    "\n",
    "        An epoch corresponds to one full pass through the entire\n",
    "        training set in successive mini-batches.\n",
    "\n",
    "        This is used by train() and should not be called manually.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "\n",
    "        tic = time.time()\n",
    "        with tqdm(total=self.num_train) as pbar:\n",
    "            for i, (x, y) in enumerate(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                plot = False\n",
    "                if (epoch % self.plot_freq == 0) and (i == 0):\n",
    "                    plot = True\n",
    "\n",
    "                # initialize location vector and hidden state\n",
    "                self.batch_size = x.shape[0]\n",
    "                h_t, l_t = self.reset()\n",
    "\n",
    "                # save images\n",
    "                imgs = []\n",
    "                imgs.append(x[0:9])\n",
    "\n",
    "                # extract the glimpses\n",
    "                locs = []\n",
    "                log_pi = []\n",
    "                baselines = []\n",
    "                for t in range(self.num_glimpses - 1):\n",
    "                    # forward pass through model\n",
    "                    h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n",
    "\n",
    "                    # store\n",
    "                    locs.append(l_t[0:9])\n",
    "                    baselines.append(b_t)\n",
    "                    log_pi.append(p)\n",
    "\n",
    "                # last iteration\n",
    "                h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n",
    "                log_pi.append(p)\n",
    "                baselines.append(b_t)\n",
    "                locs.append(l_t[0:9])\n",
    "\n",
    "                # convert list to tensors and reshape\n",
    "                baselines = torch.stack(baselines).transpose(1, 0)\n",
    "                log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "\n",
    "                # calculate reward\n",
    "                predicted = torch.max(log_probas, 1)[1]\n",
    "                R = (predicted.detach() == y).float()\n",
    "                R = R.unsqueeze(1).repeat(1, self.num_glimpses)\n",
    "\n",
    "                # compute losses for differentiable modules\n",
    "                loss_action = F.nll_loss(log_probas, y)\n",
    "                loss_baseline = F.mse_loss(baselines, R)\n",
    "\n",
    "                # compute reinforce loss\n",
    "                # summed over timesteps and averaged across batch\n",
    "                adjusted_reward = R - baselines.detach()\n",
    "                loss_reinforce = torch.sum(-log_pi * adjusted_reward, dim=1)\n",
    "                loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "\n",
    "                # sum up into a hybrid loss\n",
    "                loss = loss_action + loss_baseline + loss_reinforce * 0.01\n",
    "\n",
    "                # compute accuracy\n",
    "                correct = (predicted == y).float()\n",
    "                acc = 100 * (correct.sum() / len(y))\n",
    "\n",
    "                # store\n",
    "                losses.update(loss.item(), x.size()[0])\n",
    "                accs.update(acc.item(), x.size()[0])\n",
    "\n",
    "                # compute gradients and update SGD\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # measure elapsed time\n",
    "                toc = time.time()\n",
    "                batch_time.update(toc - tic)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    (\n",
    "                        \"{:.1f}s - loss: {:.3f} - acc: {:.3f}\".format(\n",
    "                            (toc - tic), loss.item(), acc.item()\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                pbar.update(self.batch_size)\n",
    "\n",
    "                # dump the glimpses and locs\n",
    "                if plot:\n",
    "                    imgs = [g.cpu().data.numpy().squeeze() for g in imgs]\n",
    "                    locs = [l.cpu().data.numpy() for l in locs]\n",
    "                    pickle.dump(\n",
    "                        imgs, open(self.plot_dir + \"g_{}.p\".format(epoch + 1), \"wb\")\n",
    "                    )\n",
    "                    pickle.dump(\n",
    "                        locs, open(self.plot_dir + \"l_{}.p\".format(epoch + 1), \"wb\")\n",
    "                    )\n",
    "\n",
    "                # log to tensorboard\n",
    "                if self.use_tensorboard:\n",
    "                    iteration = epoch * len(self.train_loader) + i\n",
    "                    log_value(\"train_loss\", losses.avg, iteration)\n",
    "                    log_value(\"train_acc\", accs.avg, iteration)\n",
    "\n",
    "            return losses.avg, accs.avg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch):\n",
    "        \"\"\"Evaluate the RAM model on the validation set.\n",
    "        \"\"\"\n",
    "        losses = AverageMeter()\n",
    "        accs = AverageMeter()\n",
    "\n",
    "        for i, (x, y) in enumerate(self.valid_loader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            # duplicate M times\n",
    "            x = x.repeat(self.M, 1, 1, 1)\n",
    "\n",
    "            # initialize location vector and hidden state\n",
    "            self.batch_size = x.shape[0]\n",
    "            h_t, l_t = self.reset()\n",
    "\n",
    "            # extract the glimpses\n",
    "            log_pi = []\n",
    "            baselines = []\n",
    "            for t in range(self.num_glimpses - 1):\n",
    "                # forward pass through model\n",
    "                h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n",
    "\n",
    "                # store\n",
    "                baselines.append(b_t)\n",
    "                log_pi.append(p)\n",
    "\n",
    "            # last iteration\n",
    "            h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n",
    "            log_pi.append(p)\n",
    "            baselines.append(b_t)\n",
    "\n",
    "            # convert list to tensors and reshape\n",
    "            baselines = torch.stack(baselines).transpose(1, 0)\n",
    "            log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "\n",
    "            # average\n",
    "            log_probas = log_probas.view(self.M, -1, log_probas.shape[-1])\n",
    "            log_probas = torch.mean(log_probas, dim=0)\n",
    "\n",
    "            baselines = baselines.contiguous().view(self.M, -1, baselines.shape[-1])\n",
    "            baselines = torch.mean(baselines, dim=0)\n",
    "\n",
    "            log_pi = log_pi.contiguous().view(self.M, -1, log_pi.shape[-1])\n",
    "            log_pi = torch.mean(log_pi, dim=0)\n",
    "\n",
    "            # calculate reward\n",
    "            predicted = torch.max(log_probas, 1)[1]\n",
    "            R = (predicted.detach() == y).float()\n",
    "            R = R.unsqueeze(1).repeat(1, self.num_glimpses)\n",
    "\n",
    "            # compute losses for differentiable modules\n",
    "            loss_action = F.nll_loss(log_probas, y)\n",
    "            loss_baseline = F.mse_loss(baselines, R)\n",
    "\n",
    "            # compute reinforce loss\n",
    "            adjusted_reward = R - baselines.detach()\n",
    "            loss_reinforce = torch.sum(-log_pi * adjusted_reward, dim=1)\n",
    "            loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "\n",
    "            # sum up into a hybrid loss\n",
    "            loss = loss_action + loss_baseline + loss_reinforce * 0.01\n",
    "\n",
    "            # compute accuracy\n",
    "            correct = (predicted == y).float()\n",
    "            acc = 100 * (correct.sum() / len(y))\n",
    "\n",
    "            # store\n",
    "            losses.update(loss.item(), x.size()[0])\n",
    "            accs.update(acc.item(), x.size()[0])\n",
    "\n",
    "            # log to tensorboard\n",
    "            if self.use_tensorboard:\n",
    "                iteration = epoch * len(self.valid_loader) + i\n",
    "                log_value(\"valid_loss\", losses.avg, iteration)\n",
    "                log_value(\"valid_acc\", accs.avg, iteration)\n",
    "\n",
    "        return losses.avg, accs.avg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self):\n",
    "        \"\"\"Test the RAM model.\n",
    "\n",
    "        This function should only be called at the very\n",
    "        end once the model has finished training.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "\n",
    "        # load the best checkpoint\n",
    "        self.load_checkpoint(best=self.best)\n",
    "\n",
    "        for i, (x, y) in enumerate(self.test_loader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            # duplicate M times\n",
    "            x = x.repeat(self.M, 1, 1, 1)\n",
    "\n",
    "            # initialize location vector and hidden state\n",
    "            self.batch_size = x.shape[0]\n",
    "            h_t, l_t = self.reset()\n",
    "\n",
    "            # extract the glimpses\n",
    "            for t in range(self.num_glimpses - 1):\n",
    "                # forward pass through model\n",
    "                h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n",
    "\n",
    "            # last iteration\n",
    "            h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n",
    "\n",
    "            log_probas = log_probas.view(self.M, -1, log_probas.shape[-1])\n",
    "            log_probas = torch.mean(log_probas, dim=0)\n",
    "\n",
    "            pred = log_probas.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        perc = (100.0 * correct) / (self.num_test)\n",
    "        error = 100 - perc\n",
    "        print(\n",
    "            \"[*] Test Acc: {}/{} ({:.2f}% - {:.2f}%)\".format(\n",
    "                correct, self.num_test, perc, error\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def save_checkpoint(self, state, is_best):\n",
    "        \"\"\"Saves a checkpoint of the model.\n",
    "\n",
    "        If this model has reached the best validation accuracy thus\n",
    "        far, a seperate file with the suffix `best` is created.\n",
    "        \"\"\"\n",
    "        filename = self.model_name + \"_ckpt.pth.tar\"\n",
    "        ckpt_path = os.path.join(self.ckpt_dir, filename)\n",
    "        torch.save(state, ckpt_path)\n",
    "        if is_best:\n",
    "            filename = self.model_name + \"_model_best.pth.tar\"\n",
    "            shutil.copyfile(ckpt_path, os.path.join(self.ckpt_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, best=False):\n",
    "        \"\"\"Load the best copy of a model.\n",
    "\n",
    "        This is useful for 2 cases:\n",
    "        - Resuming training with the most recent model checkpoint.\n",
    "        - Loading the best validation model to evaluate on the test data.\n",
    "\n",
    "        Args:\n",
    "            best: if set to True, loads the best model.\n",
    "                Use this if you want to evaluate your model\n",
    "                on the test data. Else, set to False in which\n",
    "                case the most recent version of the checkpoint\n",
    "                is used.\n",
    "        \"\"\"\n",
    "        print(\"[*] Loading model from {}\".format(self.ckpt_dir))\n",
    "\n",
    "        filename = self.model_name + \"_ckpt.pth.tar\"\n",
    "        if best:\n",
    "            filename = self.model_name + \"_model_best.pth.tar\"\n",
    "        ckpt_path = os.path.join(self.ckpt_dir, filename)\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # load variables from checkpoint\n",
    "        self.start_epoch = ckpt[\"epoch\"]\n",
    "        self.best_valid_acc = ckpt[\"best_valid_acc\"]\n",
    "        self.model.load_state_dict(ckpt[\"model_state\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
    "\n",
    "        if best:\n",
    "            print(\n",
    "                \"[*] Loaded {} checkpoint @ epoch {} \"\n",
    "                \"with best valid acc of {:.3f}\".format(\n",
    "                    filename, ckpt[\"epoch\"], ckpt[\"best_valid_acc\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\"[*] Loaded {} checkpoint @ epoch {}\".format(filename, ckpt[\"epoch\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def main(config):\n",
    "    prepare_dirs(config)\n",
    "\n",
    "    kwargs = {}\n",
    "    if config.use_gpu:\n",
    "        torch.cuda.manual_seed(config.random_seed)\n",
    "        kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "\n",
    "    # instantiate data loaders\n",
    "    if config.is_train:\n",
    "        train_loader = locator.data_loader(DatasetType.TRAIN)\n",
    "        valid_loader = locator.data_loader(DatasetType.VALID)\n",
    "        dloader = (train_loader,valid_loader)\n",
    "    else:\n",
    "        dloader = locator.data_loader(DatasetType.TEST)\n",
    "\n",
    "    trainer = Trainer(config, dloader)\n",
    "\n",
    "    # either train\n",
    "    if config.is_train:\n",
    "        trainer.train()\n",
    "    # or load a pretrained model and test\n",
    "    else:\n",
    "        trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading model from ./ckpt\n",
      "[*] Loaded ram_6_8x8_1_model_best.pth.tar checkpoint @ epoch 181 with best valid acc of 99.167\n",
      "[*] Test Acc: 9899/10000 (98.99% - 1.01%)\n"
     ]
    }
   ],
   "source": [
    "main(global_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "plot_dir = \"./plots/\" + global_config.model_name + \"/\"\n",
    "epoch = 182\n",
    "\n",
    "\n",
    "# read in pickle files\n",
    "glimpses = pickle.load(open(plot_dir + \"g_{}.p\".format(epoch), \"rb\"))\n",
    "locations = pickle.load(open(plot_dir + \"l_{}.p\".format(epoch), \"rb\"))\n",
    "\n",
    "glimpses = np.concatenate(glimpses)\n",
    "\n",
    "# grab useful params\n",
    "size = int(plot_dir.split(\"_\")[2][0])\n",
    "num_anims = len(locations)\n",
    "num_cols = glimpses.shape[0]\n",
    "img_shape = glimpses.shape[1]\n",
    "\n",
    "# denormalize coordinates\n",
    "coords = [denormalize(img_shape, l) for l in locations]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=num_cols)\n",
    "\n",
    "\n",
    "fig.set_dpi(100)\n",
    "\n",
    "# plot base image\n",
    "for j, ax in enumerate(axs.flat):\n",
    "    ax.imshow(glimpses[j], cmap=\"Greys_r\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    \n",
    "def updateData(i):\n",
    "    color = \"r\"\n",
    "    co = coords[i]\n",
    "    for j, ax in enumerate(axs.flat):\n",
    "        for p in ax.patches:\n",
    "            p.remove()\n",
    "        c = co[j]\n",
    "        rect = bounding_box(c[0], c[1], size, color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# animate\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, updateData, frames=num_anims, interval=500, repeat=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"600\" height=\"400\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAMO9tZGF0AAACrQYF//+p\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2MCByMzAxMSBjZGU5YTkzIC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMCAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTYg\n",
       "bG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRl\n",
       "cmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJf\n",
       "cHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9\n",
       "MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTIgc2NlbmVjdXQ9NDAgaW50cmFfcmVm\n",
       "cmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42\n",
       "MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAABHG\n",
       "ZYiEABX//vfJ78Cm61tbtb+Tz0j8LLc+wio/blsTtOoAAAMAAAMAAAMCiwXtfZHYLNxbAAADABYg\n",
       "AiAQALQJ+LUR8fI4SeMGMFTg/+IASCMtyNL7I6dUFY30wVw8jRiRDTPjc7qP40jz4BtnxMXyhNX8\n",
       "3B+y9Qg1lkoh66yavOO79d5lS8IbzT61myT9nCzX16zkQ45YnzeRUTmszsu2+tyiDQlBq8erTqY0\n",
       "HJzunmkjPWeFZNBZzlbCbsu0sw/BfVRDIIIZ8gqqdIHy1CpiJh4APEGDP5aLoGPMQnpTCWUPYiD6\n",
       "EBFzxxrhHKeJJPtEtKKgloQZABG6hC2xQmHWuRIK8S2ua/DmupP8PkI5cKLw6NuXlt45T9RKCCl0\n",
       "hIqu4/G1Yi2B8cWQ/h3mKGh2s5jQBlHr2WrWXpnPm7ubouHcItf3d48DMuhfKPo1rI/GK5xEGGHO\n",
       "RboaQAbYrZhmW31Yuy43Ozy1ZPfj5qR+b4/BUimFjrCUYQ3hc/IdI/mKBjc+lC6RI6X5UYV9Re/f\n",
       "6b6mXMvn/9qF0sPTz5q0+NFP+FG9BDV9m5XPQ9DzixeMHJ12UE6Lw/8klfrWIi6YbHbLF4IRlu3d\n",
       "396wKRf/Mue86Xud3fg0N2yMuR4bpLcbXsLCbuxiE24B+WLMs5gZq6iZY+iMPBaH9bMLbf/2W0ty\n",
       "MdEgJjD66o+eMgpbqYHuSyFYLi9hJZMtsbd5FMbC/QFmGpBm6fe/wth1enFeR01Syuh81uKtpHnO\n",
       "6aWFpXZ67Hgi+pwYrqHZjH7kkkpQ2/eMCHzzlgT9YqO/YzooPHWo8qSbKdUWG1DPwiMgH+13TPEk\n",
       "mZ3cH+Qw8j4lBkbaRea4eEBW7wQvub1swK92uLQNe5m9k4iJBxT++orQ5cHXmlskqV+uyLcrTtXF\n",
       "5tHdcdJ/gBZRUW6MHZPAGELY76Ufy7vbi91OzEGJFKamoTWBwrYdD2q4gl7GfraAfV9GaSwz/AMR\n",
       "oBAQDqeK2e6D/Zlsyjk9/W9gTtvzqe+9LPFAQZ7F0Be936/F+YemY1xXKqbkEI5BuIuzgcqKNf+9\n",
       "rwJz0xJwVTrBrx9YglabALTq+2ipj5o3gUcCk/ST5HB6sTyW5mn67t9/dv+Id2+Q3Jv3eoqWg/Aw\n",
       "90xWU+TdF0k4kaeJ50OxUmLJKPLKw3nkLb85DnxeFQgwSl2HsX4j78JGVAJqRf4QgZFWlNjxXXW6\n",
       "AEFFHjedOGZnaQzNiCkLz2tLUTE2LBvHr4vFDgh3mDoFNAgGXJBxuKZEG92tcxj2ynxAXoOxHsVd\n",
       "WJrjRY2y1GeGaKrhKN653jjAcyxQsLmGqprmYer7M3onMle8dvmNpqhJ7r0AsiRJ4NFHodAwN33h\n",
       "gBbrPLAge3XpDGh0pXCCAzIrLmXjqPvnNMtBpU8cjGHprIa0R65MyWxUBKkJSemijKRKcHJHAx/s\n",
       "IJyi0Q+BFfDqCJUkIaZ0s7+v91CPCLQUkYje8rvO2O2qm0UO0shS4j7Oc2YvjiSfbLQsFGz31x7f\n",
       "wv8y8CFmxGFuIxZUtz5wx4aF3iBzU+GuvewoaFhyNY+6VYINatQRxyqtlc2BMPEj/+cQXbiSJevy\n",
       "zS1zKtAs2ThXQ/baNmxAlYpUwB15Bkx2yL62z4p1OpNaQEPW8bm3rYoEcIdWBQrKOpkPrx43cvJp\n",
       "3QRd4BLwsq8j2wXdEUHekDecVXpegap9XipFwiMR69H/4EucirVirnTX/FcK3RiRPONq9avjmSEV\n",
       "St0djhPxAwi5qQK2DLn5nG038lhtVw601Oq/m0OYobV1vgAstDgUHkhvu2t/2MV4ER5X60pQhyNW\n",
       "VUecDz8qWQsUJxF7JdRuGPTciqWO7je0SFW3u7gEFBTOe1pWdTGJlP0W9bIsjT7mC0+QtejkMPVM\n",
       "9YyYaAmN6hRIxXHgY8kT1hRTj6UE6G48kyITtG4uY/f1VHt+yL0g1HjtuXBK6mZ7E50QZdzef+M5\n",
       "lbMXwGkrtZSqqG7J2nAMXMScQMGso+2tFyI3sifiOd09IGMwrRjg0EbF1CsvbPkp/mOUCjnYxfSc\n",
       "5ik/mJczT99BXyGgw3PnazLt11g1kttw2EbUoFnIqxFA6o0kHLhaaYzk8/9xk7f0oqQmNYDWtFc8\n",
       "556eLJxM/36bC0gV5w0umk4DgQzbUDjL7KyR8u++StKltfrinKL0kdNbgnpx+Z+oPDoy5RmaCO9m\n",
       "8ly+4TAfScqrlmQlSHBdHuqUGIwSZf/zgPmGjIwTGIW29RTEaK3qilqKDxYuTO/LMxwUghAJof1q\n",
       "BFlUt/CREe7lSKoK3yGFL3V6kNoZRSAWYIZVNr1RDy6x6oL6nJOyZF71e+U/nolTgWF1VlK3n4qS\n",
       "IN0bNilIb/NgHrzIRkiKPTsKviqaPi5OC/dqlxlj9yH2vTlHV56oXzAMmEufmWcJnUEPvjzU5CCX\n",
       "RW0CHgKvBNLWWKDqkWH44Pn8ZpfgWt7MsH187yZaaXIBXSVRYM7/zIW0ANvZSe4pOUId62wpBy2Z\n",
       "dKNgTBp176F3LlM/ZbfeG+v14W+TSnTfoEjOUrH2ZqSncCCmitBGezCt8ULxVHuLeFob4KaKcRgd\n",
       "jpO9/QualC/6uaNjONKK7LeIFc6e4Vs5JAQ8/2j1/+0v/eBM1nnh2smd7bNiG5NPnBWsORHpyXQF\n",
       "4441S6o3vsAv3ap+H8GPfTfMWJ2gCYS8XcITAkBQYAzXub8XEa8x2IVT1ceNH5gn3DySIR/KfNt8\n",
       "9m7p9MVwQG+mtmo4HF5R142i7M0SyR9n0LAuuCE9Ah9ikdMtu+qPgBVEFRIVkDIbaEmYdAEu3RvP\n",
       "JORwev95hNV/9UF7ZYxdpd3zLmKMfLpGQ78UI9mOaQHqfGl4YZnbbpymuG8FkKhnR5UQE7qaVzCJ\n",
       "vDWnhK2x0TXMidGsGabl36UWeoyefBeKzk4Ofgow/EqJlSA8U91Yu2u3yGV9Dr2JePLxdkHKDKZf\n",
       "G9boj+veO+jND4qUdFi+FB2lPwG0cSmSnDk3uS/e2gw4srDIrXGULumxKN2rLpO3917+gCVVAKro\n",
       "//IqDlEBUxl6/Jk1r7je1Ox2QLfGoOic3DGojhuljQbC5gL8swChDmefGUsftXQEdlYZAxhXGI0K\n",
       "ZILslEjWlZMSdwQU9OPZboVb237qxVm0sVdZiOaeQE0dLIIfNOwtcnJSrj9PitAMuxR7RHUQTMvy\n",
       "RgAQZTwk2OF70jA1gbbmUPnunJfYCgvYx4aDnqTbuzmCVk2A+P2F8AnwqWLo4Rp3lqo5hjwbF3ld\n",
       "t/EHkzutzUUV/eLmMvyFMWkl8MlmPUD5Os2s5KofJQZLNC5uu6IhtALMFIeOdWcRmscJwFYFh3ZL\n",
       "syfzev+Oq2QQw31CiKqa0qZ6eFxIMQ5AvylqWNQyWlk6AHxHHT5axHHqIRxgZWzoybHbfhYWl72i\n",
       "4roEooCBIlzTY6MxT/EOp2ceD07XvXWyGD0teSGFRGQrW4N246jwoFz7NWvTc5BfpDu45eAWP6wj\n",
       "Kx+9q81iGYn7uZ6/8XT2wqs3S5WR6Mpew0G8MpP/9dDKHLSVjNRQKc/lx2cL/ltHFUTIOA8afq/l\n",
       "ffW1SF8MWtByV9uUoytA68XmWPcv8+YvyDiHpHAew3oEUm4sqj42z1UxjFocdepzyeEebdUUcxl/\n",
       "WsEocNAR2CJvtVDQ40da1chrYSFL8yDRetSybwrzp3b4JFrErRXCcTLdqLpm3O9MDWgTmSdBhYS2\n",
       "KsHtuSR2TpnlCq8x2mIhwLxttoy0NXMbMTzuuDWqApgbbxSj1GfHMIFKydg5um2Qk3xD7rKUHWbD\n",
       "IKaxB0Su2uj+6iLWQKF7PO89FBdHwHfpSWzZV3i3697b8p8KzGnC3Ptdv2QX2uAa43vpOAW8l5qy\n",
       "jGO36mAQdRvAzZ3vAvBl1DWcfPWYcIK5sKpx2/9ini0KirZItGv/1RBqaINSxNWYDY0WT5SJp4bP\n",
       "nKBzjAqYvbqjfAW3jlgMf0YjotRC4+jko/Ofq3qrYAOUaN0/gwvUwHKRFusf1NjVZKGz/6Gcxyfu\n",
       "gn/SKVarOCWZOq5P4qI3ahh7G//TpMiZjRb65u4XhzRrjm/LJqaSgdNVdlbJMzp1XPdbiHpfKsWI\n",
       "B/fwxdvAv7zn8OK2yd8xeR930PmJAUr4D//5PHPnqwh84Ll/maQeo3UUus/ZVJVGdr+JJUGqd6bQ\n",
       "mjxKlEi/G7oz94EoCdlnqZjiwwiOcHe+49cbG+fRpc5Vap1rUg0Yqg2Lz++tW2NwP6VN6DJGJ4XY\n",
       "bg0KObCbwo9Hlddj2Ds0jCGQu55gcFe11f/zdVdYoGK1JvxsgBYuimS2mYW3d8QFROGR2GvsYBYH\n",
       "zwjS///4/S8Dm5ghAhL+//54poXlVYQf7vDkyx5jV82QrOkaFBz38gLtQiYJlMuIJZJ0pMQdh53G\n",
       "lixzATDLZj5KYyNNiBuSevhOjfM7BDSe4n6FNRBwRmROwt5OxG+JKZWKX3jM1rFadc413R20SMVs\n",
       "/9HeTxxUhVpeJ1QXUjGJblyIiG/o/JqyxmiiZlKzUWMIW+x6vZMGRZtv66rx1zJ5PqJaqagotZrk\n",
       "EQSe/VGNyKC3VQdY+eLE8ocTLFKvw79ODPJoA9vXWiOTIoxaR0NXQjGdl81OAmSFBvUX2QLacX8D\n",
       "EK5rTRLE6Add+z6QtH4GtFPjK7dW347ZlsLp+m7NcGIde1eJAyTjGOqK/t41tXUBXADVQhT+iD1q\n",
       "RpjQ7rD3FFo+cnwGlOqbJRR7T4YVICMIOwWP9k9RpX2UQK+qqbttbwJ65b0V4Fqg569AHZpIBOvR\n",
       "M8fshC4YqzzhDgSF+JgZlGAtlpw5z0f+5JA8NCwGQcygNo/eSY8hARVHTKefmTLsTqUiAPuisEqx\n",
       "a7A3p6cwJZJXkjcyCr6YkmwAh7jiwYkB7tXV2ER3d5vw6QawUSfDHBPUDx0onr08teO405+W+1q5\n",
       "v3gKzDvyfOAosX5+pW8kTcx1FN6cxftuDiHmgEBL8xT+W/bEO+qWwohf6VczmtGtDMmqGwya0QmF\n",
       "WkzrekA+y7wDuVd1UJZC9kS//E94EqvJ71qhNEG9TdMSGHFaZvs61kx22ieuxELjOBk6I9mEJoXi\n",
       "4kNuCLfuNStzvVMEhqL/0J8BDL4yC6WpwMU1pjjDcB+Du5sJ3BN40M+AZiae7l+ZrlAQUb6xKU7Q\n",
       "chIjjgUE2rttZavxmQqLlQHuwKAoujYbgkHjuIIkafLMHCAuCyamS5FsHXy3zCA6/z+dzoVP0ehK\n",
       "VTyZC+AZ6EcyFCcNyqXND14vv1UZ+QcJvq9VRTGaQuzZPcAbEgDF7eakCYyIN2kakl2OrD2r80E0\n",
       "n/2uUhB5v8zS3W9gJNgkgh7z4EnnOqHQ/fVceZjDBiO7canH9ZY8vkhvTkGhWpuww/pti2m+AXcQ\n",
       "AUc6y396xFdYZ0XGHFs5eRnCJRUyPV/7dTMtWTcsJSybnBWt1e2hw8sgx4AVvDZzdSMl6OLvA04z\n",
       "4M9VeZ0GJYok/D2ogzdyYGAKJr9v7sxZs8JwVCN8pHCPKz0sERG5pZQECtf6L+ycjZaQZnTv1gQQ\n",
       "RG1TNN6B8cnAMIDLx/ordm0S+MTWdyHQWmydzN2BQcDQuYTn+39GVZlYfg5iw61EG12A1ENR6KNd\n",
       "2c0WzTIXXN3/VF16bkYpnL3eaw5IwtZnCXuoBFof8tvIFVjTYOBWKQhsy0C1AkJMtBGIjD/SpF3x\n",
       "dD6KzdQmOnC7yaLSyBONnJM3b2rj+mI/uPVPOaGfv46ZvqCFoG4xuCackf7ojSAvWXpLsfRDeQs/\n",
       "R+AFH/mbOFNDeTX51Eba+AjG1fjTfO+OgRJLivLlEn8uDgvKVajw3a2/ZZ5OX1Iyb+KoanHfHlB9\n",
       "gmVDPQJ/vSLlyKkA6NQr5jlIfsb3IqKiirHZyQCjqtE2v1YUvAZj0rNhaMPsnOpOUPsrjJd3TCTs\n",
       "iMzgySEzV9uBrrp0ae3hfGS9hFMQgcH/Y27wsfl2u3mmqG63ImmX2lz2kBX8mmRPmrV52kUZ0xR6\n",
       "UZQrUbu2tmidA6cWZnIlkZMGHHTIsAVlHxgAAAMAAAMAAAMAAAMAAAMAAAMAWsEAAAi+QZokbEFP\n",
       "/taMsAAAKCQu8AEOcxU748S+lp+eiYYeTeeQ1jMqZLTXmYLXLhgtCiBjM/fJ9fr4T0Nfq7Qarfhy\n",
       "W6o5ZnzUbNlZXEq8KmbqAhAj8tKAC0odCsaClpHIBUb+uZ7FxxQNYGdmEuSe7+YerWeYBuCX/5mt\n",
       "VE7L6cg/FC0Hk9o+hbe+70WsY13liQvrXZKW+nRAefUwZCRa4hmuioe/SHhpJ9HXOxRtSQZhhgDs\n",
       "exNpuccPt5UX3KGQHwAj/d6D2wH+vhLPzDn4/AllzZScExFtnjnNcD2x4FslZFYzXbjqntFD64ag\n",
       "QcYwAQd+We7KUOwCLSfxgupQhgc0/f8N/xY1xrl2/OJuQc9LnnBfh6ORA/Pie0+JKVdd6JQ62RO1\n",
       "r8N7DYODJz2QcoT41l3qlQkunjWb8DzoP8W8MswdvQanjcyFE59cq3fnz/+/lgSWRaGrpCBgvnvI\n",
       "uDEaqZDGwygDnU0pEFCjw5DsqEmMa+c+TlgrfD9EGDceWFwXFtg7Dp/meuwb52UrEp2hsZartGb2\n",
       "5RkFevR9H+CuBaJROS9mMmPJyJF1Fca3q6l7abfmMhe1wBmz6mdiYQm3xrCx6DUZkJwGYD363BU4\n",
       "OJNwlJpTaRmYTIwekDQWqsV6C2TaooddTiBoyTWKvvLX9UYxSpW7hffO4vR4wJIAJ7JsCg94RlNr\n",
       "cFoIeRCBWSu78iGAeAaazdLZmCC373QEXxUvvO/H8ZrImf9ok6HixjCA/klyU7r3TYRy+JwRzfoM\n",
       "eQQyDreweU16eORqpyXxV6cr1ld6qCgv7QChi3FjUYtk82bVMKjAHMTzOLflsI1G/V/s7c62P4ui\n",
       "JEafM+rZ04l4MIUF//SYv0sae35yeUabWNBG/XPlferDE8V/T5kdcQ0cNWjLGtrmDV4tYC8os8EC\n",
       "awPxo3dwHoAC4ta8GjJycZZ3Q/WE6mE+N5SHcGyoc/oLcfGupkCk532fGV3c6nTO7VBPVDfMeVmD\n",
       "OyL1K6nXqdg5TBB/OYR3aXYM2tuELJldzoEMNxdiEjFBxsZcA34EKQvC3TE/WxNgLPYOOMnmZywS\n",
       "AUwUKK88CvEIzZA+4hMlwfqYyLsfaDUtAD1gaxnmjIvKTTREufySyiMA89zYFsDac9gnDTnQyAEc\n",
       "UZowSS7RQjWmKWAxESDBwSVgaETubaWIo4nWNGujRO4FCa7SRbwfTnR7hDnmzyK9NBeLR3XozEES\n",
       "fEijxmFw+7tW5NgX7anG7dQlAMw9FDoYUbqurkht7Vhdbny0TkxmfC8BlfaQlTDmjig7kNbMIPE7\n",
       "y4K+tHiFNfm5rG0LBqcSQmDOBaNWNIvGocQVoNhibyxc0v8Rcq7PEvacAVeYlsBnb+em0/sx/8Zp\n",
       "6N14bZOpfH7GIZRKrJPxmoxzLkrWyc7Hhd29Jm8LZCxh5XnyrjQ6i4JRoZBpeFClzofpjZ1ALYKV\n",
       "u+RWdZQhwO6K4wkXYpFSTeE92E/ii2iyRu9W4C8FfSIYAjnd2Wux7DxxXROCPa/BP1vTeeI5G+4Q\n",
       "qsfEgF3b46P7jUUvRJbzKmVloBXj659/mfvkbqYS9yXntnTbMpmHOTsFpOVp5o1BZqddRnyAO2Fl\n",
       "E82BUySPQqDzfiRPZdHp+eyjhhMHYsvYKAXhsl8bdIa79Y/bY5k5vgOgDmMwHwn8Ti0Ty8GyVMSo\n",
       "PMV9meSt812020yLHZ+etg9IwjQQTfqh3VbeOu6dHZ2SWBUauC8DrCOU/M6XDL2JrNQe377XTtWU\n",
       "WfKbKTYen+6c6IZGB9ozqR4TpxllwPeag44gc2Jy5x+AsWIpPHjsyL8XZn5bwh2gcZUzkY/XR8dY\n",
       "rS/KLudWxViPWw3LH2zqcpIrF8I5VxR9wowOaZ+WQAtGBVxyw9MJWzaNwG6T0GyBJsgCludMDqdJ\n",
       "ii0YAdkR2ySpgnPlqRmtZE33nNPS160TZqFxnum35PO9ntTSxePZNu/84EwNCr+y7LKB6krgukw0\n",
       "dMHvXwUhJK9Jkk+Ey72AYmUYaq8aMz2y2lxHTReC39ljnSsHjc3dZTsLNl+TreVYapR1OgUDLxL/\n",
       "7ufa6jfUu3JFWXKNQye1AypW1eXYb/12yt/AttSjyIcQZoNxtSUHDhc+iWXsExPOveW/6w34aj3b\n",
       "OMuI5EEUqsVpIK46dkzqssUPi+cC0DhWfYM08ZCsxPL2PSDKSeoXO5/ln+MRX43tiyBfbOzOWRr7\n",
       "R45NQYf8V/H2LoLouActB8LsZi126n65gDCqjOOANzeP+C+owgHB4Lfxbtg5ZntYN0c/s1P98BIp\n",
       "4Z+cMqpiXEDPvovvIityHI1UwKSb0C39qt734qSTlPfwmaX6/+j/I629CUC2X6ZrVCf3u4IkiUwk\n",
       "fNXf8Yox5+P9NIUQpX93GvIIhg/plJCf6QlKHUQGC7+6pvHNh/0kHVzlNoli82WEA1knREgJ0jKn\n",
       "X2lTyPMmIcd8Qx+obSYCUsFh+xHGx3FXdr9eGcZfKeZq140yi+508bKcPLzWgzwI5mScS9pTZWpI\n",
       "svOm5WynolevOOUVLVAWzA570/PKVj7GTLhTFvuO7pIr4y3K6gdCqbpqb2lVgsc1YzV+U5Nr+8V7\n",
       "eiLqVAyNH7WGGbPn2/NhlMlz8qS0c71uUDjWP1+tk2Q5LkdCG4k+/hJZG6hyFKEBaP0zXieGTgUl\n",
       "zqgEKOt/lIGNuwqwX33LJbLbhDfy58jjLXUHOmQspltA7KhxHCA9mZDUll8v4KybPIdf7uf6Uo4s\n",
       "ooMF8nfgoccj5gAcX4q7j7dvwolHWHByoM/ZRnDc+8ZTEf2cM3ts/P35I6tnfl5QMtIHn4zBK2sR\n",
       "kqSqNoltHdPdEh1oyJN+zRCgW+9KknDc8GhXOSicrfMIQJmtGoeqxqoism429+TSdsnk+0c/D/lc\n",
       "6wyxubUY7Ac51v9aZxhlB39L7+w4khUT1TLyeCcF9Dv2+nvJ/oVGs1w6yEhcV8BSbf5pw06DJAW2\n",
       "SlD4l0AAAOmAAAAF7EGeQniCnwAAAwAbjIPcF5AglrS/iAbWjvo+UEAALdJol6/IzzGscyBmEYDV\n",
       "b62bDPffKMYRqM1FZnVvb8H/C4CCZt0iXu9p8kP75D4l3NUkS34xnMPZm2Ua80v5UmNh1ro8ApxC\n",
       "BogkpNzxA9eByVeM4zo/HCdQVB2GbzTUZgX1LhqPKnCeOKm7PFnenAiX8O+El/0/ZTLA17/O9fn8\n",
       "eIJ2PrDnCTTanKLppaiOgRV+b8E/ROZi9SOl2PhZ0Q2u/DNUnUhsPesRh+gGnOcJuvsAX0GW+ziO\n",
       "i8JmZ1cc5p6xoK3aMAP8CeD3Khr2GZRz18Mjy2g9cO3vyO2w0oL32kGBNWDmcCPm6QAoWnCnh2d8\n",
       "3S4L7nROFYr5SMOZLsiCxb8cm5ZtfxbCvEQltjdABnSoW/+avxr3FmVhu1u0PwMIqNK+FpWpSRO/\n",
       "DJDpApDnd0qUNH9iClYHUB3asvkDD3cX8bqPEPr5hAUsSAFPuAIRa7CiRfxNVgMxBpjHq0DGAoiB\n",
       "IdV8pf0G2pIm0QeZiYA8Ym3ixdNw9GW4DisHQD9ckOtNvDpgI1nlUvFkW3FzYGIAZHXS9UgHyiAC\n",
       "JoYdS6t9gNwid0pCfiS5gaTXABliDCtckO3genTx8ncOPlGeh1DkFdEJulJadM2HD1fTluvAbXVa\n",
       "/AoKaHkG9LUEyF+R/1QMC5NowYduqYll+8Cx6nrh/pl4EPlxqhfUMmSH2ToVSAOalsHJ+PKo2t7z\n",
       "rwWuJi+yICTf7D3zXV4NOxAUUJ0G4sPUHyK+CMZW8SBHf5/hgtdL/uOKBQ9nQNkgfcgGhM6kbBBZ\n",
       "SoOMkVdD/9ETO4vA+/fcBwBLRsEtA+CmAgNjh351yPd8xvGJV2i++NBFXCfIM0J97+0tPYpR4ISt\n",
       "wq/KVg3QwFG5j5v12MpiikJpFIRwuefbG+xkWlaDvJOfVtuLN+xiITiuLvajyVAnqsw4PFrZCmIB\n",
       "Bqxs1VqWZQ9zoKgvcc8wzuyJ3cwlsyNIwy5aLqTLbclavtKUhkAXBziIi5h4RewvDfL/AP/euiBT\n",
       "LsXrBN2vL2j9h8W/zaYDHpK4tVoE0Y1DZ3gFBkYDTTpss10w7xsA7d7LFzy84CK0ty5Zhg7SmB4I\n",
       "nL8R6nNiK29pfSp/jsJ4QR36b1S7MQZfrqvRCCSD+6Eict+kB9wg0jAw+QJT15XMooo1nYtxCF8y\n",
       "s57bN23i8K5cRH6HCmYPvVteQpo4hIoeveBwdUohL9qDbHubnWGqCAqHv9I4z31/LjWPytIg5cey\n",
       "6YLi3IiFH4K44CIkv+r4QvOaPRqnkvkBZvoyVfZ3qlPZFzU8YEAnWBcHrIIEkJKlp/2abSIKaDg3\n",
       "d9Bc2KWv6FrgMzlT8eYwMAUrIFFPu3of8A/mU/CqoscDMQDLHLoy2F4QICn1cqfkBBx6ZZbstYlM\n",
       "YV0yzkcUSfM23zQVi03T69xQ8OlK1Ca304Ucyatd2Hwe1k2U1rCUOF9O82qYcq0XsBZ7/w8iSmdj\n",
       "lagmePZnrVjCBFiS/Hj+ICIgcujpI9+Wr9K+55s22d4VilnPWaEATvBJ/q1CZO5qocvRjMz1qTNC\n",
       "rA7hVnA1hghgnN0i5XDzHNlP0eCkolvVmEHbEjmwFMTng4QMZ6yYlMDlcuO4V12MXiMtoTuEKtqF\n",
       "aXLiwzT2b1Su4R3X88ovlu1PsY3SSCDCwZzO27jo4p6KUBfHoDR3ATA5vRX09en7RQk3PPoYkd95\n",
       "GI9DiDVK0VOwOSJpprhCnczB9EJBi6PJ1SiCq37buqEhL4jcTD8w0nFEve84Y8Y520FqdiND3YLH\n",
       "hBZ70jXosGzZkyxX451C7M2OR6OrhvYiyT3EoYu4pZL1SFW/eWjm1sY2Ic1yBd6O+ijouyjoOlwK\n",
       "90GNTs2qso7Inu1Uf2xIBJrNgQhmzt2ILXVaPrQF6f6V30CmDHcj41fJx6GQQws0VJHvYaOVDNzx\n",
       "87e+28ZwCD5WIYLLKe7bpt7noEcAmn2C/hqVLzJLAQnFyGDIohv58eRDii6ABH0AAAXkAZ5hdEEv\n",
       "AAADAE1EObkihXlkA8UkEkABtLdKQPDk8brAaLIUwQMLxAN/+F8UYqo73sDKrUMRiPgckVZBX26D\n",
       "tUkJi8ODYGaBI3bmjI0aTNfh2+mejvNmfokzxWVorH3YWiZr8DB7wW4p2XrHT5B2yPSR0O76oxlN\n",
       "4/46oO3NW0oxiyC6jS2nHcndn/saW4wA+OOn80qaZcpj2+HPdS/hYnsP7t/SGQ/qer8zTpQ0O78f\n",
       "bZ+eoHFHGP8nfZXMfdXH3me2ROTchg0N7dkALtPuqfY9pVHfxdfCC8UO4u5p7HdQyEd9reqFfrZ1\n",
       "KQE5WSnsdMWa6c3Tjbw6I63e5/NwDciRh3bkYTDXd2yKOXPO7HIcV74pCrWs7dPKPYqlvBHrTFem\n",
       "KvUPOlxAez6ZAmF86mAgK1huKZKfhj+86C2wfqmTozVbiDQis8X89pzzi1tnyPu+Hph6Rinfef7V\n",
       "w9tdI5i0/BF9hBQYFtLMM+l/ktRbFHNNDH/7Uv+nfWc4xgG6cueGliQz8raj3b/jyEg0gXKtHFvp\n",
       "jr6p8sApWP13fTgbcbMysB8OLiurzGG9Gs2iwshJw5TCzcZh3nPqeEMJdySYGPxaBdmMuacsWdmC\n",
       "8sptTrq7C5kRsLjLpC27xZzT4jg6tBgwE79SJ93BSjDwXRnlOG1vkRrOxG40wbidK0eRDpCpVcR/\n",
       "73iVD473nm1R2hYTP87FGgMZ44b6wD+i53wfyivnq48vO+irYno9kB+tEMAzXWCXNeIXqCLkFv4W\n",
       "nLiE/CohKf1QKRktlhIhwmFmUajGaMDecr6dVI+kk+H2WEdBOWfojYBe9OMku9gPvIRRf2d1q499\n",
       "BoI89M5u4Pbgf+pzR5ZXsajoapufVdJ7FDltp0coNYEo/MpMygNZQyFqgPotCzUzlOsoPBo0WuQK\n",
       "sNtaYD7P70zRZ4huk0nCjonVpw2jHdpPNBnFp53zj5oAUOabLY3HPAwwry0WAkN6Pt0KPvsXUyyK\n",
       "FdeXEhLRGwOHRjSPPwnsFIuOwoard9aa5fcHyIMPiSPrvcVbWDJi0CXWHRrVeYhfEo1l3wYCqByw\n",
       "oFUHZ4yjFHPq74lK0YVol9jDh015461GLvghXXZ3Azu3G7W3d7YDLsuTeyYKa/XU+LmXkbZcykSx\n",
       "Cli9xnWdeRVYVO8f2bu5I8egOmkh6lA0dIrasYT+ajfUobjnyZLPy4HroCHD0KPUPizaZMfgBXXE\n",
       "LTkD2shVGv1Cf5Rv0RSm+BrANdiNGn1KjxeUMEqcyw8NmBuk63uTzTSr3ZBbnPuOE962DTYZ5/dQ\n",
       "aChHF2LN0sJA8qGqrTeHtkJHXU9YL1tARXN/Ai2yOaPEOv6q4tyroCxhVcZBNTfDRhNRPUxiHetF\n",
       "n0sS3go+R71rtTKu7ct9vBOClZeWMSHoYED9xofKRNYH802kAQ5YMirIzX+iwhxpIN15256huinX\n",
       "yyHJAELYaL/VAZY7gV1uzYkVGVJxVSweI9jk7YQ4W0GIeIWRnhzcRVis4sQ1TzOFOQ2sr295U4s6\n",
       "ibZmFZW6aU2G4d27Y2GD6e2nw5e7gtb/JTW9U8YMjJH8VsagU5guDyvVz+x6IPUlaLcM1iPGO8Zz\n",
       "iEUQs3mc5O8w4dDeOh0s6ILTWH37qFUNIC8mGzm4wv4IFavTcGxveh7wTgzf/wHcLQIEGLD2lL6A\n",
       "FWGWTGdxJJOqS5FVGdEenD0b8Sh6FMONy45G9clX7fUBmrDEI8GxqOLAEGGOCg5LkvjyWHfGdCA2\n",
       "VKI5SywWfiqbRRuMTzLPXygjcKATGI43tKp94TUBgxmpZva+zJ2fkgdMc74vImCAoI9fZUot3mV2\n",
       "k7HZEYtu3qJjFz+wGYQO0+mbnvjJNLBSCAIuyGieMhwp0JbjoKivmKYXFVuC4kdWx8y3RzIURxCZ\n",
       "XEFNRwsmKNSaBDakWYZ8PqPh34liS6c1z5SJ6M8ogv8mriSl+9u4b+oL/SUdSsj2HDLZFkMml1Ug\n",
       "bUm23+7WfJSs9/j8gG9jeuAAJuAAAAT3AZ5jakEvAAADABz0u3LQY/66lQkAA/4lg33OjSBChcTD\n",
       "WaPlOTjuIwXbKrAtzsh9KHaXeHFJPP9jehZUCFZ+Yc8J7OStXcz4736tM3uraAgsq+qwQ4Yh7twK\n",
       "KypSyhctJOAvxIcSkUGg3YAngYgacBnKXmkWxW1JbdqHgGDf97fgtst1VreTaBVRWZ6XQuSF9iMo\n",
       "ITzW3F0hTq750pE7jIlooo2zI9ZxkXQcPtSy1v6GtB4LyGDM+ugSeTtVGpG6CCz+COMcs3zi20vJ\n",
       "WekVmLrWOS3iK7brH9YZI6UEKx/cPAIt6Ut5mbGthCIiWZ6I2+3I6EzRQkonN/skQw8etfsXUoXF\n",
       "phsqk6oTkWbtkRn84L681QugaR5sJVIkmblmoiHHjUlZ7q5j3DUwbghTQF3TANBaDmtB9bwxC3a4\n",
       "ufqmCE3I+uHgcGJIFYOmYFKOeyCRxqmNCsnRCMqW07I5XWtvAbHrojVx1oCSkmgwKTt2Q/6ffP8J\n",
       "ZMVV6Mt/GRUcGcjXNoP+wCiwy8FGYTTTi+Y6jP43Tz/i1vMOEWuOdFdj/fG9OkqD0TOMKkgU85uG\n",
       "Yjb7QOQ5Z3uHjxpk1QFjgxWZuRNtO7IcBMoR7AgCi9UipeAOyAAvN3wX0OVqwpsULE1M3USgzI21\n",
       "QJd3eHGAOXLNZBqvde75vQIvPTCI8tYAPrJzSKpzVchzrCP+5G1najhNyrWrAH04lMoVf/S/sdAT\n",
       "1wdGJ7jzqgVDZO16rad+odCZIomWe5MU/kT4hpzzQu0rCSXa3UzFKS/Oy8UnJ/LXbsaKM5bsIdvq\n",
       "EodWPJy2GJCzRhrjCnLAKZkP8vMOlQG2OvV7/txiH1ueZafgS7L2YwAuIEmodqG3KLWOydAHvUqi\n",
       "dS45TaMFHf6j7uFV1jwfKkWaBGwkLe7D+fKhl5NdMffGMTxL4HsLjjKj0xb09c9mXOZ/69bifI7V\n",
       "R4gWGJcjEXMwKqIRnWhE2B+pLiyNwPyHThFQiuejdKojJ0tgpGH5yo3K3mVutxHBgUXPxkEyZdJM\n",
       "g9Jd7bQ8hf/WkLNUQrlyWmoWw6cs8USL495Oja3Cvt8VjeMHwBbQkzCau/XYihyY52xs9tPaWUZu\n",
       "OigyaxIn2AesqUKNxnNY4iZ2GTh9Z9KGnMuXet074Fyvm+XnbiTDG3p20/IAaDogBSn7+9uO6kKf\n",
       "W7ze2ztzwZt4HhZ0Y8X/wIxXumYAbk65iTJ/6MuiRnts5adnvMJl6OTkCxL4P+ESI/CX8+dpim4c\n",
       "ZGHrYidWczJyhdXYjHr3HUXFKqXar8iBrcjdPwsyQ+dKp7ylyVDapzaoGW14jmOnvIPYrYK767fu\n",
       "WtBejd9pCevZtDokwKvhGQz/Zh6JIIhFa2+mNejDWkCKBpH8auThvA0LffOOHOsil0jCleCBqlTW\n",
       "0U0FsKDlKAL3uHXZdCQJxK5r7y91hQa+22UOdZBh7A5GRHlrz1nLnrqJYMtBRj1U+STBot0VeriC\n",
       "PfMwrRTaXHgJ2BtdVZ1X13JWB3QIgAWdSZ6vJPwl8rIzeplxw4WyDwwkOBm5QaJKlZa1UX5KG1wK\n",
       "y730/filPkCfMk7xxGey/5Q7sNuwHJrCUeDXVdTZ9oyLZpqfsV6sbUZC5YEJBvYl9qs9sX37vKy1\n",
       "lqPi/sSBqATruzSLhX8Yc6LklwchXs6LDV4yZA/4yyFH3IHhs5AAIOEAAALTQZplSahBaJlMCCX/\n",
       "/rUqgAAAOOV5s4RAAvPKiFhRi+eI7guC0rf64J4HdtQ0LmzPGof3L8426HwEh7uZYwW6Yq9dGyk2\n",
       "cm7+L2uAA6FNcjCxJPspyre0MjQ+3pszXQu7Ht5lfMbevqUORa2lsxFMqXaRBTskUyWdtlueWm5B\n",
       "TckVZGedKtQWlMGL+P8ajtGYIR/t61QfJ64hJd4dSaFsgWxW45hE3WE5XgEfXh24l+OElQzb8RL/\n",
       "4ryhcE6rf3lLIRjzBch2gg3/+HqPXaUjC2izEXTKZKvaGeXldrkeXQJQAvzQ9cgaSVrZMLGYEBwl\n",
       "LJsGDiIfYIvr6p02KQUfYBHhKouSfnftgNtzv42aZezbEZ74zfDOxYehUDcPtc27f0BpZvqazSMy\n",
       "VhMvLQH6lXwod4x/3eBBydD+0e/R5umS3H6tWjUvM0YpeKE4vmqjBMzNf1b+OauE4nO1m2AX2rE+\n",
       "ok88dH0nUc3bTf9sU3OGhNjFCOCzIzwCO9L+SjnzE4cD4t2u/EvzAojStvfgoQruuYlI4VvodVLt\n",
       "qjpa7KVLWmtPrq2B7HpbnwCOQW5D6UraKPBVMbFdH03ccfet//YVQ9ngEe3Km2P86fnsXyW3niOC\n",
       "+xNtFk1Lw0lfh/ikRCBtPYFG1JLgt3GKdXqwnTAtPxzSjYTVGOP7/TRuhnrPXUHTEuPPj9QY7Xu4\n",
       "paN1IqTSK7IPxFn9UwuXchVW506343KTIeRoCKiv69HLRXhiLOwnmlqfRSkXiZ8H0eXI2Tii2uAA\n",
       "PZpmnV+ekScu+fDLO+IWwTBPrQnhisJjB4+y0HOIy8n9mMK5gIG2qjj/S2R5S32tn/cd4wyQQr3I\n",
       "t48He9ueBdwnW04Bweqjcd/rPtUJmMDpRStV9UKBmj6AvF3sbXBZhI11rdiECcInSxzwJfxZPTQI\n",
       "vngFV9ikXgxP4XozbfaXQe8vk7MmwMLTG5D5AAADe21vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAA\n",
       "A+gAAAu4AAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAA\n",
       "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAKldHJhawAAAFx0a2hkAAAAAwAAAAAAAAAA\n",
       "AAAAAQAAAAAAAAu4AAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAA\n",
       "AAAAQAAAAAJYAAABkAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAALuAAAQAAAAQAAAAACHW1k\n",
       "aWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAQAAAAMAAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAA\n",
       "AAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAAAchtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGlu\n",
       "ZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAGIc3RibAAAALhzdHNkAAAAAAAAAAEAAACo\n",
       "YXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAJYAZAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAA\n",
       "AAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADZhdmNDAWQAFv/hABlnZAAWrNlAmDPl4QAAAwAB\n",
       "AAADAAQPFi2WAQAGaOvjyyLA/fj4AAAAABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAYc3R0\n",
       "cwAAAAAAAAABAAAABgAAIAAAAAAUc3RzcwAAAAAAAAABAAAAAQAAAEBjdHRzAAAAAAAAAAYAAAAB\n",
       "AABAAAAAAAEAAKAAAAAAAQAAQAAAAAABAAAAAAAAAAEAACAAAAAAAQAAQAAAAAAcc3RzYwAAAAAA\n",
       "AAABAAAAAQAAAAYAAAABAAAALHN0c3oAAAAAAAAAAAAAAAYAABR7AAAIwgAABfAAAAXoAAAE+wAA\n",
       "AtcAAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAA\n",
       "AG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTgu\n",
       "NDUuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(anim.to_html5_video()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
