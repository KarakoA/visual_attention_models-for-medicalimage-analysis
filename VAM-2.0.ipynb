{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Depdencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchviz matplotlib numpy tqdm scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Dataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "import skimage.measure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import unittest\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetName(Enum):\n",
    "    MNIST = 1\n",
    "    AUGMENTED = 2\n",
    "    TRANSFORMED = 3\n",
    "    AUGMENTED_MEDICAL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseSensorConfig(object):\n",
    "    def __init__(self):\n",
    "        self.patch_size = 14\n",
    "        self.scale      = 4\n",
    "        self.n          = 2\n",
    "    \n",
    "class GlimpseNetworkConfig():\n",
    "    def __init__(self):\n",
    "        self.output_size       = 28\n",
    "        self.hidden_layer_size = 32\n",
    "        pass\n",
    "    \n",
    "class CoreNetworkConfig():\n",
    "    def __init__(self):\n",
    "        self.lstm_hidden_size   = 28\n",
    "        #equal to glimpse output size\n",
    "        self.glimpse_input_size = 28\n",
    "        self.num_layers         = 2\n",
    "    \n",
    "class LocationNetworkConfig():\n",
    "    def __init__(self):\n",
    "        # equal to LSTM hidden size\n",
    "        self.input_size  = 28\n",
    "        self.hidden_size = 100\n",
    "        self.std_dev     = 0.05\n",
    "    \n",
    "class ActionNetworkConfig():\n",
    "    def __init__(self):\n",
    "        # equal to LSTM hidden size\n",
    "        self.input_size   = 28\n",
    "        self.num_classes  = 10\n",
    "        \n",
    "class BaselineNetworkConfig():\n",
    "    def __init__(self):\n",
    "        # equal to LSTM hidden size\n",
    "        self.input_size  = 28\n",
    "        self.output_size = 1\n",
    "    \n",
    "class TrainingConfig():\n",
    "    def __init__(self):\n",
    "        self.num_glimpses     = 8\n",
    "        self.batch_size       = 50\n",
    "        self.learning_rate    = 0.0005\n",
    "        self.num_epochs       = 200\n",
    "        self.model_patience   = 100\n",
    "        self.plots_each_epoch = True \n",
    "        self.dataset          = DatasetName.TRANSFORMED \n",
    "    \n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.glimpse_sensor_conf   = GlimpseSensorConfig()\n",
    "        self.glimpse_network_conf  = GlimpseNetworkConfig()\n",
    "        self.core_network_conf     = CoreNetworkConfig() \n",
    "        self.action_network_conf   = ActionNetworkConfig()\n",
    "        self.training_conf         = TrainingConfig()\n",
    "        self.baseline_network_conf = BaselineNetworkConfig()\n",
    "        self.location_network_conf = LocationNetworkConfig()\n",
    "        \n",
    "        prefix = \"\"\n",
    "        self.model_name = f\"{prefix}{self.training_conf.dataset}-{self.training_conf.num_epochs}-{self.glimpse_sensor_conf.n}*{self.glimpse_sensor_conf.scale}:{self.glimpse_sensor_conf.patch_size}x{self.glimpse_sensor_conf.patch_size}_{self.training_conf.num_glimpses}\"\n",
    "        \n",
    "# global variables\n",
    "global_gpu_run = torch.cuda.is_available()\n",
    "global_device = torch.device('cuda' if global_gpu_run else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMedicalMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Augmented mnist meant to mimic whole-slide-images of tumor cells.\n",
    "    9's represent cancer cells. There are 4 different labels, based on the number of 9's:\n",
    "    \n",
    "    zero 9's          - no cancer\n",
    "    one 9             - isolated tumor cell\n",
    "    two 9's           - micro-metastasis \n",
    "    three or more 9's - macro-metastasis\n",
    "    \n",
    "    Each image contains between 3 and 10 cells at random, which may be overlapping.\n",
    "    It consists of 5000 items of each category(total 20.000) for training and 500(2.000) of each for testing\n",
    "    of size 256 x 256. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 root_dir,\n",
    "                 train,\n",
    "                 mnist_transform = None,\n",
    "                 transform = None, \n",
    "                 total_train = 20000,\n",
    "                 total_test = 2000,\n",
    "                 n_partitions_test = 1,\n",
    "                 n_partitions_train = 5):\n",
    "        \n",
    "        self.mnist_transform = mnist_transform\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.total = total_train if self.train else total_test\n",
    "        self.n_partitions_test  = n_partitions_test\n",
    "        self.n_partitions_train = n_partitions_train\n",
    "        self.dir = os.path.join(root_dir,\"MEDNIST\", \"train\" if train else \"test\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.__create_dataset_if_needed()\n",
    "                \n",
    "        self.__load_data()\n",
    "        \n",
    "    def __dataset_exists(self):\n",
    "        # mkdir if not exists\n",
    "        os.makedirs(self.dir, exist_ok = True)\n",
    "        len_files = len(os.listdir(self.dir)) \n",
    "        if len_files > 0:\n",
    "            print(\"Data existing, skipping creation.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Dataset missing. Creating...\")\n",
    "        return False\n",
    "            \n",
    "    \n",
    "    def __combine_images(self,images, output_dim):\n",
    "        \"\"\"\n",
    "        Combines the given images into a single image of output_dim size. Combinations are done randomly and \n",
    "        overlapping is possible. Images will always be within bounds completely.\n",
    "        \"\"\"\n",
    "        np_images = np.array(images)\n",
    "        input_dim = np_images.shape[-1]\n",
    "        new_image = np.zeros(shape=(output_dim,output_dim), dtype = np.float32)\n",
    "        for image in np_images:\n",
    "            i, j = np.random.randint(0, output_dim  - input_dim, size = 2)\n",
    "            new_image[i:i+input_dim, j:j+input_dim] = image\n",
    "        return new_image\n",
    "    \n",
    "    def __get_cell_counts(self, items_per_class_count, class_index):\n",
    "        # exclusive\n",
    "        max_items = 11\n",
    "        min_number_of_cells = 3\n",
    "        # 0,1,2,3+ for no tumor cells, isolated tumor cells, \n",
    "        # micro-metastasis and macro-metastasis respectively\n",
    "        num_tumor_cells = class_index if class_index != 3 else np.random.randint(3, max_items) \n",
    "\n",
    "        num_healthy_cells = max_items - num_tumor_cells\n",
    "        if num_healthy_cells + num_tumor_cells < min_number_of_cells:\n",
    "            num_healthy_cells = min_number_of_cells - num_tumor_cells\n",
    "\n",
    "        return (num_tumor_cells, num_healthy_cells)\n",
    "            \n",
    "    def __generate_for_class(self,\n",
    "                                   items,\n",
    "                                   items_per_class_count,\n",
    "                                   class_index,\n",
    "                                   uid,\n",
    "                                   all_tumor_cell_images,\n",
    "                                   all_healthy_cell_images):\n",
    "        for _ in range(items_per_class_count):\n",
    "            num_tumors, num_healthy = self.__get_cell_counts(items_per_class_count, class_index)\n",
    "\n",
    "            healthy_idxs = np.random.randint(0,len(all_healthy_cell_images), num_healthy)\n",
    "            tumor_idxs   = np.random.randint(0,len(all_tumor_cell_images), num_tumors)\n",
    "\n",
    "            cells = np.vstack((all_healthy_cell_images[healthy_idxs], all_tumor_cell_images[tumor_idxs]))\n",
    "            image = self.__combine_images(cells, 256)\n",
    "            image = np.expand_dims(image, axis = 0)\n",
    "            self.data.append(image)\n",
    "            self.labels.append(class_index)\n",
    "            uid += 1\n",
    "        return uid\n",
    "            \n",
    "    def __create_dataset_if_needed(self):\n",
    "        if self.__dataset_exists():\n",
    "            return \n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # in how many partitions to split dataset creation\n",
    "        partitions_count = 10\n",
    "        \n",
    "        # number of classes in output (fixed)\n",
    "        num_classes = 4\n",
    "        \n",
    "        mnist = torchvision.datasets.MNIST(root ='./data',\n",
    "                                           train = self.train,\n",
    "                                           download = True,\n",
    "                                           transform = self.mnist_transform)\n",
    "        \n",
    "        mnist_loader = iter(torch.utils.data.DataLoader(mnist, \n",
    "                                                        batch_size = int(self.total/partitions_count), \n",
    "                                                        shuffle = True, \n",
    "                                                        num_workers = 0))\n",
    "        uid = 0\n",
    "        \n",
    "        for _ in range(partitions_count):\n",
    "            batch, mnist_labels = mnist_loader.next()\n",
    "            # 9's represent tumors\n",
    "            all_tumor_cell_images = batch[mnist_labels == 9]\n",
    "            # everything else except 6's healthy cells\n",
    "            all_healthy_cell_images = batch[(mnist_labels != 9) & (mnist_labels != 6)]\n",
    "            \n",
    "            items_per_class_count = int(self.total/(num_classes * partitions_count))\n",
    "            \n",
    "            for class_index in range(num_classes):\n",
    "                    uid = self.__generate_for_class(class_index, \n",
    "                                                  items_per_class_count,\n",
    "                                                  class_index,\n",
    "                                                  uid,\n",
    "                                                  all_tumor_cell_images,\n",
    "                                                  all_healthy_cell_images)\n",
    "        self.__store()\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, uid):\n",
    "        if torch.is_tensor(uid):\n",
    "            uid = uid.tolist()\n",
    "        label = self.labels[uid]\n",
    "        sample = self.data[uid]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return (sample,label)\n",
    "\n",
    "    def __store(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        \n",
    "        assert(len(self.data) == len(self.labels))\n",
    "        max_index = len(self.data)\n",
    "        partition_size = max_index/n_partitions\n",
    "        for i in range(n_partitions):\n",
    "            start,end =(int(partition_size * i), int(partition_size * (i+1)))\n",
    "            partition = np.array(self.data[start:end])\n",
    "            np.save(os.path.join(self.dir, \"part_\" + str(i)), partition)\n",
    "        \n",
    "        np.save(os.path.join(self.dir, \"labels\"), np.array(self.labels))\n",
    "    \n",
    "    def __load_data(self):\n",
    "        n_partitions = self.n_partitions_train if self.train else self.n_partitions_test\n",
    "        data = []\n",
    "        for i in range(n_partitions):\n",
    "            data.append(np.load(os.path.join(self.dir, \"part_\" + str(i)+\".npy\")))\n",
    "        self.data = np.vstack(data)\n",
    "        self.labels = np.load(os.path.join(self.dir, \"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(Enum):\n",
    "    TRAIN = 1\n",
    "    VALID = 2\n",
    "    TEST  = 3\n",
    "    \n",
    "class DatasetLocator():\n",
    "    def __init__(self, training_conf: TrainingConfig):\n",
    "        \n",
    "        self.dataset = training_conf.dataset\n",
    "        self.batch_size = training_conf.batch_size\n",
    "        train, valid, test = self.__load_data()\n",
    "        \n",
    "        self.dataset_dict = {\n",
    "            DatasetType.TRAIN: train,\n",
    "            DatasetType.VALID: valid,\n",
    "            DatasetType.TEST: test\n",
    "        }\n",
    "        \n",
    "    def __f(self,image):\n",
    "        np_image = np.array(image)\n",
    "        input_dim = np_image.shape[-1]\n",
    "        new_image = np.zeros(shape=(60,60), dtype = np.float32)\n",
    "        i, j = np.random.randint(0, 60  - input_dim, size = 2)\n",
    "        new_image[i:i+input_dim, j:j+input_dim] = np_image\n",
    "        return new_image\n",
    "    \n",
    "    def __transformed_mnist_transformation(self):\n",
    "        return transforms.Compose(\n",
    "            [torchvision.transforms.Lambda(self.__f),\n",
    "            torchvision.transforms.ToTensor()])\n",
    "\n",
    "    def __augmented_mnist_transformation(self):\n",
    "        return transforms.Compose([\n",
    "            torchvision.transforms.RandomAffine(degrees = (-180,180),scale = (0.5,1.0),),\n",
    "            torchvision.transforms.ToTensor()])\n",
    "\n",
    "    def __load_data(self):\n",
    "        train_total = self.__load_dataset(True)\n",
    "        test = self.__load_dataset(False)\n",
    "        \n",
    "        train_length = int(len(train_total) * 0.9)\n",
    "        valid_length = len(train_total) - train_length\n",
    "        (train, valid) = torch.utils.data.random_split(train_total,(train_length, valid_length))\n",
    "        return (train, valid, test)\n",
    "    \n",
    "    def __load_dataset(self, is_train):\n",
    "        if self.dataset == DatasetName.MNIST:\n",
    "            transform = torchvision.transforms.ToTensor()\n",
    "        elif self.dataset == DatasetName.AUGMENTED:\n",
    "            transform = self.__augmented_mnist_transformation()\n",
    "        elif self.dataset == DatasetName.TRANSFORMED:\n",
    "            transform = self.__transformed_mnist_transformation()\n",
    "        elif self.dataset == DatasetName.AUGMENTED_MEDICAL:\n",
    "            return AugmentedMedicalMNISTDataset(root_dir='.', train = train,mnist_transform = augmented_mnist_transformation())\n",
    "        return torchvision.datasets.MNIST(root='./data', train = is_train , download = True, transform = transform)\n",
    "        \n",
    "    def data_loader(self, dataset:DatasetType):\n",
    "        should_shuffle = dataset == DatasetType.TRAIN\n",
    "        data = self.dataset_dict[dataset] \n",
    "        return torch.utils.data.DataLoader(data,\n",
    "                                           batch_size = self.batch_size,\n",
    "                                           pin_memory = global_gpu_run,\n",
    "                                           shuffle = should_shuffle,\n",
    "                                           num_workers = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = DatasetLocator(global_config.training_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = iter(locator.data_loader(DatasetType.TRAIN)).next()\n",
    "images = images[0:4]\n",
    "labels = labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(images,labels = None):\n",
    "    if labels == None:\n",
    "        labels = [\" \"] * len(images)\n",
    "    elif isinstance(labels,torch.Tensor): \n",
    "        labels = [label.item() for label in labels]\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(25,25))\n",
    "    for (a,image,label) in zip(axes.ravel(),images,labels):\n",
    "        a.imshow(image[0].numpy(), cmap = plt.cm.gray)\n",
    "        a.set_title(label, fontsize=20)\n",
    "\n",
    "    fig.tight_layout()\n",
    "show(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-isBN9KdNzI"
   },
   "source": [
    "### 3.1 Glimpse Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_position(positions, dim):\n",
    "    \"\"\" \n",
    "    Given positions `positions` [0, dim] and image dimension `dim`,\n",
    "    converts them to normalized positions [-1, -1]\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    positions: 2D Tensor (B x 2)\n",
    "    dim: Int\n",
    "    \"\"\"\n",
    "    res = (2 * positions / float(dim)) - 1\n",
    "    return torch.clamp(res,-1,1)\n",
    "\n",
    "def inverse_normalized_position(norm_positions, dim):\n",
    "    \"\"\" \n",
    "    Given normalized positions `norm_positions` [-1,-1] and image dimension `dim`,\n",
    "    converts them to positions within the image [0, dim]\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    norm_positions: 2D Tensor (B x 2)\n",
    "    dim: Int\n",
    "    \"\"\"\n",
    "    res = (((norm_positions + 1) * float(dim)) / 2).int()\n",
    "    return torch.clamp(res,0,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseSensor:\n",
    "    \"\"\"\n",
    "    Extracts retina-like representations of given images.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GlimpseSensorConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        n: Int          - number of patches to extract\n",
    "        patch_size: Int - size of a patch\n",
    "        scale: Int      - scaling factor for each lower resolution \n",
    "        \"\"\"\n",
    "        self.n = config.n\n",
    "        self.scale = config.scale\n",
    "        self.patch_size = config.patch_size\n",
    "    \n",
    "    def extract_glimpses(self, images, location):\n",
    "        \"\"\"\n",
    "        Extracts a retina like representation of the given images `x`, \n",
    "        centered at `location` and containing `n` resolution patches of size (`patch_size`, `patch_size`), \n",
    "        where each has a `scale` times lower resolution than the previous.\n",
    "        \n",
    "        Currently images of only 1 Channel (greyscale) can be processed, \n",
    "        but extension to coloured images is not hard.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        x: 4D Tensor (B x C x H x W) - The image mini batch\n",
    "        location: 2D Tensor (B x 2)  - normalized positions in images\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        4D Tensor (B x n x scale_size x scale_size ) - extracted glimpses\n",
    "        \"\"\"\n",
    "        \n",
    "        B, C, H ,W = images.shape\n",
    "        l = inverse_normalized_position(location, W)\n",
    "        patches = []\n",
    "        for patch_index in range(self.n):\n",
    "            current_scale = self.scale ** patch_index\n",
    "            size = self.patch_size * current_scale\n",
    "            patch = self.extract_patch(images, l , size)\n",
    "            patches.append(patch)\n",
    "        \n",
    "        # for each patch in patches make sure size is equal to patch_size\n",
    "        patches_square = []\n",
    "        for patch in patches:\n",
    "            size = patch.shape[-1]\n",
    "            stride = int(size / self.patch_size)\n",
    "            squared = F.avg_pool2d(patch, stride)\n",
    "            patches_square.append(squared)\n",
    "        # stack and bring in desired dimensions\n",
    "        return torch.stack(patches_square).squeeze().transpose(0,1)\n",
    "        \n",
    "    def extract_patch(self, images, l , size):\n",
    "        # shift image by half-size, effectively making centered l coordinates into top left coordinates\n",
    "        # and at the same time handling out of bounds problems\n",
    "        half_size = math.ceil (size / 2)\n",
    "        images = F.pad(images,(half_size, half_size, half_size, half_size))\n",
    "        \n",
    "        # x,y tuples\n",
    "        start = l\n",
    "        # shifted x,y tuples\n",
    "        end = start + size\n",
    "        \n",
    "        n_images = images.shape[0]\n",
    "        patch = []\n",
    "        for i in range(n_images):\n",
    "            patch_i = images[i, :, start[i, 1] : end[i, 1], start[i, 0] : end[i, 0]]\n",
    "            patch.append(patch_i)\n",
    "        return torch.stack(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some images by looking at the center\n",
    "gs = GlimpseSensor(global_config.glimpse_sensor_conf)\n",
    "glimpses = gs.extract_glimpses(images,torch.zeros((images.shape[0],2)))\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    scales = [(gs.scale ** n_i) * gs.patch_size for n_i in range(gs.n)]\n",
    "    l = [f\"class: {labels[i].item()}, {images[i].shape[1]} x {images[i].shape[2]}\"] + [f\"{scale} x {scale}\" for scale in scales]\n",
    "    to_show = [images[i]] + [g.unsqueeze(0) for g in glimpses[i,:,:,:]]\n",
    "    show(to_show,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zV38KcPXdNzX"
   },
   "source": [
    "### 3.2 Glimpse Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a location and encodes a `where` and `what` feature representation, which are combined together. \n",
    "    Consists of a combination of convolutional, max pooling and fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 glimpse_sensor_config: GlimpseSensorConfig,\n",
    "                 config: GlimpseNetworkConfig\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        n: Int                   - number of patches to extract\n",
    "        patch_size: Int          - size of a patch\n",
    "        scale: Int               - scaling factor for each lower resolution \n",
    "        \"\"\"\n",
    "        super(GlimpseNetwork, self).__init__()\n",
    "        self.glimpse_sensor = GlimpseSensor(glimpse_sensor_config)\n",
    "        self.glimpse_output_size = config.output_size\n",
    "        self.hidden_layer_size = config.hidden_layer_size\n",
    "                \n",
    "        # what \n",
    "        \n",
    "        # padding of 1, to ensure same dimensions\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.glimpse_sensor.n, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 16, track_running_stats = True)\n",
    "         \n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 3, stride = 3, padding = 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 3, stride = 3, padding = 1)\n",
    "        \n",
    "        # TODO size in and out\n",
    "        # W * H of previous layer * depth\n",
    "        self.fc1 = nn.Linear(in_features = 128, out_features = self.glimpse_output_size)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = self.glimpse_output_size, track_running_stats = True)\n",
    "        \n",
    "        # where\n",
    "        # in_features = 2, loc is a tuple of (x,y)\n",
    "        self.loc_fc1 = nn.Linear(in_features = 2, out_features = self.hidden_layer_size)\n",
    "        self.loc_fc2 = nn.Linear(in_features = self.hidden_layer_size, out_features = self.glimpse_output_size)\n",
    "        \n",
    "    def forward(self, x, prev_location):\n",
    "        glimpses = self.glimpse_sensor.extract_glimpses(x, prev_location)\n",
    "        \n",
    "        # what\n",
    "        # 3 conv layers\n",
    "        h = self.conv1(glimpses)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self.max_pool1(h)\n",
    "        \n",
    "        h = F.relu(self.conv3(h))\n",
    "        \n",
    "        h = self.max_pool2(h) \n",
    "        \n",
    "        # flatten\n",
    "        # keep batch dimension and determine other one automatically\n",
    "        h = h.view(x.shape[0],-1)\n",
    "        \n",
    "        # fully connected layers\n",
    "        h = self.fc1(h)\n",
    "        h = self.bn2(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # where\n",
    "        #TODO does loc need to be flattened?\n",
    "        l = self.loc_fc1(prev_location)\n",
    "        l = F.relu(l)\n",
    "        l = self.loc_fc2(l)\n",
    "        \n",
    "        # combine what and where\n",
    "        g = F.relu(h + l)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Core Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the `what` and `where` feature representation by the Glimpse Network\n",
    "    and a hidden state to produce a new hidden state and prediction.\n",
    "    Consists of 2 LSTM layers stacked on top of each other.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config: CoreNetworkConfig, \n",
    "                 train_config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        batch_size:Int          - size of a batch. Used compute the inital hidden state \n",
    "        glimpse_input_size: Int - size of the input. Equal to the output size of the Glimpse Network \n",
    "        lstm_hidden_size: Int   - size of the output and hidden layers\n",
    "        num_layers: Int         - number of stacked LSTM layers\n",
    "        ----\n",
    "        \"\"\"\n",
    "        super(CoreNetwork, self).__init__()\n",
    "        self.glimpse_input_size = config.glimpse_input_size\n",
    "        self.lstm_hidden_size = config.lstm_hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.batch_size = train_config.batch_size\n",
    "        # batch_first = false -> (SEQ x B x Features)\n",
    "        self.stacked_lstm = nn.LSTM(input_size = self.glimpse_input_size,\n",
    "                                    hidden_size = self.lstm_hidden_size,\n",
    "                                    num_layers = self.num_layers,\n",
    "                                    batch_first = False)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # does one step \n",
    "        (prediction, new_hidden) = self.stacked_lstm(x, hidden)\n",
    "        # new_hidden is a tuple of layer state and cell state \n",
    "        # remove channel dimension\n",
    "        return (prediction[0], new_hidden)\n",
    "        \n",
    "    def initial_hidden_state(self):\n",
    "        \"\"\"Initialize hidden state and cell state with zeroes.\"\"\"\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.lstm_hidden_size, requires_grad = True, device = global_device),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.lstm_hidden_size, requires_grad = True,  device = global_device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Location Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the hidden state of the Core Network and produces a next location to attend to. \n",
    "    The next location is determined stochastically from a normal distribution \n",
    "    with a fixed deviation and a learned mean.\n",
    "    \n",
    "    Outputs the next location alongside the mean.\n",
    "    \n",
    "    Non-differentiable and needs to be trained with Reinforcment Learning.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                config:LocationNetworkConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        input_size: Int - the input size. Equal to the output size of the Core Network\n",
    "        std_dev: Float  - the fixed standard deviation/variance \n",
    "        \"\"\"\n",
    "        super(LocationNetwork, self).__init__()\n",
    "        # location -> (x, y)\n",
    "        self.fc = nn.Linear(config.input_size, config.hidden_size)\n",
    "        self.fc_out = nn.Linear(config.hidden_size, 2)\n",
    "        self.std_dev = config.std_dev\n",
    "    \n",
    "    def forward(self, h_t):\n",
    "        # compute mean\n",
    "        feat = F.relu(self.fc(h_t.detach()))\n",
    "        mu = torch.tanh(self.fc_out(feat))\n",
    "\n",
    "        # reparametrization trick\n",
    "        l_t = torch.distributions.Normal(mu, self.std).rsample()\n",
    "        l_t = l_t.detach()\n",
    "        log_pi = Normal(mu, self.std).log_prob(l_t)\n",
    "\n",
    "        log_pi = torch.sum(log_pi, dim=1)\n",
    "\n",
    "        # bound between [-1, 1]\n",
    "        l_t = torch.clamp(l_t, -1, 1)\n",
    "\n",
    "        return log_pi, l_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Action Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the internal state of the Core Network to produce the final output classification.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 conf:ActionNetworkConfig):\n",
    "        super(ActionNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(conf.input_size, conf.num_classes)\n",
    "        \n",
    "    def forward(self, hidden_t_bottom_layer):\n",
    "        predictions = F.log_softmax(self.fc(hidden_t_bottom_layer), dim = 1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Baseline Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Incorporates the baseline in the reward function\n",
    "    to reduce the variance of the gradient update.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 config:BaselineNetworkConfig):\n",
    "        super(BaselineNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(config.input_size, config.output_size)\n",
    "\n",
    "    def forward(self, hidden_t):\n",
    "        baseline_t = self.fc(hidden_t.detach())\n",
    "        return baseline_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRecurrentAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines all networks described above.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config:Config\n",
    "                ):\n",
    "        super(DeepRecurrentAttentionModel, self).__init__()\n",
    "        self.batch_size = config.training_conf.batch_size\n",
    "      \n",
    "        \n",
    "        self.glimpse_network = GlimpseNetwork(config.glimpse_sensor_conf, config.glimpse_network_conf)\n",
    "        \n",
    "        self.core_network = CoreNetwork(config.core_network_conf, config.training_conf)\n",
    "        \n",
    "        self.location_network = LocationNetwork(config.location_network_conf)\n",
    "        \n",
    "        self.action_network = ActionNetwork(config.action_network_conf)\n",
    "        \n",
    "        self.baseline_network = BaselineNetwork(config.baseline_network_conf)\n",
    "    \n",
    "    def initial_hidden_state(self):\n",
    "        return self.core_network.initial_hidden_state()\n",
    "    \n",
    "    def initial_glimpse_location(self):\n",
    "        init_location = torch.FloatTensor(self.batch_size, 2).uniform_(-1, 1).to(global_device)\n",
    "        init_location.requires_grad = True\n",
    "        return init_location\n",
    "        #eturn torch.Tensor(self.batch_size, 2).fill_(0.0).to(global_device)\n",
    "    \n",
    "    def forward(self,\n",
    "                x,\n",
    "                prev_hidden,\n",
    "                prev_location,\n",
    "                also_compute_probabilities = False):\n",
    "        \n",
    "        g_t = self.glimpse_network(x, prev_location.detach())\n",
    "        # add seq dimension\n",
    "        g_t = g_t.unsqueeze(0)\n",
    "        \n",
    "        # hidden_t - top layer output\n",
    "        # total_hidden_t - all layers including cell parameters\n",
    "        (hidden_t, total_hidden_t) = self.core_network(g_t, prev_hidden)\n",
    "        \n",
    "        (location_t, location_mean_t) = self.location_network(hidden_t.detach())\n",
    "        baseline_t = self.baseline_network(hidden_t.detach())\n",
    "        \n",
    "        if also_compute_probabilities:\n",
    "            # takes not the top, but the bottom layer in the paper\n",
    "            probs_t = self.action_network(total_hidden_t[0][0])\n",
    "            return (total_hidden_t, location_t, location_mean_t, baseline_t, probs_t)\n",
    "        else:\n",
    "            return (total_hidden_t, location_t, location_mean_t, baseline_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline no relu\n",
    "#g = F.relu(h + l) instead of h * l\n",
    "# loss_reinforce * 0.01\n",
    "\n",
    "# init location uniform\n",
    "# both it and other require grad\n",
    "\n",
    "# forward of loc net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage(object):\n",
    "    \"\"\"Computes a running average.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def add(self, val, n = 1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \"\"\"\n",
    "    Entry class, takes care of training, checkpoints and output.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 train_loader,\n",
    "                 valid_loader,\n",
    "                 test_loader,\n",
    "                 conf:Config,\n",
    "                 checkpoint_dir = \"./models\",\n",
    "                 plot_dir = \"./plots\",\n",
    "                 resume = True\n",
    "                 ):\n",
    "        self.learning_rate = conf.training_conf.learning_rate\n",
    "        self.num_epochs = conf.training_conf.num_epochs\n",
    "        self.batch_size = conf.training_conf.batch_size\n",
    "        self.model_name = conf.model_name\n",
    "        self.num_glimpses = conf.training_conf.num_glimpses\n",
    "        self.plots_each_epoch = conf.training_conf.plots_each_epoch\n",
    "        \n",
    "        self.plot_dir = plot_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.best_valid_acc = 0\n",
    "        self.start_epoch = 0\n",
    "        self.resume = resume\n",
    "        self.model_patience = conf.training_conf.model_patience\n",
    "        self.current_without_improvement = 0\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = global_device\n",
    "        \n",
    "        # loaders\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.num_train = len(train_loader.dataset)\n",
    "        self.num_valid = len(valid_loader.dataset)\n",
    "        self.num_test = len(test_loader.dataset)\n",
    "        \n",
    "        self.model = DeepRecurrentAttentionModel(conf).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        if self.resume:\n",
    "            self.load_checkpoint(load_best = False)\n",
    "        for epoch in range(self.start_epoch, self.num_epochs):\n",
    "            # switch layers to train mode\n",
    "            self.model.train()\n",
    "            # train for 1 epoch\n",
    "            train_loss, train_acc = self.train_single_epoch()            \n",
    "            print(\"Epoch: {}, avg. loss: {:3.3f}, avg. acc: {:3.3f}\".format((epoch), train_loss, train_acc))\n",
    "            \n",
    "            # evaluate on validation set\n",
    "            valid_loss, valid_acc = self.evaluate(is_test = False, epoch = epoch)\n",
    "\n",
    "            # update best\n",
    "            is_best = valid_acc >  self.best_valid_acc\n",
    "            if is_best:\n",
    "                self.current_without_improvement = 0\n",
    "            else:\n",
    "                self.current_without_improvement += 1\n",
    "            self.best_valid_acc = max(valid_acc, self.best_valid_acc)\n",
    "            \n",
    "            # checkpoint\n",
    "            self.save_checkpoint(\n",
    "                {'epoch': epoch + 1,\n",
    "                 'model_state': self.model.state_dict(),\n",
    "                 'optim_state': self.optimizer.state_dict(),\n",
    "                 'best_valid_acc': self.best_valid_acc,\n",
    "                 },is_best\n",
    "            )\n",
    "            if self.current_without_improvement > self.model_patience:\n",
    "                print(f\"{self.model_patience} epochs without improvement. Stopping.\")\n",
    "                return\n",
    "    \n",
    "    def forward_pass(self, images, labels):\n",
    "        hidden_t = self.model.initial_hidden_state()\n",
    "        location_t = self.model.initial_glimpse_location()\n",
    "        locations = [location_t]\n",
    "        baselines = []\n",
    "        log_probs = []\n",
    "        for glimpse_i in range(self.num_glimpses):\n",
    "            is_last = glimpse_i == self.num_glimpses - 1\n",
    "            if is_last :\n",
    "                (hidden_t, location_t, log_probs_t, baseline_t, probs_t) = self.model(images,\n",
    "                                                                                  hidden_t,\n",
    "                                                                                  location_t, \n",
    "                                                                                  also_compute_probabilities = True)\n",
    "            else:\n",
    "                (hidden_t, location_t, log_probs_t, baseline_t) = self.model(images, hidden_t, location_t)\n",
    "            baselines.append(baseline_t[:,0])\n",
    "            locations.append(location_t)\n",
    "            log_probs.append(log_probs_t)\n",
    " \n",
    "        baselines = torch.stack(baselines).transpose(1, 0)\n",
    "        log_probs = torch.stack(log_probs).transpose(1, 0)\n",
    "         \n",
    "        # compute loss and rewards\n",
    "\n",
    "        # reward\n",
    "        predicted = probs_t.argmax(dim = 1).detach()\n",
    "        rewards = (predicted == labels).float()\n",
    "\n",
    "        rewards = rewards.unsqueeze(1).repeat(1, self.num_glimpses)\n",
    "\n",
    "        # losses \n",
    "        # action\n",
    "        loss_action = F.nll_loss(probs_t, labels)\n",
    "\n",
    "        # baseline\n",
    "        loss_baseline = F.mse_loss(baselines, rewards)\n",
    "\n",
    "        # reinforcement\n",
    "        adjusted_reward = rewards - baselines.detach()\n",
    "        \n",
    "        #minus because the higher the reward: the better\n",
    "        log_likelihood_rewards = torch.sum( - log_probs * adjusted_reward, dim = 1)\n",
    "        loss_reinforce = torch.mean(log_likelihood_rewards, dim = 0)\n",
    "\n",
    "        loss = loss_action + loss_baseline + loss_reinforce * 0.01\n",
    "        \n",
    "        #if np.random.randint(1,51,1).item() == 50:\n",
    "        #    print(f\"Loss = [A:{loss_action:+7.3},B:{loss_baseline:+7.3},R:{loss_reinforce:+7.3}] - Weight sum = [A:{self.model.action_network.fc.weight.detach().sum():+7.3},B:{self.model.baseline_network.fc.weight.detach().sum():+7.3},R:{self.model.location_network.fc.weight.detach().sum():+7.3}]\")\n",
    "        \n",
    "        return (predicted, loss, locations)\n",
    "    \n",
    "    def train_single_epoch(self):\n",
    "        epoch_losses = RunningAverage()\n",
    "        epoch_accuracies = RunningAverage()\n",
    "        \n",
    "        with tqdm(total = self.num_train) as pbar:\n",
    "            start = time.time()\n",
    "            # images - mini batch\n",
    "            for i, (images, labels) in enumerate(self.train_loader): \n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                #print(self.model.action_network.fc.weight.grad)\n",
    "                # zero out current gradiate accumulators\n",
    "                self.optimizer.zero_grad()\n",
    "                # forward pass\n",
    "                (predictions,loss, locations) = self.forward_pass(images, labels)\n",
    "                # backward and optimize\n",
    "\n",
    "                # propagate new ones\n",
    "                loss.backward()\n",
    "                \n",
    "                #print(self.model.baseline_network.fc.weight.grad)    \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute accuracy\n",
    "                correct = (predictions == labels).float()\n",
    "                acc = 100 * (correct.sum() / len(labels))\n",
    "                \n",
    "                elapsed = time.time() - start\n",
    "                # store\n",
    "                epoch_losses.add(loss.item(), len(images))\n",
    "                epoch_accuracies.add(acc.item(), len(images))\n",
    "                \n",
    "                pbar.set_description((\"{:.1f}s loss: {:.3f} , acc: {:3.3f}\"\n",
    "                                      .format(elapsed, loss.item(), acc.item())))\n",
    "                pbar.update(self.batch_size)\n",
    "        return (epoch_losses.avg, epoch_accuracies.avg)\n",
    "        \n",
    "    def evaluate(self, is_test, epoch = None):\n",
    "        if epoch is None: \n",
    "            epoch = self.start_epoch\n",
    "        loader = self.test_loader if is_test else self.valid_loader\n",
    "        name = \"test\" if is_test else f\"Epoch: {epoch}, validation\"\n",
    "        \n",
    "        \n",
    "        n = self.num_test if is_test else self.num_valid\n",
    "        if is_test:\n",
    "            self.load_checkpoint(load_best = True)\n",
    "        \n",
    "        # switch layers to evaluation mode    \n",
    "        self.model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        losses = RunningAverage()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #with tqdm(total = self.num_train) as pbar:\n",
    "            start = time.time()\n",
    "            is_first = True\n",
    "            # images - mini batch\n",
    "            for i, (images, labels) in enumerate(loader):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                # forward pass\n",
    "                (predictions, loss, locations) = self.forward_pass(images, labels)\n",
    "                \n",
    "                #save plots if needed\n",
    "                if is_first:\n",
    "                    self.maybe_save_images_with_locations(images,locations, epoch)\n",
    "                is_first = False\n",
    "                \n",
    "                # compute accuracy\n",
    "                correct += (predictions == labels).int().sum()\n",
    "                losses.add(loss.item(), len(images))\n",
    "                elapsed = time.time() - start\n",
    "                \n",
    "                # store\n",
    "                losses.add(loss.item(), len(images))\n",
    "\n",
    "                #pbar.set_description((\"{:.1f}s Evaluating... \".format(elapsed)))\n",
    "                #pbar.update(self.batch_size)\n",
    "        accuracy = (100. * correct) / n\n",
    "        print(f'[*] {name} - {correct}/{n} Correct, Acc: ({accuracy:3.2f}%), Err({(100-accuracy):3.2f}%)')\n",
    "        return (losses.avg, accuracy)\n",
    "    \n",
    "    def maybe_save_images_with_locations(self, images, locations, epoch):\n",
    "        if not self.plots_each_epoch:\n",
    "            return\n",
    "        path = os.path.join(self.plot_dir,self.model_name)\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "        imgs = [g.cpu().data.numpy().squeeze() for g in images]\n",
    "        locs = [l.cpu().data.numpy() for l in locations]\n",
    "        pickle.dump(locs, open(path + f\"/loc_{epoch + 1}.p\", \"wb\"))\n",
    "        pickle.dump(imgs, open(path + f\"/data_{epoch + 1}.p\", \"wb\"))\n",
    "        \n",
    "    def save_checkpoint(self, state, is_best):\n",
    "        filename = self.model_name + '_ckpt'\n",
    "        ckpt_path = os.path.join(self.checkpoint_dir, filename)\n",
    "        os.makedirs(self.checkpoint_dir,exist_ok = True)\n",
    "        torch.save(state, ckpt_path)\n",
    "                    \n",
    "        if is_best:\n",
    "            filename = self.model_name + '_model_best'\n",
    "            shutil.copyfile(ckpt_path, os.path.join(self.checkpoint_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, load_best):\n",
    "        print(\"[*] Loading model from {}\".format(self.checkpoint_dir))\n",
    "\n",
    "        filename = self.model_name + '_ckpt'\n",
    "        if load_best:\n",
    "            filename = self.model_name + '_model_best'\n",
    "        ckpt_path = os.path.join(self.checkpoint_dir, filename)\n",
    "        if  not os.path.exists(ckpt_path):\n",
    "            print(\"No checkpoint found, starting from scratch\")\n",
    "            return\n",
    "        ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        # load variables from checkpoint\n",
    "        self.best_valid_acc = ckpt['best_valid_acc']\n",
    "        self.start_epoch = ckpt['epoch']\n",
    "        self.model.load_state_dict(ckpt['model_state'])\n",
    "        self.optimizer.load_state_dict(ckpt['optim_state'])\n",
    "\n",
    "        if load_best:\n",
    "            print(\n",
    "                \"[*] Loaded {} checkpoint @ epoch {} \"\n",
    "                \"with best valid acc of {:.3f}\".format(\n",
    "                    filename, ckpt['epoch'], ckpt['best_valid_acc'])\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"[*] Loaded {} checkpoint @ epoch {}\".format(\n",
    "                    filename, ckpt['epoch'])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = locator.data_loader(DatasetType.TRAIN)\n",
    "valid_loader = locator.data_loader(DatasetType.VALID)\n",
    "test_loader = locator.data_loader(DatasetType.TEST)\n",
    "\n",
    "trainer = Train(train_loader = train_loader,\n",
    "      valid_loader = valid_loader,\n",
    "      test_loader = test_loader,\n",
    "        conf = global_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# plot_dir = os.path.join(trainer.plot_dir, trainer.model_name)\n",
    "epoch =  4\n",
    "\n",
    "def bounding_box(x, y, size, color=\"w\"):\n",
    "    x = int(x - (size / 2))\n",
    "    y = int(y - (size / 2))\n",
    "    rect = patches.Rectangle(\n",
    "        (x, y), size, size, linewidth=1, edgecolor=color, fill=False\n",
    "    )\n",
    "    return rect\n",
    "\n",
    "# read in pickle files\n",
    "glimpses  = pickle.load(open(plot_dir + f\"/data_{epoch}.p\", \"rb\"))[0:9]\n",
    "locations = [l[0:9] for l in pickle.load(open(plot_dir + f\"/loc_{epoch}.p\", \"rb\"))]\n",
    "\n",
    "num_anims = len(locations)\n",
    "num_cols  = len(glimpses)\n",
    "img_shape = glimpses[0].shape[1]\n",
    "\n",
    "coords = [inverse_normalized_position(torch.tensor(l), img_shape) for l in locations]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=num_cols)\n",
    "fig.set_dpi(200)\n",
    "\n",
    "# plot base image\n",
    "for j, ax in enumerate(axs.flat):\n",
    "    ax.imshow(glimpses[j], cmap=\"Greys_r\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "def updateData(i):\n",
    "    color = \"r\"\n",
    "    co = coords[i]\n",
    "    for j, ax in enumerate(axs.flat):\n",
    "        for p in ax.patches:\n",
    "            p.remove()\n",
    "        c = co[j]\n",
    "        rect = bounding_box(c[0], c[1], size, color)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "# animate\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, updateData, frames=num_anims, interval=500, repeat=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
